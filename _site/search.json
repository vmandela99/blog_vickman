[
  {
    "objectID": "courses/upcoming_courses.html",
    "href": "courses/upcoming_courses.html",
    "title": "Upcoming Courses",
    "section": "",
    "text": "Click the button below to pay for this course using Mastercard, M-Pesa, or Visa.\nAfter payment, you will receive your access credentials via email.\n\n\n\n\n\n\n\n\nPay Now\n\n\n\n\n\n  \n    \n    Get latest Courses and Webinars\n    \n    \n      \n        Your email:\n        \n      \n\n      \n        Name and Designation:\n        \n      \n\n      \n        \n          Send\n        \n        \n          Exit"
  },
  {
    "objectID": "courses/upcoming_courses.html#ai-for-me-masterclass-access",
    "href": "courses/upcoming_courses.html#ai-for-me-masterclass-access",
    "title": "Upcoming Courses",
    "section": "",
    "text": "Click the button below to pay for this course using Mastercard, M-Pesa, or Visa.\nAfter payment, you will receive your access credentials via email.\n\n\n\n\n\n\n\n\nPay Now\n\n\n\n\n\n  \n    \n    Get latest Courses and Webinars\n    \n    \n      \n        Your email:\n        \n      \n\n      \n        Name and Designation:\n        \n      \n\n      \n        \n          Send\n        \n        \n          Exit"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nR\n\n\nAnalysis\n\n\nVisualization\n\n\nStatistics\n\n\nRegression\n\n\nPractical Evaluation\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Absorption Rates and Their Implications in Kenya’s County Health Funding\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nThe Growing Importance of Wildlife Economy: A Path to Sustainable Growth\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nAnalysis\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Child Stunting in Ethiopia: Trends, Impacts, and Future Directions: USING DHS DATA\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nOct 19, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Life Expectancy and Healthy Life Expectancy (HALE) in Morocco (2000-2018): USING DHS DATA\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nOct 19, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoring and Evaluation in Health and Wellness: A Statistical Analysis Approach\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Develop a Research, Monitoring, and Evaluation Framework for Your Project\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Monitoring Plan for Your Project\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build an Evaluation Plan for Your Project\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nKoboToolbox Introduction to Computer assistes programming interface(CAPI)\n\n\n\n\n\n\nSurvey\n\n\nData Collection\n\n\nKoboToolbox\n\n\nCAPI\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nBest Practices for Survey Design and Field Management Using KoboToolbox\n\n\n\n\n\n\nSurvey\n\n\nData Collection\n\n\nKoboToolbox\n\n\nCAPI\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nData Quality, Analysis, and Case Studies Using KoboToolbox\n\n\n\n\n\n\nSurvey\n\n\nData Collection\n\n\nKoboToolbox\n\n\nCAPI\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey CTO Analysis, Challenges, and Future Trends\n\n\n\n\n\n\nSurvey\n\n\nData Collection\n\n\nSurveyCTO\n\n\nCAPI\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey CTO Data Collection and Management\n\n\n\n\n\n\nSurvey\n\n\nData Collection\n\n\nSurveyCTO\n\n\nCAPI\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey CTO Foundational Knowledge and Setup\n\n\n\n\n\n\nSurvey\n\n\nData Collection\n\n\nSurveyCTO\n\n\nCAPI\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking Insights: A Step-by-Step Guide to Calculating Multidimensional Poverty Index (MPI) Using SPSS\n\n\n\n\n\n\nDemographicHealthSurvey\n\n\nStatistics\n\n\nSPSS\n\n\nMultidimensional Poverty Index (MPI)\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\nSurvey\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntergrating Citizen Science Data in Conversation\n\n\n\n\n\n\nClimate\n\n\nGIS\n\n\nRegression\n\n\nR\n\n\nStatistics\n\n\nVisualization\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Have a Portfolio Website: Documenting Your Experience for Success!\n\n\n\n\n\n\nSoft Skills\n\n\nWebsite\n\n\nProfile\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a website for free: Ultimate guide\n\n\n\n\n\n\nSoft Skills\n\n\nWebsite\n\n\nGit codes\n\n\nProfile\n\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting Team Performance\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReceived the Job, Excel and Word for Life!\n\n\n\n\n\n\nMonitoring and Evaluation\n\n\nSoft Skills\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDHS data - Convert Column Values into Column Headers in R\n\n\n\n\n\n\nDemographicHealthSurvey\n\n\nStatistics\n\n\nR\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDHS Survey Design Computation\n\n\n\n\n\n\nDemographicHealthSurvey\n\n\nStatistics\n\n\nR\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProgress and Challenges: HIV Infections in Kenya from 2019 to 2021 Analysis with R\n\n\n\n\n\n\nVisualization\n\n\nStatistics\n\n\nR\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKenya Milk Export and Imports Analysis with R\n\n\n\n\n\n\nVisualization\n\n\nStatistics\n\n\nR\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nComparisons 01: Gender Equality Analysis Olympics\n\n\n\n\n\n\nVisualization\n\n\nStatistics\n\n\nR\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPotential causes of repetition and dropout in Primary Education covering Primary 1 (P1) to Primary 6 case study: Rural and Urban areas(P6)\n\n\n\n\n\n\nVisualization\n\n\nStatistics\n\n\nR\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nMar 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOrganizing Survey logistics\n\n\n\n\n\n\nSoft Skills\n\n\nSurvey\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nTraining staff\n\n\n\n\n\n\nSoft Skills\n\n\nSurvey\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm\n\n\n\n\n\n\nVisualization\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Weather Data via API with R\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\nAPI\n\n\nWeather Data\n\n\nClimate\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nAlbert Rapp\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey tools\n\n\n\n\n\n\nSoft Skills\n\n\nSurvey\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nMastering the Art of Task Juggling: A Simple Guide to Prioritizing and Multi-Tasking\n\n\n\n\n\n\nSoft Skills\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Project Management: A Simple Guide to Success\n\n\n\n\n\n\nSoft Skills\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nCrunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace\n\n\n\n\n\n\nSoft Skills\n\n\n\n\n\n\n\n\n\nAug 9, 2022\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nSpartial regression: Lesson3\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\nRegression\n\n\n\n\n\n\n\n\n\nJul 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Data Manipulation in R: Lesson2\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 23, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial analysis with R: Lesson1\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 16, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness intelligence tools\n\n\n\n\n\n\nBI\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/spatial regression/index.html",
    "href": "posts/spatial regression/index.html",
    "title": "Spartial regression: Lesson3",
    "section": "",
    "text": "Spatial regression is a powerful technique in spatial analytics that allows us to model relationships between variables while accounting for the spatial dependencies inherent in geospatial data. In this advanced blog post, we will dive into the intricacies of spatial regression using R. Our goal is to uncover hidden patterns and relationships in geospatial datasets, focusing on a hypothetical scenario of housing prices and neighborhood characteristics."
  },
  {
    "objectID": "posts/spatial regression/index.html#mastering-spatial-regression-in-r-unveiling-patterns-in-geospatial-data",
    "href": "posts/spatial regression/index.html#mastering-spatial-regression-in-r-unveiling-patterns-in-geospatial-data",
    "title": "Spartial regression: Lesson3",
    "section": "",
    "text": "Spatial regression is a powerful technique in spatial analytics that allows us to model relationships between variables while accounting for the spatial dependencies inherent in geospatial data. In this advanced blog post, we will dive into the intricacies of spatial regression using R. Our goal is to uncover hidden patterns and relationships in geospatial datasets, focusing on a hypothetical scenario of housing prices and neighborhood characteristics."
  },
  {
    "objectID": "posts/spatial regression/index.html#understanding-spatial-regression",
    "href": "posts/spatial regression/index.html#understanding-spatial-regression",
    "title": "Spartial regression: Lesson3",
    "section": "Understanding Spatial Regression",
    "text": "Understanding Spatial Regression\nSpatial regression extends traditional regression models by incorporating spatial relationships. It acknowledges that observations closer in space may exhibit similarities or dependencies that traditional models overlook. There are different types of spatial regression models, and in this blog, we will focus on the Spatial Lag Model.\n\nSpatial Lag Model\nThe Spatial Lag Model introduces a spatially lagged dependent variable, indicating the influence of neighboring observations. Let’s consider housing prices as the dependent variable and neighborhood characteristics as independent variables.\n# Install and load required packages\ninstall.packages(\"spdep\")\nlibrary(spdep)\n\n# Load geospatial data (housing prices and neighborhood characteristics)\nhousing_data &lt;- st_read(\"path/to/housing_data.shp\")\n\n# Create spatial weights matrix\nw &lt;- poly2nb(st_as_sfc(housing_data))\nlw &lt;- nb2listw(w)\n\n# Fit Spatial Lag Model\nmodel &lt;- lm(y ~ x1 + x2 + lag(y, listw = lw), data = housing_data)\nsummary(model)\n\nReplace “path/to/housing_data.shp” with the actual path to your Shapefile. This example assumes you have a dependent variable y (housing prices) and independent variables x1 and x2 (neighborhood characteristics)."
  },
  {
    "objectID": "posts/spatial regression/index.html#interpretation-of-results",
    "href": "posts/spatial regression/index.html#interpretation-of-results",
    "title": "Spartial regression: Lesson3",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\nThe summary output provides information about coefficients, standard errors, and statistical significance. Pay special attention to the spatial lag coefficient, which indicates the impact of neighboring observations on the dependent variable."
  },
  {
    "objectID": "posts/spatial regression/index.html#diagnostic-checks",
    "href": "posts/spatial regression/index.html#diagnostic-checks",
    "title": "Spartial regression: Lesson3",
    "section": "Diagnostic Checks",
    "text": "Diagnostic Checks\nAssess the model’s validity and assumptions through diagnostic checks.\n# Spatial autocorrelation of residuals\nresiduals &lt;- residuals(model)\nmoran.test(residuals, listw = lw)\n\nA significant Moran’s I statistic for residuals indicates spatial autocorrelation, suggesting the need for further model refinement."
  },
  {
    "objectID": "posts/spatial regression/index.html#visualization",
    "href": "posts/spatial regression/index.html#visualization",
    "title": "Spartial regression: Lesson3",
    "section": "Visualization",
    "text": "Visualization\nVisualize the spatial patterns and regression results on a map.\n# Plotting observed vs. predicted values\nplot(housing_data$y, fitted(model), main = \"Observed vs. Predicted\", xlab = \"Observed\", ylab = \"Predicted\")\n\n# Spatial autocorrelation map of residuals\nspplot(residuals, main = \"Spatial Autocorrelation Map of Residuals\", col.regions = colorRampPalette(c(\"blue\", \"white\", \"red\")))\n\nThese visualizations help in understanding how well the model captures spatial patterns and where adjustments might be needed."
  },
  {
    "objectID": "posts/spatial regression/index.html#conclusion",
    "href": "posts/spatial regression/index.html#conclusion",
    "title": "Spartial regression: Lesson3",
    "section": "Conclusion",
    "text": "Conclusion\nSpatial regression in R opens up new dimensions for analyzing geospatial data. In this blog post, we explored the Spatial Lag Model as an advanced technique for modeling spatial dependencies in housing prices and neighborhood characteristics. The interpretation of results, diagnostic checks, and visualizations are crucial components of spatial regression analysis.\nAs you venture into spatial analytics, consider exploring other spatial regression models, incorporating additional spatial variables, and applying advanced techniques to enhance the robustness of your models. Stay tuned for more advanced spatial analytics topics, including machine learning with geospatial data and building interactive web maps. Happy analyzing!"
  },
  {
    "objectID": "posts/organizing-survey-logistics/index.html",
    "href": "posts/organizing-survey-logistics/index.html",
    "title": "Organizing Survey logistics",
    "section": "",
    "text": "Efficient logistics are a cornerstone for the seamless execution of data collection initiatives. Plan and schedule surveys strategically to optimize resources and minimize disruptions. Leverage project management tools such as Trello or Asana to coordinate field staff and track progress.\n\n\nBoard Title: Data Collection Survey Schedule\nLists:\n\nTo-Do:\n\nCard 1: Prepare Survey Questions\nCard 2: Create Google Form\nCard 3: Set Up Trello Board\nCard 4: Define Data Collection Period\n\nIn Progress:\n\nCard 5: Share Google Form Link\nCard 6: Conduct Mock Data Collection Exercise\n\nData Collection - Phase 1:\n\nCard 7: Distribute Survey to Farmers in Region A\nCard 8: Monitor Responses in Real-Time\nCard 9: Address Any Technical Issues\n\nData Collection - Phase 2:\n\nCard 10: Distribute Survey to Farmers in Region B\nCard 11: Monitor Responses in Real-Time\nCard 12: Address Any Technical Issues\n\nData Collection - Phase 3:\n\nCard 13: Distribute Survey to Farmers in Region C\nCard 14: Monitor Responses in Real-Time\nCard 15: Address Any Technical Issues\n\nData Analysis:\n\nCard 16: Analyze Survey Results\nCard 17: Generate Summary Report\n\n\nTeam Member Assignments:\n\nJohn (Project Manager):\n\nCards 1-4\nOversee the entire project and ensure all tasks are on schedule.\n\nAlice (Survey Coordinator):\n\nCards 5-6\nCoordinate the mock data collection exercise and share the survey link.\n\nBob (Field Team - Region A):\n\nCards 7-9\nDistribute surveys and monitor responses in Region A.\n\nCharlie (Field Team - Region B):\n\nCards 10-12\nDistribute surveys and monitor responses in Region B.\n\nDiana (Field Team - Region C):\n\nCards 13-15\nDistribute surveys and monitor responses in Region C.\n\nEva (Data Analyst):\n\nCards 16-17\nAnalyze survey results and generate a summary report.\n\n\nBoard Overview:\n\nTeam members can move their assigned cards across lists as they progress through different stages.\nThe board provides a clear visual representation of the survey schedule and each team member’s responsibilities.\nReal-time updates on Trello enable everyone to track progress and identify any bottlenecks.\n\nBenefits:\n\nVisual Representation: Team members can quickly grasp the project’s status and upcoming tasks.\nReal-Time Monitoring: Progress is updated in real-time, enhancing transparency and accountability.\nEfficient Collaboration: Team members can leave comments on cards for discussions and updates.\nIdentifying Bottlenecks: Delays or issues can be easily identified by tracking the movement of cards.\n\nThis Trello board example facilitates effective team coordination, ensuring that each stage of the survey schedule is managed efficiently and promoting accountability within the team. Adjust the structure based on the specific needs and complexity of your data collection project."
  },
  {
    "objectID": "posts/organizing-survey-logistics/index.html#organizing-survey-logistics",
    "href": "posts/organizing-survey-logistics/index.html#organizing-survey-logistics",
    "title": "Organizing Survey logistics",
    "section": "",
    "text": "Efficient logistics are a cornerstone for the seamless execution of data collection initiatives. Plan and schedule surveys strategically to optimize resources and minimize disruptions. Leverage project management tools such as Trello or Asana to coordinate field staff and track progress.\n\n\nBoard Title: Data Collection Survey Schedule\nLists:\n\nTo-Do:\n\nCard 1: Prepare Survey Questions\nCard 2: Create Google Form\nCard 3: Set Up Trello Board\nCard 4: Define Data Collection Period\n\nIn Progress:\n\nCard 5: Share Google Form Link\nCard 6: Conduct Mock Data Collection Exercise\n\nData Collection - Phase 1:\n\nCard 7: Distribute Survey to Farmers in Region A\nCard 8: Monitor Responses in Real-Time\nCard 9: Address Any Technical Issues\n\nData Collection - Phase 2:\n\nCard 10: Distribute Survey to Farmers in Region B\nCard 11: Monitor Responses in Real-Time\nCard 12: Address Any Technical Issues\n\nData Collection - Phase 3:\n\nCard 13: Distribute Survey to Farmers in Region C\nCard 14: Monitor Responses in Real-Time\nCard 15: Address Any Technical Issues\n\nData Analysis:\n\nCard 16: Analyze Survey Results\nCard 17: Generate Summary Report\n\n\nTeam Member Assignments:\n\nJohn (Project Manager):\n\nCards 1-4\nOversee the entire project and ensure all tasks are on schedule.\n\nAlice (Survey Coordinator):\n\nCards 5-6\nCoordinate the mock data collection exercise and share the survey link.\n\nBob (Field Team - Region A):\n\nCards 7-9\nDistribute surveys and monitor responses in Region A.\n\nCharlie (Field Team - Region B):\n\nCards 10-12\nDistribute surveys and monitor responses in Region B.\n\nDiana (Field Team - Region C):\n\nCards 13-15\nDistribute surveys and monitor responses in Region C.\n\nEva (Data Analyst):\n\nCards 16-17\nAnalyze survey results and generate a summary report.\n\n\nBoard Overview:\n\nTeam members can move their assigned cards across lists as they progress through different stages.\nThe board provides a clear visual representation of the survey schedule and each team member’s responsibilities.\nReal-time updates on Trello enable everyone to track progress and identify any bottlenecks.\n\nBenefits:\n\nVisual Representation: Team members can quickly grasp the project’s status and upcoming tasks.\nReal-Time Monitoring: Progress is updated in real-time, enhancing transparency and accountability.\nEfficient Collaboration: Team members can leave comments on cards for discussions and updates.\nIdentifying Bottlenecks: Delays or issues can be easily identified by tracking the movement of cards.\n\nThis Trello board example facilitates effective team coordination, ensuring that each stage of the survey schedule is managed efficiently and promoting accountability within the team. Adjust the structure based on the specific needs and complexity of your data collection project."
  },
  {
    "objectID": "posts/getting data from Api/index.html",
    "href": "posts/getting data from Api/index.html",
    "title": "Getting Weather Data via API with R",
    "section": "",
    "text": "I came across this awesome post from Albert Rapp about how to get weather data from API and I thought it was a great technique to use. Here we go.\n\nWhat is an API?\nWe are focusing on two APIs (application programming interfaces) for our project. Broadly speaking, an API is anything that we can throw code at to get results that we want.\nOften this refers to some data source that we tap into. But sometimes it also simply means the syntax of code. For example, ggplot2 has a very distinctive API, i.e. a code syntax to create a chart.\nIn our current case, we will just refer to APIs as data sources and we will need to tap into two such APIs, namely these ones:\n\nUS National Weather Service API\nGoogle Geocoding API\n\nThe first one will give us weather forecasts based on specified coordinates and the second one will turn any address into coordinates for us. Today, we’ll focus on the first one.\n\n\nMaking requests to an API\nIf you’ve never worked with APIs, you know that it can feel like data is hidden away behind an API. Thankfully, the {httr2} package helps us a lot, even if we’ve never dealt with APIs before.\nCase in point, my fellow YouTubeR (see what I did there? it’s “YouTube” and “R”) Melissa Van Bussel put together an excellent video that shows you how to use {httr2} to call the API of openAI or GitLab.\nAnyway, here’s how to make a request to an API to get data:\n\nNavigate to the URL the data can be accessed from\n(Optional depending on the API) Authenticate\nGet the response\n\nWith the National Weather Service, you can easily try this yourself. Just head to the following url using your web browser:\nhttps://api.weather.gov/points/38.8894,-77.0352\nIf you navigate there, you will get cryptic data like that:\n\nThis is what is known as a JSON file. More on that later. For now, notice that what you see at the end of the url after points/ corresponds to the coordinates that are given in the JSON output.\nThis means that the recipe for calling the weather API is simple: Append points/{lat},{long} at the end of the base_url, i.e. https://api.weather.gov/. In this case, {lat},{long} corresponds to the latitude and longitude of the location you want to get weather forecasts for.\n\n\nMaking a request with {httr2}\nThe {httr2} syntax to make this work mimics this quite well. Here’s how it looks.\n\nBasically, at the core of every request is the request() function that needs to know the base_url. This returns an &lt;httr2_request&gt; object that can be passed to further req_*() functions to modify the request.\nHere, we used req_url_path_append() to modify the request but there are also other functions (and next week we’ll learn about req_url_query()). Finally, to actually make the request, you can pass everything to req_perform().\n\n\n\nGetting the response\nAs you’ve just seen, your request will return a &lt;httr2_response&gt; and if everything went well, the output will also show you Status: 200 OK. You can get the actual content (the JSON that you’ve seen in your web browser earlier) via one of the many resp_*() functions that handle responses.\n\n\n\nWorking with the response\nAs you’ve seen in the output, the JSON file you receive is structured as a highly nested list. To make sense of this data, we use glimpse() to understand the structure of the object\n\nAnd with pluck() you can easily drill down into specific parts of the list. For example, this could be used to get the URL for the hourly forecasts\n\n\n\nRepeat process for forecasts\nWith the new forecast URL, we can get new JSON data about the forecasts for our location.\n\nIn that output, we can see that there is a list called periods inside of the properties list that contains lists which always have 16 entries. This might be where our forecast data lives.\n\nAha! Each of those 16-entries lists seem to correspond to the forecast of one hour. Thus, to get the things like temperature, startTime, probabilityOfPrecipitation, and short_forecast into a nice tibble format, we have to iterate over all of these 16-entries lists and wrap the values we want into a tibble.\n\nAnd once we have that data, we can transform the time column into a nice date-time object:\n\nThis was awesome. !"
  },
  {
    "objectID": "posts/Training_data_collection/index.html",
    "href": "posts/Training_data_collection/index.html",
    "title": "Training staff",
    "section": "",
    "text": "The competence of your field staff is pivotal to the success of your data collection program. When recruiting, prioritize individuals with a blend of domain expertise and technological proficiency. Ensure comprehensive training sessions that cover both survey tools and the intricacies of the data being collected.\n\n\nObjective: To simulate the process of using survey tools for data collection, specifically Google Forms, and to familiarize staff with the survey creation, distribution, and data retrieval processes.\nScenario: Imagine you are part of an agricultural organization tasked with collecting data on fruit preferences from participants attending a farmers’ conference. The goal is to understand the most popular fruits among farmers for potential agricultural planning.\nSteps:\n\nCreate a Mock Survey:\n\nCreate a Google Form with questions related to fruit preferences. Include multiple-choice questions, checkboxes, and a text entry for comments.\nExample Questions:\n\nWhich fruit do you prefer the most? (Multiple-choice: Apple, Orange, Banana, Other)\nDo you grow any fruits on your farm? (Checkbox: Apple, Orange, Banana, Mango, Other)\nAny additional comments or suggestions?\n\n\nShare the Mock Survey:\n\nShare the Google Form with the training participants using a unique link.\nIn a real-world scenario, this link could be distributed via email, QR codes, or any other method.\n\nSimulate Data Collection:\n\nIn the training session, ask participants to fill out the mock survey using the provided link.\nEncourage them to use different devices such as smartphones, tablets, or laptops to simulate real-world conditions.\n\nReview Survey Responses:\n\nDemonstrate how to access and review survey responses in real-time.\nDiscuss how to analyze the collected data within the Google Forms interface.\n\nTroubleshooting Exercise:\n\nIntroduce common issues that may arise during data collection, such as incomplete submissions, errors in responses, or technical difficulties.\nEncourage participants to troubleshoot and find solutions collaboratively.\n\nFeedback and Discussion:\n\nFacilitate a discussion on the challenges faced during the mock data collection exercise.\nEncourage participants to share insights and lessons learned.\nProvide tips on improving the data collection process.\n\n\nBenefits:\n\nFamiliarity: Participants gain hands-on experience using the survey tool, making them more comfortable with its features.\nTroubleshooting Skills: Staff learn to identify and address issues that may arise during real data collection.\nTeam Collaboration: The exercise fosters teamwork as participants work together to solve challenges.\nFeedback Loop: Insights from the exercise can be used to enhance training materials and improve the actual data collection process.\n\nBy incorporating a mock data collection exercise into training, staff members can build practical skills and confidence, ensuring a smoother transition to actual fieldwork."
  },
  {
    "objectID": "posts/Training_data_collection/index.html#recruitment-and-training-of-data-collection-staff",
    "href": "posts/Training_data_collection/index.html#recruitment-and-training-of-data-collection-staff",
    "title": "Training staff",
    "section": "",
    "text": "The competence of your field staff is pivotal to the success of your data collection program. When recruiting, prioritize individuals with a blend of domain expertise and technological proficiency. Ensure comprehensive training sessions that cover both survey tools and the intricacies of the data being collected.\n\n\nObjective: To simulate the process of using survey tools for data collection, specifically Google Forms, and to familiarize staff with the survey creation, distribution, and data retrieval processes.\nScenario: Imagine you are part of an agricultural organization tasked with collecting data on fruit preferences from participants attending a farmers’ conference. The goal is to understand the most popular fruits among farmers for potential agricultural planning.\nSteps:\n\nCreate a Mock Survey:\n\nCreate a Google Form with questions related to fruit preferences. Include multiple-choice questions, checkboxes, and a text entry for comments.\nExample Questions:\n\nWhich fruit do you prefer the most? (Multiple-choice: Apple, Orange, Banana, Other)\nDo you grow any fruits on your farm? (Checkbox: Apple, Orange, Banana, Mango, Other)\nAny additional comments or suggestions?\n\n\nShare the Mock Survey:\n\nShare the Google Form with the training participants using a unique link.\nIn a real-world scenario, this link could be distributed via email, QR codes, or any other method.\n\nSimulate Data Collection:\n\nIn the training session, ask participants to fill out the mock survey using the provided link.\nEncourage them to use different devices such as smartphones, tablets, or laptops to simulate real-world conditions.\n\nReview Survey Responses:\n\nDemonstrate how to access and review survey responses in real-time.\nDiscuss how to analyze the collected data within the Google Forms interface.\n\nTroubleshooting Exercise:\n\nIntroduce common issues that may arise during data collection, such as incomplete submissions, errors in responses, or technical difficulties.\nEncourage participants to troubleshoot and find solutions collaboratively.\n\nFeedback and Discussion:\n\nFacilitate a discussion on the challenges faced during the mock data collection exercise.\nEncourage participants to share insights and lessons learned.\nProvide tips on improving the data collection process.\n\n\nBenefits:\n\nFamiliarity: Participants gain hands-on experience using the survey tool, making them more comfortable with its features.\nTroubleshooting Skills: Staff learn to identify and address issues that may arise during real data collection.\nTeam Collaboration: The exercise fosters teamwork as participants work together to solve challenges.\nFeedback Loop: Insights from the exercise can be used to enhance training materials and improve the actual data collection process.\n\nBy incorporating a mock data collection exercise into training, staff members can build practical skills and confidence, ensuring a smoother transition to actual fieldwork."
  },
  {
    "objectID": "posts/Survey_guidelines/index.html",
    "href": "posts/Survey_guidelines/index.html",
    "title": "Survey tools",
    "section": "",
    "text": "Choosing the right survey tools is paramount to successful data collection. Opt for platforms like SurveyMonkey, Google Forms, or REDCap for their user-friendly interfaces and adaptability. These tools enable customization of surveys, facilitating the incorporation of context-specific questions for both household and agricultural data.\nExample:\nImagine you are collecting agricultural data on crop yields. Use SurveyMonkey to craft dynamic surveys that adjust based on respondents’ previous answers. If a farmer indicates they grow wheat, subsequent questions can automatically shift to focus on wheat-specific variables, streamlining the data collection process.\n\n\n\nEnsuring data quality in data collection tools is crucial to obtain reliable and accurate information. Here are some key practices to help ensure data quality:\n\nClear and Well-Defined Data Collection Protocols:\n\nClearly define the purpose of data collection.\nDevelop standardized data collection protocols, including detailed instructions for data collectors.\nProvide examples and guidelines for each data entry field.\n\nTraining and Capacity Building:\n\nConduct thorough training sessions for data collectors, ensuring they understand the data collection process and tools.\nInclude training on the importance of data quality, potential challenges, and how to address them.\nRegularly update the training to incorporate any changes or improvements.\n\nUse of Validated and Reliable Tools:\n\nChoose data collection tools that are validated and have a track record of reliability.\nRegularly update and patch software to address any bugs or security issues.\nConsider user-friendly interfaces to minimize errors during data entry.\n\nPre-Testing and Piloting:\n\nBefore full-scale data collection, conduct pre-testing or piloting to identify and address potential issues.\nEvaluate the effectiveness of data collection tools in a controlled environment.\n\nData Validation Checks:\n\nImplement validation checks in data collection tools to ensure the accuracy and integrity of entered data.\nUse range checks, logical checks, and consistency checks to identify and prevent errors.\n\nReal-Time Data Monitoring:\n\nMonitor incoming data in real-time to detect anomalies or inconsistencies.\nImplement automated alerts for data quality issues, enabling immediate corrective actions.\n\nStandardized Coding and Classification:\n\nUse standardized coding and classification systems to ensure consistency across data entries.\nProvide clear definitions for each code or category to minimize misinterpretation.\n\nRandom Audits and Quality Assurance Checks:\n\nConduct random audits of collected data to verify its accuracy.\nEstablish a quality assurance team to regularly review a subset of collected data for consistency and completeness.\n\nUser Feedback Mechanism:\n\nEncourage users to provide feedback on data collection tools and processes.\nEstablish a feedback mechanism to address issues reported by data collectors promptly.\n\nData Cleaning and Deduplication:\n\nRegularly perform data cleaning procedures to correct errors, inconsistencies, and missing values.\nImplement deduplication processes to identify and resolve duplicate entries.\n\nDocumentation and Metadata:\n\nDocument the data collection process, including the data dictionary, variable definitions, and any transformations applied.\nMaintain clear metadata to provide context for each dataset.\n\nRegular Review and Continuous Improvement:\n\nSchedule regular reviews of data collection processes and tools.\nContinuously seek feedback from data collectors and stakeholders to identify areas for improvement."
  },
  {
    "objectID": "posts/Survey_guidelines/index.html#selecting-program-survey-tools",
    "href": "posts/Survey_guidelines/index.html#selecting-program-survey-tools",
    "title": "Survey tools",
    "section": "",
    "text": "Choosing the right survey tools is paramount to successful data collection. Opt for platforms like SurveyMonkey, Google Forms, or REDCap for their user-friendly interfaces and adaptability. These tools enable customization of surveys, facilitating the incorporation of context-specific questions for both household and agricultural data.\nExample:\nImagine you are collecting agricultural data on crop yields. Use SurveyMonkey to craft dynamic surveys that adjust based on respondents’ previous answers. If a farmer indicates they grow wheat, subsequent questions can automatically shift to focus on wheat-specific variables, streamlining the data collection process.\n\n\n\nEnsuring data quality in data collection tools is crucial to obtain reliable and accurate information. Here are some key practices to help ensure data quality:\n\nClear and Well-Defined Data Collection Protocols:\n\nClearly define the purpose of data collection.\nDevelop standardized data collection protocols, including detailed instructions for data collectors.\nProvide examples and guidelines for each data entry field.\n\nTraining and Capacity Building:\n\nConduct thorough training sessions for data collectors, ensuring they understand the data collection process and tools.\nInclude training on the importance of data quality, potential challenges, and how to address them.\nRegularly update the training to incorporate any changes or improvements.\n\nUse of Validated and Reliable Tools:\n\nChoose data collection tools that are validated and have a track record of reliability.\nRegularly update and patch software to address any bugs or security issues.\nConsider user-friendly interfaces to minimize errors during data entry.\n\nPre-Testing and Piloting:\n\nBefore full-scale data collection, conduct pre-testing or piloting to identify and address potential issues.\nEvaluate the effectiveness of data collection tools in a controlled environment.\n\nData Validation Checks:\n\nImplement validation checks in data collection tools to ensure the accuracy and integrity of entered data.\nUse range checks, logical checks, and consistency checks to identify and prevent errors.\n\nReal-Time Data Monitoring:\n\nMonitor incoming data in real-time to detect anomalies or inconsistencies.\nImplement automated alerts for data quality issues, enabling immediate corrective actions.\n\nStandardized Coding and Classification:\n\nUse standardized coding and classification systems to ensure consistency across data entries.\nProvide clear definitions for each code or category to minimize misinterpretation.\n\nRandom Audits and Quality Assurance Checks:\n\nConduct random audits of collected data to verify its accuracy.\nEstablish a quality assurance team to regularly review a subset of collected data for consistency and completeness.\n\nUser Feedback Mechanism:\n\nEncourage users to provide feedback on data collection tools and processes.\nEstablish a feedback mechanism to address issues reported by data collectors promptly.\n\nData Cleaning and Deduplication:\n\nRegularly perform data cleaning procedures to correct errors, inconsistencies, and missing values.\nImplement deduplication processes to identify and resolve duplicate entries.\n\nDocumentation and Metadata:\n\nDocument the data collection process, including the data dictionary, variable definitions, and any transformations applied.\nMaintain clear metadata to provide context for each dataset.\n\nRegular Review and Continuous Improvement:\n\nSchedule regular reviews of data collection processes and tools.\nContinuously seek feedback from data collectors and stakeholders to identify areas for improvement."
  },
  {
    "objectID": "posts/Survey_CTO_Data Collection and Management/index.html",
    "href": "posts/Survey_CTO_Data Collection and Management/index.html",
    "title": "Survey CTO Data Collection and Management",
    "section": "",
    "text": "Understanding Data Collection in the Field When collecting data in the field, researchers go directly to the respondents, which can be households, businesses, or other locations, to gather information. This process can be challenging but is crucial for obtaining high-quality data.\nExample: Imagine a team conducting a survey on household energy use. Enumerators would visit homes to ask questions about energy sources, costs, and usage patterns.\nTips for Effective Data Collection\n\nTrain Enumerators: Ensure that your enumerators understand the survey instrument and are familiar with the technology they will use.\nExample: Conduct a training session where enumerators practice using SurveyCTO on their devices and role-play various interview scenarios.\nEstablish Clear Protocols: Create guidelines for how enumerators should conduct interviews, including how to approach respondents and handle refusals.\nExample: Provide enumerators with scripts to politely introduce themselves and explain the purpose of the survey to gain the respondent’s trust.\nUse Offline Capabilities: SurveyCTO allows data collection offline, which is especially useful in areas with poor internet connectivity.\nExample: Enumerators can collect data while visiting remote areas, and their responses will sync to the server once they have internet access."
  },
  {
    "objectID": "posts/Survey_CTO_Data Collection and Management/index.html#collecting-and-managing-data-in-the-field",
    "href": "posts/Survey_CTO_Data Collection and Management/index.html#collecting-and-managing-data-in-the-field",
    "title": "Survey CTO Data Collection and Management",
    "section": "",
    "text": "Understanding Data Collection in the Field When collecting data in the field, researchers go directly to the respondents, which can be households, businesses, or other locations, to gather information. This process can be challenging but is crucial for obtaining high-quality data.\nExample: Imagine a team conducting a survey on household energy use. Enumerators would visit homes to ask questions about energy sources, costs, and usage patterns.\nTips for Effective Data Collection\n\nTrain Enumerators: Ensure that your enumerators understand the survey instrument and are familiar with the technology they will use.\nExample: Conduct a training session where enumerators practice using SurveyCTO on their devices and role-play various interview scenarios.\nEstablish Clear Protocols: Create guidelines for how enumerators should conduct interviews, including how to approach respondents and handle refusals.\nExample: Provide enumerators with scripts to politely introduce themselves and explain the purpose of the survey to gain the respondent’s trust.\nUse Offline Capabilities: SurveyCTO allows data collection offline, which is especially useful in areas with poor internet connectivity.\nExample: Enumerators can collect data while visiting remote areas, and their responses will sync to the server once they have internet access."
  },
  {
    "objectID": "posts/Survey_CTO_Data Collection and Management/index.html#integrating-gps-and-multimedia-in-capi",
    "href": "posts/Survey_CTO_Data Collection and Management/index.html#integrating-gps-and-multimedia-in-capi",
    "title": "Survey CTO Data Collection and Management",
    "section": "2. Integrating GPS and Multimedia in CAPI",
    "text": "2. Integrating GPS and Multimedia in CAPI\nEnhancing Data Collection with Technology Integrating GPS and multimedia features in CAPI can enrich your data and provide valuable context to the responses collected.\nUsing GPS GPS functionality can help track the location where data is collected, which is especially useful for spatial analysis.\nExample: If you’re surveying agricultural practices, GPS data can pinpoint the locations of farms, allowing for a better understanding of regional farming trends.\nUsing Multimedia Incorporating multimedia elements like images, audio recordings, or videos can enhance the data collection process.\nExample: An interviewer might take photos of different energy sources (like solar panels or firewood) used by households during the energy use survey to provide visual context for the responses."
  },
  {
    "objectID": "posts/Survey_CTO_Data Collection and Management/index.html#data-security-and-privacy-considerations",
    "href": "posts/Survey_CTO_Data Collection and Management/index.html#data-security-and-privacy-considerations",
    "title": "Survey CTO Data Collection and Management",
    "section": "3. Data Security and Privacy Considerations",
    "text": "3. Data Security and Privacy Considerations\nImportance of Data Security When collecting sensitive information, it’s vital to ensure that the data is secure and that respondents’ privacy is protected.\nTips for Ensuring Data Security\n\nUse Encrypted Data Transmission: SurveyCTO encrypts data, ensuring that it remains secure while being transmitted from devices to the server.\nExample: When an enumerator submits survey responses through their device, the data is encrypted, protecting it from unauthorized access.\nLimit Access to Data: Only allow authorized personnel to access the data collected to reduce the risk of breaches.\nExample: Set user roles in SurveyCTO so that only team leaders and data analysts can view sensitive data, while enumerators have limited access.\nObtain Informed Consent: Always inform respondents about how their data will be used and obtain their consent before collecting personal information.\nExample: During the introduction, the enumerator can explain, “Your responses will help us understand community energy needs. Your answers will remain confidential and will be used for research purposes only.”\n\nConclusion Effective data collection and management are crucial for the success of any research project. By implementing proper training, leveraging technology like GPS and multimedia, and prioritizing data security, you can enhance the quality and reliability of your data. In the upcoming posts, we’ll explore how to analyze CAPI data effectively and discuss common challenges you might encounter. Stay tuned!"
  },
  {
    "objectID": "posts/Skills to excel and word/index.html",
    "href": "posts/Skills to excel and word/index.html",
    "title": "Received the Job, Excel and Word for Life!",
    "section": "",
    "text": "🚨 Beware of Innovation Black Holes: My Experience with Stifled Skills 🚨\n\n\nHave you ever landed your dream job, only to find yourself trapped in a time warp where your cutting-edge skills are rendered useless? Imagine joining a company, eager to apply your advanced techniques, only to spend over a year battling with Excel, Word, and PowerPoint. 📊📄📈\n\n\n\nYes, you read that right. I found myself in a professional paradox. My excitement to innovate was met with a management style that was more “status quo” than “let’s grow.” 🚫💡\n\n\n\nThe Reality Check\nIf a workplace doesn’t encourage new ideas or innovation, it’s a red flag 🚩. You might be in a toxic environment where your potential is not just underutilized, but entirely ignored.\n\n\n\n\n\n🔍 Do Your Homework\nBefore jumping ship to a new role, especially with big companies or NGOs, dig deep. Look beyond the shiny job description and ask the tough questions:\n\nHow does the company support innovation?\nAre there opportunities for professional growth?\nWhat’s the management’s attitude towards new ideas?\n\n\n\n✨ Career Motivators, Listen Up\nYour skills are your greatest asset. Don’t let them collect dust in a place that doesn’t value progress. Advocate for yourself, and if you find yourself in a stifling environment, it might be time to move on. 🌟\n\n\nTo all job seekers out there: trust your gut and do your due diligence. The right company will not only recognize your talents but will also provide a fertile ground for your growth and innovation. 🌱\n\n\n\n\n\n\nConclusion\nRemember, your career is too precious to waste in an innovation black hole. Stay curious, stay ambitious, and never settle for less than what you deserve. 💪🚀"
  },
  {
    "objectID": "posts/Prioritize and multi task/index.html",
    "href": "posts/Prioritize and multi task/index.html",
    "title": "Mastering the Art of Task Juggling: A Simple Guide to Prioritizing and Multi-Tasking",
    "section": "",
    "text": "Mastering the Art of Task Juggling: A Simple Guide to Prioritizing and Multi-Tasking\n\nIntroduction\nHello, task-tacklers! Whether you’re a seasoned professional or just diving into the busy world of responsibilities, we’ve got your back. In this guide, we’ll unravel the secrets behind prioritizing tasks and mastering the delicate dance of multi-tasking. So, let’s make this easy to understand for everyone, because everyone deserves a stress-free to-do list!\n1. “The Magic of Prioritizing: Sorting Your To-Do List”\n\n\n\n\n\nImagine your to-do list as a garden. Some tasks are delicate flowers needing immediate attention, while others are sturdy shrubs that can wait a bit. Prioritizing is like being a gardener – identify the high-priority blooms and nurture them first. Consider deadlines, importance, and impact to decide which tasks get the sunlight of your focus.\n2. “ABCs of Prioritization: High, Medium, Low”\n\n\n\n\n\nThink of tasks in terms of urgency and importance. Label them as high, medium, or low priority. Tackle the high-priority tasks first – they’re like the VIPs in your task list, demanding immediate attention. Once the VIPs are sorted, move on to the medium and low-priority tasks. This simple ABC strategy keeps your to-do list organized and manageable.\n3. “Multi-Tasking: A Symphony of Skills”\n\n\n\n\n\nMulti-tasking is like conducting a symphony – each instrument (task) contributes to the overall harmony (completion of all tasks). Start by understanding which tasks complement each other. For example, responding to emails while waiting for a meeting to start. Keep in mind that not all tasks harmonize well together, so choose wisely.\n4. “Limit Distractions: The Peaceful Orchestra”\n\n\n\n\n\nImagine trying to conduct a symphony while fireworks go off around you. Not ideal, right? The same goes for multi-tasking. Minimize distractions to maintain focus on your tasks. Close unnecessary tabs, turn off non-urgent notifications, and create a serene environment for your task symphony to play out smoothly.\n5. “Know Your Limits: Juggling vs. Overloading”\n\n\n\n\n\nPicture multi-tasking as juggling. It’s impressive, but too many balls in the air can lead to a circus disaster. Know your limits – juggle a manageable number of tasks. It’s better to have a controlled juggling act than a chaotic mess. Quality over quantity, always.\nConclusion:\nTask management is like orchestrating a beautiful melody – it requires organization, balance, and a bit of finesse. Prioritize tasks like a skilled gardener, using the ABC strategy. When it’s time to juggle, approach multi-tasking like conducting a symphony – harmonize your tasks and limit distractions. Remember, knowing your limits is key. With these simple strategies, you’ll not only manage your tasks effectively but also enjoy the symphony of productivity. Happy task-tackling!"
  },
  {
    "objectID": "posts/New HIV infections among Children/index.html",
    "href": "posts/New HIV infections among Children/index.html",
    "title": "Progress and Challenges: HIV Infections in Kenya from 2019 to 2021 Analysis with R",
    "section": "",
    "text": "Introduction\n\nIn recent years, Kenya has made significant strides in combating HIV/AIDS, but challenges remain. Understanding trends in new infections is crucial for guiding interventions and policy decisions. This article examines the trends in new HIV infections in Kenya from 2019 to 2021, focusing on both overall and pediatric cases.\n\n\n\nMap of Kenya\n\n\nOverview of New HIV Infections: According to data from the National AIDS Control Council (NACC) of Kenya, the total number of new HIV infections in the country declined slightly from 53,200 in 2019 to 52,200 in 2021. While this represents a modest decrease of 1.13%, it signifies progress in the fight against the epidemic.\nTrends Among Children: One particularly encouraging trend is the decrease in new HIV infections among children aged 0 to 14. In 2019, there were 6,200 new pediatric infections, which decreased to 5,200 in 2021, marking a notable 16.13% reduction over the two-year period.\n\nFactors Contributing to the Decline: Several factors may have contributed to the decline in new HIV infections in Kenya. Expanded access to HIV testing and counseling services, improved maternal and child health programs, and the scale-up of prevention of mother-to-child transmission (PMTCT) interventions have likely played a role. Additionally, increased awareness, education, and community mobilization efforts have helped reduce stigma and discrimination associated with HIV/AIDS, encouraging more people to seek testing and treatment.\nChallenges and Areas for Improvement: Despite the progress made, challenges persist in the fight against HIV/AIDS in Kenya. Access to comprehensive prevention services, including condoms and pre-exposure prophylaxis (PrEP), remains uneven, particularly among key populations such as sex workers, men who have sex with men, and people who inject drugs. Additionally, gaps in testing coverage and linkage to care continue to hinder efforts to achieve epidemic control.\n\nConclusion: In conclusion, the modest decrease in new HIV infections overall and the significant reduction among children aged 0 to 14 in Kenya from 2019 to 2021 is a testament to the country’s commitment to ending the epidemic. However, sustained efforts are needed to address remaining challenges and achieve the goal of an AIDS-free generation. By continuing to prioritize evidence-based interventions, invest in health systems strengthening, and promote equity and inclusivity, Kenya can build on its progress and move closer to ending HIV/AIDS.\nSources:\n\nNational AIDS Control Council (NACC) Kenya. (2022). Kenya AIDS Strategic Framework 2021/22 - 2025/26. Retrieved from https://nacc.or.ke\nKenya Ministry of Health. (2021). Kenya HIV Estimates Report 2021. Retrieved from https://www.health.go.ke\n\nAttached is the r code used to generate.\n## packages\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(systemfonts)\nlibrary(camcorder)\n\n\ngg_record(dir = here::here(\"2024/03\"), device = \"png\", \n          width = 1500 * 2, height = 1350 * 2, units = \"px\", dpi = 320)\n\n\ntheme_set(theme_minimal(base_size = 19, base_family = \"Girassol\"))\n\ntheme_update(\n  text = element_text(color = \"grey12\"),\n  axis.title = element_blank(),\n  axis.text.x = element_text(family = \"Iosevka Curly\"),\n  axis.text.y = element_blank(),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor = element_blank(),\n  plot.margin = margin(20, 5, 10, 10),\n  plot.subtitle = element_textbox_simple(family = \"Roboto Condensed\", size = 14,\n                                         lineheight = 1.6),\n  plot.title.position = \"plot\",\n  plot.caption = element_text(family = \"Iosevka Curly\", color = \"#b40059\", hjust = .5,\n                              size = 10, margin = margin(35, 0, 0, 0))\n)\n\n# read the data\ndf &lt;- read_csv(\"2024/data/new_infected_data.csv\")\n\n#prep data\ndf_prep &lt;- df %&gt;%\n  pivot_longer(cols = c(-Newly_infected_with_HIV)) %&gt;% \n  rename(year = name, \n         n = value) %&gt;% \n  group_by(Newly_infected_with_HIV) %&gt;% \n  mutate(\n    total = sum(n),\n    current = n[which(year == 2021)]\n  ) %&gt;% \n  ungroup() %&gt;%\n  mutate(\n    Newly_infected_with_HIV = fct_reorder(Newly_infected_with_HIV, total),\n    Newly_infected_with_HIV = fct_relevel(Newly_infected_with_HIV, \"Young_people_(ages 15-24)\")\n  )\n\n\n# summary computation\ndf_sum &lt;-\n  df_prep %&gt;% \n  filter(year &lt;= 2021) %&gt;% \n  group_by(year) %&gt;% \n  summarize(n = sum(n))\n\n# plot\np1 &lt;- \n  df_sum %&gt;% \n  ggplot(aes(year, n)) +\n  geom_col(aes(fill = factor(year)), width = .85) +\n  geom_col(\n    data = df_prep %&gt;% filter(Newly_infected_with_HIV == \"Children_(ages 0-14)\" & year &lt;= 2021),\n    aes(alpha = year == 2021),\n    fill = \"blue\", width = .5\n  ) +\n  geom_text(\n    data = df_sum %&gt;% \n      mutate(n_lab = if_else(year %in% c(2012, 2021), paste0(n, \"\\nTotal\"), as.character(n))),\n    aes(label = n_lab), \n    family = \"Iosevka Curly\", size = 3.3, lineheight = .8, \n    nudge_y = 12, vjust = 0, color = \"black\", fontface = \"bold\"\n  ) +\n  geom_text(\n    data = df_prep %&gt;% filter(Newly_infected_with_HIV == \"Children_(ages 0-14)\" & year &lt;= 2021) %&gt;% \n      mutate(n_lab = if_else(year %in% c(2012, 2021), paste0(n, \"\\nChildren\"), as.character(n))), \n    aes(label = n_lab), \n    family = \"Iosevka Curly\",\n    color = \"white\", lineheight = .8, size = 3.0, \n    nudge_y = 12, vjust = 0, fontface = \"bold\"\n  ) +\n  geom_text(\n    data = df_prep %&gt;% filter(Newly_infected_with_HIV == \"Children_(ages 0-14)\" & year &lt;= 2021),\n    aes(y = -15, label = year, color = factor(year)), \n    family = \"Iosevka Curly\", size = 6, hjust = .5, vjust = 1\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_y_continuous(limits = c(-15, NA)) +\n  scale_color_manual(values = c(rep(\"black\", 9), \"#b40059\", \"grey70\"), guide = \"none\") +\n  scale_fill_manual(values = c(rep(\"purple\", 9), \"#b40059\", \"yellow\"), guide = \"none\") +\n  scale_alpha_manual(values = c(.25, .4), guide = \"none\") +\n  labs(title = \"&lt;br&gt;&lt;span style='font-size:20pt'&gt;New HIV infections among children in Kenya decreased by 13% in the &lt;b style='color:#b40059'&gt;Pandemic Year&lt;/b&gt;\",\n       subtitle = \"&lt;br&gt;&lt;span style='font-size:14pt'&gt;From 2019 to 2021, there was a 1.13% decrease in new HIV infections overall in Kenya, and a 16.13% decrease in new infections among children aged 0 to 14.\",\n       caption = paste(\"&lt;br&gt;&lt;span style='font-size:6pt'&gt;&lt;b style='color:black'&gt;Source: World Bank  \", \"      \", \"\\nGraphics: Victor Mandela\")) +\n  theme(\n    plot.title = element_markdown(size = 28, margin = margin(5, 35, 25, 35), color = \"black\"),\n    plot.subtitle = element_textbox_simple(margin = margin(5, 35, 15, 35)),\n    panel.grid.major = element_blank(),\n    axis.text.x = element_blank(),\n    plot.caption = element_markdown(hjust = 0.6, size = 9, lineheight = 0.8,\n                                    family = \"Charm\", face = \"bold\",\n                                    margin = margin(t = 8))\n  )\n\np1"
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html",
    "href": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html",
    "title": "Understanding Life Expectancy and Healthy Life Expectancy (HALE) in Morocco (2000-2018): USING DHS DATA",
    "section": "",
    "text": "Morocco has seen significant changes in both life expectancy and healthy life expectancy (HALE) between 2000 and 2018. While life expectancy at birth has risen to 74.5 years, the number of years lived in full health (HALE) has decreased. This shift presents important insights for researchers interested in the health outcomes of the Moroccan population.\nIn this blog post, we will discuss the definitions of life expectancy and HALE, examine their trends in Morocco, and highlight the importance of focusing on quality of life alongside longevity. Our analysis is based on data from Global Health Estimates and Demographic and Health Surveys (DHS).\n\n\nLife expectancy is the average number of years a newborn is expected to live, assuming current mortality rates remain constant throughout their lifetime. It is often used as a benchmark to measure the health and development of a population.\nHealthy life expectancy (HALE), on the other hand, measures the number of years an individual can expect to live in good health, free from serious illnesses and disabilities. HALE provides a more comprehensive picture of population health by accounting for both mortality and the burden of disease.\n\n\n\nFrom 2000 to 2018, Morocco experienced an impressive increase in life expectancy, reaching 74.5 years in 2018. Several factors have contributed to this improvement, including:\n\nEnhanced healthcare systems and increased access to medical services.\nImprovements in nutrition, education, and living conditions.\nA significant reduction in infant and child mortality rates.\n\nThese changes have extended the lifespan of the population, signaling overall progress in Morocco’s health outcomes. However, a deeper look into HALE reveals a more complex reality.\n\n\n\nWhile life expectancy has risen, HALE has seen a decline in Morocco. This suggests that although people are living longer, many of these additional years are spent living with illnesses or disabilities. Chronic diseases, such as cardiovascular diseases, diabetes, and other non-communicable diseases (NCDs), have become more prevalent, affecting the quality of life during these later years.\nThis trend highlights a growing gap between total life expectancy and the number of years lived in good health. The burden of disease, especially NCDs, is a significant contributor to the reduction in HALE. As life expectancy increases, public health policies must focus on managing chronic illnesses to improve overall quality of life.\n\n\n\nThe life expectancy and HALE data for Morocco were sourced from the Global Health Estimates (GHE) and analyzed using standard statistical methods. HALE is calculated by adjusting life expectancy to account for years lived with illness or disability. This calculation provides a more accurate reflection of population health compared to life expectancy alone.\nDemographic and Health Surveys (DHS) also offer valuable insights into Morocco’s health trends. DHS data on maternal and child health, disease prevalence, and healthcare access play a crucial role in understanding how HALE is affected by various health determinants."
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#introduction",
    "href": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#introduction",
    "title": "Understanding Life Expectancy and Healthy Life Expectancy (HALE) in Morocco (2000-2018): USING DHS DATA",
    "section": "",
    "text": "Morocco has seen significant changes in both life expectancy and healthy life expectancy (HALE) between 2000 and 2018. While life expectancy at birth has risen to 74.5 years, the number of years lived in full health (HALE) has decreased. This shift presents important insights for researchers interested in the health outcomes of the Moroccan population.\nIn this blog post, we will discuss the definitions of life expectancy and HALE, examine their trends in Morocco, and highlight the importance of focusing on quality of life alongside longevity. Our analysis is based on data from Global Health Estimates and Demographic and Health Surveys (DHS).\n\n\nLife expectancy is the average number of years a newborn is expected to live, assuming current mortality rates remain constant throughout their lifetime. It is often used as a benchmark to measure the health and development of a population.\nHealthy life expectancy (HALE), on the other hand, measures the number of years an individual can expect to live in good health, free from serious illnesses and disabilities. HALE provides a more comprehensive picture of population health by accounting for both mortality and the burden of disease.\n\n\n\nFrom 2000 to 2018, Morocco experienced an impressive increase in life expectancy, reaching 74.5 years in 2018. Several factors have contributed to this improvement, including:\n\nEnhanced healthcare systems and increased access to medical services.\nImprovements in nutrition, education, and living conditions.\nA significant reduction in infant and child mortality rates.\n\nThese changes have extended the lifespan of the population, signaling overall progress in Morocco’s health outcomes. However, a deeper look into HALE reveals a more complex reality.\n\n\n\nWhile life expectancy has risen, HALE has seen a decline in Morocco. This suggests that although people are living longer, many of these additional years are spent living with illnesses or disabilities. Chronic diseases, such as cardiovascular diseases, diabetes, and other non-communicable diseases (NCDs), have become more prevalent, affecting the quality of life during these later years.\nThis trend highlights a growing gap between total life expectancy and the number of years lived in good health. The burden of disease, especially NCDs, is a significant contributor to the reduction in HALE. As life expectancy increases, public health policies must focus on managing chronic illnesses to improve overall quality of life.\n\n\n\nThe life expectancy and HALE data for Morocco were sourced from the Global Health Estimates (GHE) and analyzed using standard statistical methods. HALE is calculated by adjusting life expectancy to account for years lived with illness or disability. This calculation provides a more accurate reflection of population health compared to life expectancy alone.\nDemographic and Health Surveys (DHS) also offer valuable insights into Morocco’s health trends. DHS data on maternal and child health, disease prevalence, and healthcare access play a crucial role in understanding how HALE is affected by various health determinants."
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#key-insights",
    "href": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#key-insights",
    "title": "Understanding Life Expectancy and Healthy Life Expectancy (HALE) in Morocco (2000-2018): USING DHS DATA",
    "section": "Key Insights :",
    "text": "Key Insights :\n\nRising Life Expectancy: Morocco’s life expectancy increased steadily from 2000 to 2018, reaching 74.5 years, showcasing overall progress in health and longevity.\nDecreasing HALE: The decline in HALE underscores the need to focus not just on extending life but on improving the quality of those additional years.\nChronic Disease Burden: With the rise of NCDs, Morocco faces challenges in ensuring that its population can live longer, healthier lives. Addressing chronic illness is crucial to closing the gap between life expectancy and HALE.\nData-Driven Public Health: Utilizing high-quality data from sources like Global Health Estimates and DHS can inform policies aimed at enhancing healthcare systems and managing the burden of chronic diseases.\n\n\nHow to Analyze HALE and Life Expectancy\nFor data enthusiasts and researchers, analyzing HALE and life expectancy trends over time can be achieved through various statistical tools. In this blog, we’ll provide an example analysis code using R to visualize these trends. The code will allow you to compare life expectancy at birth and HALE over time, shedding light on the growing disparity between longevity and healthy living years.\nGetting the data\n# Fetching the data\nmorocco_df &lt;- readr::read_csv(\"https://data.humdata.org/dataset/1f98948b-5c63-48c6-a92c-44cf2cec1e9f/resource/11c05ae5-ce13-4cbb-a888-f2175bb5266c/download/global_health_estimates_life_expectancy_and_leading_causes_of_death_and_disability_indicators_ma.csv\")\n\n\ntrans_morocco &lt;- morocco_df %&gt;% \n  filter(`DIMENSION (NAME)` == \"Female\") %&gt;% #choose females\n  select(`GHO (DISPLAY)`, `YEAR (DISPLAY)`, Numeric) %&gt;% # choose columns\n  group_by(`GHO (DISPLAY)`) %&gt;% \n  add_count(`YEAR (DISPLAY)`) %&gt;% #introduce a counting column\n    filter(n == 1) %&gt;% \n  pivot_wider(names_from = `GHO (DISPLAY)`,\n              values_from = Numeric) %&gt;% #change to wide format\n  arrange(`YEAR (DISPLAY)`) %&gt;% \n  drop_na() %&gt;% \n  select(1, 4, 5, 12, 13) # select the columns of interest\n\n#write_csv(trans_morocco, \"morocco_life_expectancy.csv\")\n#You can write the csv file to export to an external viz tool\n#I used datawrapper"
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#conclusion",
    "href": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#conclusion",
    "title": "Understanding Life Expectancy and Healthy Life Expectancy (HALE) in Morocco (2000-2018): USING DHS DATA",
    "section": "Conclusion",
    "text": "Conclusion\nThe health improvements in Morocco over the past two decades are undeniable. Life expectancy has reached its highest point, but the decline in HALE raises questions about how long Moroccans are able to live in good health. As chronic diseases continue to affect the population, policymakers must shift their focus from merely extending life to improving the quality of those extra years.\nBy addressing these health challenges, Morocco can ensure that its aging population enjoys not just longer lives, but healthier ones too. For researchers and public health professionals, HALE provides a valuable metric that complements life expectancy, offering a fuller picture of a nation’s health."
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#references",
    "href": "posts/Monitoring_and_Evaluation_morocco_life_expectancy_HALE/index.html#references",
    "title": "Understanding Life Expectancy and Healthy Life Expectancy (HALE) in Morocco (2000-2018): USING DHS DATA",
    "section": "References",
    "text": "References\n\nWorld Health Organization (WHO). Global Health Estimates: Life Expectancy and Healthy Life Expectancy (HALE). Available at: WHO GHE\nDemographic and Health Surveys (DHS). Health data for Morocco. Available at: DHS Morocco\nInstitute for Health Metrics and Evaluation (IHME). Global Burden of Disease study. Available at: IHME\n\nFurther Reading and Data Resources (H2): For more in-depth data analysis and information on life expectancy and health outcomes, explore the following resources:\n\nWorld Bank Open Data: Country-specific health and development indicators.\nOur World in Data: Visualizations on global health, life expectancy, and disease burden.\nThe Lancet Global Health: Research on health trends and life expectancy in different regions."
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_Monitoring/index.html",
    "href": "posts/Monitoring_and_Evaluation_Monitoring/index.html",
    "title": "How to Build a Monitoring Plan for Your Project",
    "section": "",
    "text": "Once you’ve laid the foundation with a strong Research, Monitoring, and Evaluation (M&E) framework, the next step is to create a Monitoring Plan. This plan ensures you track the progress of your project in real time, helping you stay on course toward achieving your objectives. In this post, we’ll break down the steps to build a simple, effective Monitoring Plan, with examples to make it easier to follow.\n\n1. Identify What You Want to Monitor\nMonitoring helps you check if your project is going as planned. You don’t need to monitor everything—focus on what’s most important. These can be the key activities, outcomes, and outputs you outlined in your M&E framework.\nFor example, if you’re running a project to improve access to clean water in a community, you might want to monitor:\n\nActivities: Installation of water pumps.\nOutputs: Number of water pumps installed.\nOutcomes: Number of households using clean water.\nImpact: Reduction in waterborne diseases in the community.\n\nThe idea is to choose indicators that will tell you whether you’re making progress toward your goals.\n\n\n2. Define Your Data Collection Methods\nOnce you know what to monitor, decide how you’ll collect the information. The method you choose depends on what you’re measuring, but here are some simple examples:\n\nSurveys: If you want to monitor household use of clean water, you could conduct household surveys asking how frequently they use the new water pumps.\nObservation: You can visit project sites and observe how the water pumps are being used and maintained.\nRecords and Reports: For outputs like “number of water pumps installed,” you can keep a record of the installations as they happen.\n\nIt’s important to choose methods that are easy and practical for your team to carry out. If your project is large, you might need to set up mobile data collection tools like ODK or KoboCollect to streamline the process.\n\n\n3. Set a Monitoring Schedule\nMonitoring needs to happen regularly to keep your project on track. But how often should you collect data? This will depend on the type of project and the indicators you’re tracking.\nFor example:\n\nDaily or Weekly: You might want to track how many water pumps are being installed on a weekly basis. This helps to spot any delays early and adjust the plan.\nMonthly: If you’re looking at outcomes like household use of clean water, you might conduct monthly surveys to see if more people are benefiting from the project.\nQuarterly or Annually: For impact-level indicators, such as the reduction in waterborne diseases, quarterly or annual monitoring might be more appropriate since these changes take time.\n\nCreating a monitoring calendar can help your team stay on top of data collection. This schedule will tell them what to collect, when to collect it, and who is responsible.\n\n\n4. Assign Responsibilities\nClear roles and responsibilities are crucial to ensure everyone knows what they’re supposed to do. Depending on the size of your team, you might have:\n\nData Collectors: These are the people gathering the information. For example, local community volunteers or field officers might be responsible for visiting households to collect data.\nData Managers: They ensure that the collected data is entered into a database or monitoring system accurately and on time.\nProject Managers: The person or team responsible for reviewing the data regularly and making decisions based on the findings.\n\nLet’s say you’re monitoring the installation of water pumps. A field officer might report every week on how many pumps have been installed, while the project manager checks the progress and adjusts the plan if needed.\n\n\n5. Analyze and Use the Data\nOnce you’ve gathered the data, it’s time to make sense of it. Ask yourself:\n\nIs the project on track? For example, if you planned to install 20 water pumps by mid-year and you’ve only installed 5, it’s clear something needs to change.\nAre we seeing the expected results? If the outcome you wanted was increased household access to clean water, but only a few households are using the pumps, you’ll need to investigate why. Maybe the pumps are not located in convenient places, or people need more education on how to use them.\n\nData analysis doesn’t have to be complex. Simple comparisons of actual results against your targets can give you valuable insights.\n\n\nFinal Thoughts\nA good Monitoring Plan ensures that you’re not flying blind. It helps you see if your project is progressing as expected or if adjustments need to be made. By identifying key things to monitor, choosing the right data collection methods, setting a schedule, and assigning roles, you can keep your project on track and achieve the results you’re aiming for."
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_Ethiopia_Child_Stanted_health/index.html",
    "href": "posts/Monitoring_and_Evaluation_Ethiopia_Child_Stanted_health/index.html",
    "title": "Understanding Child Stunting in Ethiopia: Trends, Impacts, and Future Directions: USING DHS DATA",
    "section": "",
    "text": "Introduction\nEthiopia has made significant strides in improving child nutrition over the past two decades. However, child stunting remains a pressing issue, with many regions still facing alarming rates. This blog post analyzes the stunting rates across various Ethiopian regions from 2000 to 2019, highlighting the progress made, the persistent challenges, and the implications for donors and investors looking to improve child health outcomes.\n\n\nKey Definitions\n\nChild Stunting: A form of chronic malnutrition characterized by low height for age. Stunting is a critical indicator of a child’s health and nutritional status and can have long-term impacts on cognitive development and economic productivity.\nMalnutrition: Refers to deficiencies, excesses, or imbalances in a person’s intake of energy and/or nutrients. It encompasses undernutrition (including stunting, wasting, and underweight) as well as overnutrition (obesity).\nDemographic and Health Surveys (DHS): Nationally representative surveys conducted in many countries to collect data on health and nutrition, providing essential insights for policymakers and stakeholders.\n\n\n\nWhy Should You Care?\nChild stunting is not just a personal health issue; it has far-reaching implications for communities and nations. High rates of stunting can lead to:\n\nEconomic Consequences: Malnourished children are more likely to underperform in school and have lower productivity as adults, which can hinder national development.\nHealthcare Burden: Increased stunting rates correlate with higher healthcare costs due to related health complications.\nIntergenerational Effects: Poor nutrition can affect not just the current generation but also future generations, perpetuating cycles of poverty and malnutrition.\n\nAddressing child stunting is crucial for achieving sustainable development goals, particularly those related to health, education, and poverty reduction.\n\n\nTrends in Child Stunting Rates\n\nBased on data from the DHS, the stunting rates of children under five in Ethiopia have shown varying trends across regions. Below is a summary of the key findings:\n\nTigray: Stunting rates decreased from 61.2% in 2000 to 48.4% in 2019, indicating progress but still reflecting high levels of chronic malnutrition.\nAfar: A reduction from 53.1% to 42.2% showcases improvements, yet the region faces significant environmental challenges.\nAmhara: Although stunting rates dropped from 63% to 41.5%, the high prevalence highlights ongoing nutritional concerns.\nOromia: This region improved from 53.9% to 35.3%, signaling effective interventions but still necessitating further efforts.\nSomali: A notable decline from 51.8% to 30.6% demonstrates progress, but vulnerability to food insecurity persists.\nBenishangul-Gumuz: Stunting decreased from 49.7% to 40.7%, indicating a need for targeted interventions in remote areas.\nSNNPR: A drop from 60.8% to 36.4% reflects significant improvements, but further action is essential to address diverse needs.\nGambela: This region saw a remarkable decline from 41.7% to 17.3%, showcasing the potential for successful interventions.\nHarari: Stunting fell from 42.1% to 36.4%, signaling a need for urban-based nutrition strategies.\nAddis Ababa: Stunting rates dropped from 32.9% to 15%, indicating positive trends in urban health initiatives.\nDire Dawa: A slight decrease from 34.3% to 25.4% suggests persistent urban challenges related to food security.\n\n\n\nImplications for Donors and Investors\nThe findings underscore a dual narrative: while there has been substantial progress in reducing stunting rates across many regions, significant challenges remain in others. Donors and investors can play a crucial role in supporting initiatives that target both immediate and long-term solutions to child malnutrition. Here are some key messages for consideration:\n\nTargeted Interventions: Regions like Tigray and Amhara require focused nutritional programs to combat high stunting rates. Investments in health and nutrition initiatives can yield significant benefits for child development.\nClimate Resilience: Areas like Afar and Somali highlight the need for investments in climate-resilient agriculture and food systems to mitigate the impact of environmental shocks on food security.\nUrban Strategies: The challenges faced by urban areas like Dire Dawa and Harari necessitate tailored approaches to address nutrition and food access in densely populated settings.\nReplication of Success: The dramatic reduction in stunting rates in Gambela serves as a model for what can be achieved with sustained support and targeted interventions. Donors can explore strategies to replicate these successes in other vulnerable regions.\n\n\n\nData Analysis Code in R\nTo analyze stunting rates in Ethiopia using DHS data follow the following R code:\n#Load library\nlibrary(tidyverse)\n\n# Loading the data\neth_food &lt;- readr::read_csv(\"https://data.humdata.org/dataset/6e5d9d29-656e-40ec-9d42-e827777a94ad/resource/517a9d11-7e4b-4e0f-be29-5e9f8429345b/download/select-nutrition-indicators_subnational_eth.csv\")\n\n#Transform the data\ntrans_eth &lt;- eth_food %&gt;% \n  select(2, 4, 5,9) %&gt;% \n  filter(Indicator == \"Children stunted\") %&gt;% # Children stunted data\n  select(-Indicator) %&gt;% \n  group_by(SurveyYear) %&gt;% \n  pivot_wider(names_from = Location,\n              values_from = Value) \n \n# do not run this part -- used in the datawrapper app for viz\n#write_csv(trans_eth, \"ethiopia_children_stunted_wasted.csv\")\n\n\nData Source\nThis analysis utilizes data from the Demographic and Health Surveys (DHS), which provides comprehensive data on health and nutrition indicators across various countries, including Ethiopia.\n\n\nConclusion\nWhile Ethiopia has made remarkable progress in reducing child stunting, ongoing challenges must be addressed to sustain and accelerate these gains. By investing in targeted interventions and promoting climate resilience, donors can help ensure that every child in Ethiopia has the opportunity for a healthy start in life. Addressing child stunting is not only a moral imperative but also a strategic investment in the future of the nation."
  },
  {
    "objectID": "posts/MDPI_SPSS/index.html",
    "href": "posts/MDPI_SPSS/index.html",
    "title": "Unlocking Insights: A Step-by-Step Guide to Calculating Multidimensional Poverty Index (MPI) Using SPSS",
    "section": "",
    "text": "A Step-by-Step Guide to Calculating the Multidimensional Poverty Index (MPI) Using SPSS\nIn poverty research, the Multidimensional Poverty Index (MPI) is a crucial tool for measuring poverty beyond income, capturing deprivations across multiple dimensions such as education and living standards. This blog outlines the exact SPSS code to compute MPI using specific survey indicators.\nLet’s break down the process step by step:\n\nStep 1: Compute the MPI Indicators\nMPI is calculated based on two dimensions—Education and Living Standards—with specific indicators for each. Here’s how you can compute the binary deprivation indicators for each component:\n\nEducation Indicators (Weight: 1/4 for each)\n\nYears of Schooling Completed\nCOMPUTE FIVE_YRS_SCHOOLING_COMPLETED = Q403b &lt; 5.\nSchool-Aged Child Enrollment\nCOMPUTE SCHOOL_AGED_CHILD_NOT_ENROLLED_IN_SCHOOL = Q404 = 1.\n\n\n\nLiving Standards Indicators (Weight: 1/12 for each)\n\nNo Electricity\nCOMPUTE No_electricity = Q409_1 = 0.\nAccess to Clean Drinking Water\nCOMPUTE ACCESS_TO_CLEAN_DRINKING_WATER = Q410 = 6 | Q410 = 7 | Q410 = 8 | Q410 = 10 | Q410 = 11 | Q410 = 12.\nAccess to Improved Sanitation\nCOMPUTE ACCESS_TO_IMPROVED_SANITATION = Q413 = 10 | Q413 = 11 | Q413 = 12 | Q413 = 13.\nDirt Floor\nCOMPUTE DIRT_FLOOR = Q407 = 1 | Q407 = 2.\nDirty Cooking Fuel\nCOMPUTE DIRTY_COOKING_FUEL = Q412 = 7 | Q412 = 8 | Q412 = 9 | Q412 = 10 | Q412 = 11 | Q412 = 12.\nAsset Holding\nCOMPUTE ASSET_HOLDING = (Q409_2 = 0 & Q409_3 = 0 & Q409_4 = 0 & Q409_6 = 0 & Q409_7 = 0 & Q409_8 = 0 & Q409_9 = 0) & (Q409_11 = 0 | Q409_12 = 0).\n\n\n\n\n\nStep 2: Weighting the Indicators\nAfter computing the individual indicators, apply the appropriate weights:\n\nEducation Indicators: Weight = 1/4\nLiving Standard Indicators: Weight = 1/12\n\nExample for the Years of Schooling Completed:\nCOMPUTE FIVE_YRS_SCHOOLING_COMPLETED_weighted = FIVE_YRS_SCHOOLING_COMPLETED * EDUCATION_WGHT.\nSimilarly, compute the weighted versions of all other indicators for education and living standards.\n\n\nStep 3: Calculate the Deprivation Score\nThe deprivation score is a sum of the weighted indicators, with scores ranging from 0 to 1, where 1 represents the highest deprivation.\nSyntax:\nCOMPUTE DEPRIVATION_SCORE = FIVE_YRS_SCHOOLING_COMPLETED_weighted + SCHOOL_AGED_CHILD_NOT_ENROLLED_IN_SCHOOL_weighted + No_electricity_WEIGHTED + ACCESS_TO_CLEAN_DRINKING_WATER_weighted + ACCESS_TO_IMPROVED_SANITATION_weighted + DIRT_FLOOR_weighted + DIRTY_COOKING_FUEL_weighted + ASSET_HOLDING_WEIGHTED.\n\n\n\nStep 4: MPI Classification\nFinally, classify households based on their deprivation scores:\n\nNon-Poor: 0-0.19\nVulnerable to Poverty: 0.20-0.33\nMultidimensionally Poor: 0.34-0.50\nSeverely Poor: 0.51+\n\nSyntax:\nRECODE DEPRIVATION_SCORE (0 thru 0.1999=1) (0.2000 thru 0.3333=2) (0.3334 thru 0.5000=3) (0.5001 thru Highest=4) INTO MPI_POVERTY_STATUS2.\n\n\nConclusion\nBy following these steps and using the corresponding SPSS code, you can efficiently compute the Multidimensional Poverty Index (MPI) and gain meaningful insights into poverty beyond income-based measures.\nMPI helps identify vulnerable populations, prioritize areas of intervention, and assess the overall well-being of households. If you’re interested in further understanding poverty data and the SPSS process, this guide serves as a comprehensive starting point!"
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "",
    "text": "Cover picture courtesy of Blessman International"
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#variable-definitions",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#variable-definitions",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "1. Variable Definitions",
    "text": "1. Variable Definitions\nIn designing the evaluation framework for the School Feeding Program (SFP), it is essential to define variables that align with the program’s goals and context. Below, we outline the variables categorized into Outcome Variables, Control Variables, and Other Variables, ensuring they mirror the structure of a health program evaluation dataset.\n\nOutcome Variable\nThis variable measures the primary goal of the SFP: improving school attendance.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nattendance_rate\nAverage student attendance rate per school term (percent)\n\n\n\n\n\nControl Variables\nThese variables account for household and socio-economic factors that might influence school attendance or participation in the program.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nage_hh\nAge of the head of the household (in years)\n\n\nage_sp\nAge of the spouse (in years)\n\n\neduc_hh\nEducation of the head of the household (completed years of schooling)\n\n\neduc_sp\nEducation of the spouse (completed years of schooling)\n\n\nfemale_hh\nHead of the household is a woman (0=no, 1=yes)\n\n\nindigenous\nHead of household speaks an indigenous language (0=no, 1=yes)\n\n\nhhsize\nNumber of household members (baseline)\n\n\ndirtfloor\nHome has a dirt floor at baseline (0=no, 1=yes)\n\n\nbathroom\nHome with private bathroom at baseline (0=no, 1=yes)\n\n\nland\nNumber of hectares of land owned by household at baseline\n\n\n\n\n\nOther Variables\nThese variables define the experimental design, program eligibility, and participation details.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nlocality_identifier\nLocality identifier\n\n\nhousehold_identifier\nUnique household identifier\n\n\ntreatment_locality\nSchool is in a locality with the feeding program (0=no, 1=yes)\n\n\npromotion_locality\nSchool is in a locality where the feeding program was promoted (0=no, 1=yes)\n\n\neligible\nHousehold is eligible for the feeding program (0=no, 1=yes)\n\n\nenrolled\nChild is enrolled in the feeding program (0=no, 1=yes)\n\n\nenrolled_rp\nChild enrolled in the feeding program under the random promotion scenario (0=no, 1=yes)\n\n\npoverty_index\nPoverty Index 1-100\n\n\nround\nSurvey round (0=baseline; 1=follow-up)\n\n\nhospital\nHH member visited hospital in the past year (0=no, 1=yes)\n\n\n\nThis comprehensive set of variables enables a detailed analysis of the program’s impact while controlling for household-level differences and program design elements. By aligning the variable structure with the program’s objectives, we can effectively measure the success of the intervention and uncover insights for future implementation."
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#causal-inference-and-counterfactuals",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#causal-inference-and-counterfactuals",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "2. Causal Inference and Counterfactuals",
    "text": "2. Causal Inference and Counterfactuals\nCausal inference is the process of determining whether a program or intervention (like the School Feeding Program) directly causes a change in an outcome, such as improved school attendance. It goes beyond simple associations to uncover cause-and-effect relationships by comparing what happened with what could have happened if the program had not been implemented.\nThe idea of counterfactuals lies at the heart of causal inference. A counterfactual refers to the hypothetical scenario of what would have occurred in the absence of the program. Since we cannot observe both the actual outcome and the counterfactual for the same household, researchers rely on rigorous study designs (such as randomized control trials) or statistical techniques to estimate the counterfactual and isolate the program’s impact.\n\nBefore-After Designs\nThe first “expert” consultant you hire suggests that to estimate the impact of the School Feeding Program (SFP), you should calculate the change in student attendance rates over time for the schools where households enrolled in the program. The consultant argues that because SFP provides meals that alleviate hunger and improve student focus, any increase in attendance rates over time can be attributed to the program’s effect.\nUsing the subset of schools in treatment localities, you calculate their average student attendance rates before the implementation of the program and then again two years later. The analysis focuses on comparing the average attendance rates at baseline and follow-up to assess the program’s impact in villages participating in the School Feeding Program.\n\nm_ba1 &lt;- lm_robust(attendance_rate ~ round, \n                   clusters = locality_identifier,\n                   data = trans_df %&gt;% filter(treatment_locality==1 & enrolled ==1))\n\n\nm_ba2 &lt;- lm_robust(attendance_rate ~ round + age_hh + age_sp + educ_hh + \n                     educ_sp + female_hh + indigenous + hhsize + dirtfloor + \n                     bathroom + land + school_distance, \n                   clusters = locality_identifier,\n                   data = trans_df %&gt;% filter(treatment_locality==1 & enrolled ==1))\n\nt0 &lt;- tbl_regression(m_ba1, intercept = T)\nt01 &lt;- tbl_regression(m_ba2, intercept = T)\n\ntbl_merge_m_ba &lt;-\n  tbl_merge(\n    tbls = list(t0, t01),\n    tab_spanner = c(\"No Controls\", \"With Controls\")\n  )\n\ntbl_merge_m_ba\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Controls\n\n\nWith Controls\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n88\n87, 88\n&lt;0.001\n79\n78, 80\n&lt;0.001\n\n\nround\n5.7\n5.3, 6.1\n&lt;0.001\n5.7\n5.3, 6.1\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.05\n-0.07, -0.03\n&lt;0.001\n\n\nage_sp\n\n\n\n\n\n\n0.00\n-0.03, 0.02\n&gt;0.9\n\n\neduc_hh\n\n\n\n\n\n\n-0.05\n-0.11, 0.01\n0.079\n\n\neduc_sp\n\n\n\n\n\n\n0.07\n0.00, 0.13\n0.038\n\n\nfemale_hh\n\n\n\n\n\n\n-0.86\n-1.4, -0.28\n0.004\n\n\nindigenous\n\n\n\n\n\n\n1.7\n1.3, 2.1\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.5\n1.5, 1.6\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n2.0\n1.7, 2.3\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.31\n-0.60, -0.02\n0.039\n\n\nland\n\n\n\n\n\n\n-0.08\n-0.13, -0.04\n&lt;0.001\n\n\nschool_distance\n\n\n\n\n\n\n0.00\n0.00, 0.01\n0.14\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nDoes the before-and-after comparison control for all the factors that affect student attendance over time?\nNo, it is unlikely that this analysis accounts for all the factors influencing attendance. For instance, there could be other educational or health-related interventions occurring simultaneously in the communities receiving the School Feeding Program (SFP), which might also contribute to changes in attendance. Additionally, external factors like a regional economic crisis or natural disasters could have independently affected attendance. In the absence of SFP, attendance might have increased or decreased due to these factors, making it challenging to attribute all observed changes solely to the program.\nBased on these results produced by the before-and-after analysis, should SFP be scaled up nationally?\nNo, based on the current results, scaling up the program nationally might not be justified yet. While the School Feeding Program appears to have improved average attendance rates, the increase of 5.7 percentage points may not be sufficient to meet the government’s threshold for program effectiveness. Moreover, without understanding the contribution of confounding factors, it remains unclear whether the observed improvements are entirely due to the program.\n\n\nEnrolled vs. Non-Enrolled\nAnother consultant proposes a different approach, suggesting it would be more appropriate to estimate the counterfactual in the post-intervention period, two years after the program’s start. The consultant correctly notes that of the 5,929 households in the baseline sample, only 2,907 enrolled in the School Feeding Program (SFP), leaving approximately 51 percent of households without access to SFP.\nThe consultant argues that all schools within the 100 pilot villages were eligible to enroll in the program, with households in these communities sharing similar characteristics. For example, households rely on comparable school infrastructures, face similar regional conditions, and have children subject to the same school policies. Furthermore, economic activities and living standards within these localities are generally uniform.\nThe consultant asserts that under such circumstances, attendance rates for households not enrolled in SFP after the intervention can reasonably estimate the counterfactual outcomes for those enrolled. Consequently, you decide to compare average student attendance rates in the post-intervention period for both groups—schools participating in the School Feeding Program and those that opted out.\n\nm_ene1 &lt;- lm_robust(attendance_rate ~ enrolled, \n                    clusters = locality_identifier,\n                    data = trans_df %&gt;% filter(treatment_locality==1 & round ==1))\n\nm_ene2 &lt;- lm_robust(attendance_rate ~ enrolled + age_hh + age_sp + educ_hh + \n                      educ_sp + female_hh + indigenous + hhsize + dirtfloor + \n                      bathroom + land + school_distance, \n                    clusters = locality_identifier,\n                    data = trans_df %&gt;% filter(treatment_locality==1 & round ==1))\n\nt0a &lt;- tbl_regression(m_ene1, intercept = T)\nt0a1 &lt;- tbl_regression(m_ene2, intercept = T)\n\ntbl_merge_m_ene &lt;-\n  tbl_merge(\n    tbls = list(t0a, t0a1),\n    tab_spanner = c(\"No Controls\", \"With Controls\")\n  )\n\ntbl_merge_m_ene\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Controls\n\n\nWith Controls\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n81\n80, 81\n&lt;0.001\n74\n72, 76\n&lt;0.001\n\n\nenrolled\n12\n12, 13\n&lt;0.001\n8.5\n8.0, 9.1\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.08\n-0.12, -0.04\n&lt;0.001\n\n\nage_sp\n\n\n\n\n\n\n0.04\n0.00, 0.09\n0.045\n\n\neduc_hh\n\n\n\n\n\n\n0.04\n-0.06, 0.14\n0.5\n\n\neduc_sp\n\n\n\n\n\n\n0.10\n-0.02, 0.22\n0.088\n\n\nfemale_hh\n\n\n\n\n\n\n-0.50\n-1.6, 0.58\n0.4\n\n\nindigenous\n\n\n\n\n\n\n1.7\n0.84, 2.5\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.7\n1.6, 1.8\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n1.7\n1.2, 2.3\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.53\n-1.1, 0.00\n0.052\n\n\nland\n\n\n\n\n\n\n0.01\n-0.09, 0.11\n0.8\n\n\nschool_distance\n\n\n\n\n\n\n0.01\n-0.01, 0.02\n0.3\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nDoes this analysis likely control for all the factors that determine differences in student attendance between the enrolled and non-enrolled groups?\nNo, it is unlikely that the multivariate analysis fully controls for all the factors that influence the difference in attendance rates between the two groups. There could be unobservable factors that contribute to why some schools enroll in the feeding program while others do not. For instance, household preferences, school engagement levels, or the motivation of parents could play a role in determining which schools opt for the program. These factors may not be fully captured in the analysis.\nBased on these results produced by the enrolled vs. non-enrolled method, should the School Feeding Program (SFP) be scaled up nationally?\nBased strictly on the estimate from the multivariate linear regression, the SFP should not be scaled up nationally based on the findings here. The program increased average student attendance by 8.5%, which is a positive but modest improvement. While this result is statistically significant (p-value &lt; 0.001), it is lower than the expected national threshold improvement in attendance, suggesting that scaling up the program may not immediately achieve the desired outcomes at a larger scale. However, the modest effect size means that further investigation into the program’s impact across different contexts and regions is necessary to determine if it could contribute meaningfully to national efforts in improving student attendance."
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#randomized-assignment",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#randomized-assignment",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "3. Randomized Assignment",
    "text": "3. Randomized Assignment\nRandom Assignment in the context of our school feeding program evaluation means that households or communities are randomly assigned to either the treatment group (where they receive the feeding program) or the control group (where they do not). This random assignment ensures that every participant has an equal chance of being placed in either group, making it more likely that the groups are similar at the start of the study. As a result, any differences in outcomes, such as changes in children’s health or learning outcomes, can be attributed to the school feeding program itself, rather than other external factors. This method strengthens the validity of our findings and helps ensure that the observed impacts are genuinely due to the program intervention.\nThe key is to find a group of villages that are very similar to the 100 treatment villages, except for the fact that one group participated in the school feeding program and the other did not. Since the treatment villages were randomly selected from the pool of rural villages, they should, on average, have similar characteristics to those villages that did not participate in the program.\nTo improve the counterfactual estimate, we utilize an additional 100 rural villages that were not part of the feeding program. These comparison villages were also randomly selected, ensuring that they share similar characteristics with the treatment villages at the outset of the program. The random assignment of the program ensures that any differences in outcomes (e.g., improvements in children’s nutrition or learning) between the treatment and comparison villages can be attributed to the program, not external factors.\nTo validate this assumption, we would need to test whether the characteristics of eligible households in both the treatment and comparison villages were similar at the baseline, ensuring that no major differences existed before the program began. If the characteristics are similar, it further supports the idea that the program’s effects are due to the intervention itself rather than other external factors.\n\ndf_elig &lt;- trans_df %&gt;%\n  filter(eligible == 1) \n\ndf_elig %&gt;% \n  filter(round == 0) %&gt;%\n  dplyr::select(treatment_locality, locality_identifier,\n                age_hh, age_sp, educ_hh, educ_sp, female_hh, indigenous, \n                hhsize, dirtfloor, bathroom, land, school_distance) %&gt;%\n  tidyr::pivot_longer(-c(\"treatment_locality\",\"locality_identifier\")) %&gt;%\n  group_by(name) %&gt;%\n  do(tidy(lm_robust(value ~ treatment_locality, data = .))) %&gt;%\n  filter(term == \"treatment_locality\") %&gt;%\n  dplyr::select(name, estimate, std.error, p.value) %&gt;%\n  kable()\n\n\n\n\nname\nestimate\nstd.error\np.value\n\n\n\n\nage_hh\n-0.6354625\n0.3759583\n0.0910361\n\n\nage_sp\n-0.0386302\n0.3120790\n0.9014911\n\n\nbathroom\n0.0149907\n0.0132340\n0.2573724\n\n\ndirtfloor\n-0.0129497\n0.0118744\n0.2755159\n\n\neduc_hh\n0.1607976\n0.0697576\n0.0211978\n\n\neduc_sp\n0.0289107\n0.0670018\n0.6661273\n\n\nfemale_hh\n-0.0041155\n0.0070493\n0.5593691\n\n\nhhsize\n0.0596953\n0.0530454\n0.2604833\n\n\nindigenous\n0.0091048\n0.0131969\n0.4902756\n\n\nland\n-0.0402168\n0.0704607\n0.5681787\n\n\nschool_distance\n2.9087631\n1.1323148\n0.0102288\n\n\n\n\n\nThe average characteristics of households in both the treatment and comparison villages appear very similar. Among the various variables tested, the only statistically significant differences are in the number of years of education of the head of household and the distance to the nearest school, which are relatively small in magnitude. Specifically, the difference in the education of the household head is 0.16 years (which is less than 6% of the average years of education in the comparison group), and the difference in the distance to school is 2.91 kilometers (less than 3% of the comparison group’s average distance). These differences are statistically significant, but small, indicating that the two groups are quite similar in terms of key demographic factors.\nEven in a randomized experiment involving a large sample, small differences can occur by chance due to the nature of statistical tests. In fact, using a typical 5% significance level, we would expect some differences in around 5% of the characteristics simply due to random variability. Therefore, although small statistically significant differences exist, the overall similarity between the two groups suggests that the random assignment was effective and that the treatment and comparison groups are comparable for the evaluation of the feeding program’s impact.\nEstimate the average attendance rate for eligible households in the treatment and comparison villages for each period. What is the impact of the program?\n\nout_round0 &lt;- lm_robust(attendance_rate ~ treatment_locality,\n                        data = df_elig %&gt;% filter(round == 0),\n                        clusters = locality_identifier)\nout_round1 &lt;- lm_robust(attendance_rate ~ treatment_locality,\n                        data = df_elig %&gt;% filter(round == 1),\n                        clusters = locality_identifier)\n\nt0b &lt;- tbl_regression(out_round0, intercept = T)\nt0b1 &lt;- tbl_regression(out_round1, intercept = T)\n\ntbl_merge_m_ba1 &lt;-\n  tbl_merge(\n    tbls = list(t0b, t0b1),\n    tab_spanner = c(\"Baseline\", \"Follow Up\")\n  )\n\ntbl_merge_m_ba1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBaseline\n\n\nFollow Up\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n88\n87, 88\n&lt;0.001\n85\n84, 85\n&lt;0.001\n\n\ntreatment_locality\n0.07\n-0.29, 0.44\n0.7\n8.7\n8.0, 9.4\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nAt baseline, there is no statistically significant difference in the average characteristics between the treatment and comparison groups. This confirms that the groups are similar, as expected under randomized assignment. The baseline results show that the treatment locality (the area receiving the feeding program) does not significantly differ from the comparison group in terms of the outcome measure (Beta = 0.07, p-value = 0.7).\nAt follow-up, however, the treatment locality shows a statistically significant and positive effect on the outcome measure, with a beta coefficient of 8.7 (p-value &lt; 0.001). This indicates that households in the treatment locality saw a notable improvement compared to those in the comparison villages. Specifically, the intervention appears to have resulted in an increase in the outcome, possibly reflecting the positive effects of the feeding program, given the substantial change in the beta coefficient.\nThe impact of the program is therefore evident in the follow-up period, and the reduction in the treatment and comparison villages’ differences shows a clear program effect, with an estimated increase of 8.7 units on the attendance rate, which is statistically significant.\nThus, these findings support the conclusion that the feeding program had a positive impact on the target population over the course of the study period.\nRe-estimate using a multivariate regression analysis that controls for the other observable characteristics of the sample households. How does your impact estimate change?\n\nout_round1_nocov &lt;- lm_robust(attendance_rate ~ treatment_locality,\n                              data = df_elig %&gt;% filter(round == 1),\n                              clusters = locality_identifier)\nout_round1_wcov &lt;- lm_robust(attendance_rate ~ treatment_locality +\n                               age_hh + age_sp + educ_hh + educ_sp + \n                               female_hh + indigenous + hhsize + dirtfloor + \n                               bathroom + land + school_distance,\n                             data = df_elig %&gt;% filter(round == 1),\n                             clusters = locality_identifier)\nt2 &lt;- tbl_regression(out_round1_nocov, intercept = T)\nt3 &lt;- tbl_regression(out_round1_wcov, intercept = T)\n\ntbl_merge_out_round1 &lt;-\n  tbl_merge(\n    tbls = list(t2, t3),\n    tab_spanner = c(\"**No Covariate Adjust.**\", \"**With Covariate Adjust.**\")\n  )\n\ntbl_merge_out_round1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Covariate Adjust.\n\n\nWith Covariate Adjust.\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n85\n84, 85\n&lt;0.001\n76\n75, 78\n&lt;0.001\n\n\ntreatment_locality\n8.7\n8.0, 9.4\n&lt;0.001\n8.6\n8.0, 9.2\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.04\n-0.06, -0.01\n0.006\n\n\nage_sp\n\n\n\n\n\n\n0.00\n-0.03, 0.03\n0.9\n\n\neduc_hh\n\n\n\n\n\n\n0.03\n-0.05, 0.11\n0.4\n\n\neduc_sp\n\n\n\n\n\n\n0.02\n-0.06, 0.10\n0.7\n\n\nfemale_hh\n\n\n\n\n\n\n-0.55\n-1.3, 0.21\n0.2\n\n\nindigenous\n\n\n\n\n\n\n1.6\n1.0, 2.2\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.4\n1.3, 1.5\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n1.6\n1.1, 2.1\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.24\n-0.67, 0.18\n0.3\n\n\nland\n\n\n\n\n\n\n-0.03\n-0.10, 0.03\n0.3\n\n\nschool_distance\n\n\n\n\n\n\n0.00\n-0.01, 0.01\n0.5\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWithout Covariate Adjustment:\nAt the baseline, the coefficient for the treatment locality is 8.7 (with a 95% confidence interval of 8.0 to 9.4), which is statistically significant (p-value &lt; 0.001). This suggests that, without adjusting for other factors, the households in the treatment locality (those receiving the feeding program) exhibit a significant improvement compared to those in the comparison group.\nWith Covariate Adjustment:\nWhen adjusting for other observable characteristics such as age, education, and household size, the coefficient for the treatment locality is slightly reduced to 8.6 (95% CI: 8.0 to 9.2) but remains statistically significant (p-value &lt; 0.001). This indicates that even when accounting for factors like age, education, and household characteristics, the treatment locality still shows a strong and positive effect, with the intervention leading to a substantial improvement in the outcome measure.\n\nWhy is the Impact Estimate Unchanged with Covariate Adjustment?\nThe treatment effect remains nearly unchanged when controlling for additional factors because of the randomized assignment. Randomization ensures that the treatment and comparison groups are very similar in characteristics at baseline, and external factors affecting the outcome should affect both groups equally over time. Therefore, any changes observed in the treatment locality compared to the comparison group can confidently be attributed to the feeding program rather than differences in baseline characteristics or external influences.\n\n\nConclusion on the Program’s Impact\nGiven that the estimated impact remains consistent even after controlling for additional characteristics, it is clear that the feeding program has a significant positive effect on the target population. The treatment group shows a noticeable improvement in outcomes, and this improvement is robust to covariate adjustments.\n\n\nShould the Feeding Program Be Scaled Up?\nYes, the feeding program should be scaled up. The impact on the outcome measure is statistically significant and substantial. The effect of the intervention, even after accounting for other factors, supports the case for expanding the program to other regions to improve the well-being of households in similar circumstances."
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#instrumental-variables",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#instrumental-variables",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "4. Instrumental Variables",
    "text": "4. Instrumental Variables\nInstrumental Variables in the context of our school feeding program help us figure out how the program affects attendance rates when other factors might confuse the results. An instrumental variable is something that influences whether a child participates in the program (like whether their school is in a treatment area) but doesn’t directly impact attendance rates except through the program itself. This approach helps isolate the program’s true effect on attendance, even if there are other overlapping influences.\nLet us now try using the randomized promotion method to evaluate the impact of the school feeding program (SFP) on attendance rates. Imagine the Ministry of Education decides that the feeding program should eventually be made available to all schools nationwide. This is a different situation from the randomized assignment design we’ve considered so far. However, given the logistical realities of scaling the program, you propose an incremental rollout.\nTo assess its impact, you randomly select a subset of schools (indicated by promotion_locality) to receive an intensive promotion campaign aimed at increasing awareness and participation in the feeding program. This campaign includes activities such as community outreach, parent meetings, and tailored communication materials to emphasize the program’s benefits. Importantly, the promotion focuses solely on raising awareness and boosting program enrollment, ensuring it does not directly encourage unrelated behaviors that could influence attendance rates. This design ensures the promotion can be used as a valid instrumental variable (IV) for understanding how the feeding program affects attendance rates.\nWhat was the effect of the promotion campaign upon enrollment?\nNote you should use the variable enrolled_rp for this question\n\nm_enroll &lt;- lm_robust(enrolled_rp ~ promotion_locality,\n                      clusters = locality_identifier,\n                      data = trans_df %&gt;% filter(round == 1))\n\nt_enroll &lt;- tbl_regression(m_enroll, intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\nt_enroll\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.08\n0.04, 0.13\n0.001\n    promotion_locality\n0.41\n0.34, 0.48\n&lt;0.001\n  \n  \n    \n      Adjusted R² = 0.200; R² = 0.200; No. Obs. = 9,914\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nAfter two years of promotion and program implementation, you find that 41% of students in schools randomly assigned to the promotion campaign are attending school more regularly, compared to only 8% in non-promoted schools.\nBecause the promoted and non-promoted schools were assigned at random, we can confidently assume that the baseline characteristics of the two groups were similar in the absence of the promotion. This assumption is validated by comparing baseline attendance rates and other school-related characteristics, which showed no significant differences.\nFrom the results in Table 2, the estimated impact of the promotion locality on attendance rates is a significant increase of 41 percentage points (Beta = 0.41, 95% CI [0.34, 0.48], p &lt; 0.001). The intercept (baseline attendance rate in non-promoted schools) was estimated at 8% (Beta = 0.08, 95% CI [0.04, 0.13], p = 0.001). The model explains 20% of the variation in attendance rates (Adjusted R² = 0.20), indicating a strong relationship between promotion and improved attendance outcomes.\nCompare baseline attendance rates based upon assignment to promotion.\n\nm_base_attend &lt;- lm_robust(attendance_rate ~ promotion_locality,\n                           clusters = locality_identifier,\n                           data = trans_df %&gt;% filter(round == 0)\n)\n\nt_base_attend &lt;- tbl_regression(m_base_attend, intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  ) \nt_base_attend\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n85\n85, 86\n&lt;0.001\n    promotion_locality\n0.05\n-0.44, 0.53\n0.9\n  \n  \n    \n      Adjusted R² = 0.000; R² = 0.000; No. Obs. = 9,913\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nEstimate the difference in attendance rates by assignment to promotion, in the post-treatment period\n\nm_post_attend &lt;- lm_robust(attendance_rate ~ promotion_locality,\n                           clusters = locality_identifier,\n                           data = trans_df %&gt;% filter(round == 1)\n)\n\nt_post_attend &lt;- tbl_regression(m_post_attend, intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  ) \nt_post_attend\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n84\n83, 85\n&lt;0.001\n    promotion_locality\n3.3\n2.2, 4.4\n&lt;0.001\n  \n  \n    \n      Adjusted R² = 0.026; R² = 0.027; No. Obs. = 9,914\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nUsing this attendance rate estimate and the estimated proportion of “compliers”, estimate the LATE/CACE\n\nLATE (Local Average Treatment Effect):\nLATE measures the effect of the program only on the group that complied with the treatment assignment (e.g., those who were offered the feeding program and actually participated). It tells us the impact on attendance rates for these participants, not for everyone assigned to the treatment or control groups.\n\n\nCACE (Complier Average Causal Effect):\nCACE is essentially the same as LATE in many contexts, particularly in randomized trials. It focuses on estimating the causal effect of the program for those who adhered to their assignment (e.g., those who were assigned to the treatment and participated, or those in the control who did not access the treatment).\n\nm_cace &lt;- iv_robust(attendance_rate ~ enrolled_rp |\n                      promotion_locality,\n                    clusters = locality_identifier,\n                    data = trans_df %&gt;% filter(round == 1))\n\nm_cace_wcov &lt;- iv_robust(attendance_rate ~ enrolled_rp + \n                           age_hh + age_sp + educ_hh + educ_sp + \n                           female_hh + indigenous + hhsize + dirtfloor + \n                           bathroom + land + school_distance | \n                           promotion_locality + \n                           age_hh + age_sp + educ_hh + educ_sp + \n                           female_hh + indigenous + hhsize + dirtfloor + \n                           bathroom + land + school_distance ,\n                         clusters = locality_identifier,\n                         data = trans_df %&gt;% filter(round == 1))\n\n\nt_cace &lt;- tbl_regression(m_cace, intercept = T)\nt_cace_wcov &lt;- tbl_regression(m_cace_wcov, intercept = T)\n\ntbl_merge_cace &lt;-\n  tbl_merge(\n    tbls = list(t_cace, t_cace_wcov),\n    tab_spanner = c(\"**No Covariate Adjust.**\", \"**With Covariate Adjust.**\")\n  )\n\ntbl_merge_cace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Covariate Adjust.\n\n\nWith Covariate Adjust.\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n83\n82, 84\n&lt;0.001\n75\n74, 76\n&lt;0.001\n\n\nenrolled_rp\n8.1\n6.2, 10\n&lt;0.001\n8.3\n6.7, 10\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.06\n-0.09, -0.04\n&lt;0.001\n\n\nage_sp\n\n\n\n\n\n\n0.01\n-0.02, 0.04\n0.4\n\n\neduc_hh\n\n\n\n\n\n\n-0.03\n-0.11, 0.04\n0.4\n\n\neduc_sp\n\n\n\n\n\n\n0.04\n-0.04, 0.12\n0.4\n\n\nfemale_hh\n\n\n\n\n\n\n-0.88\n-1.7, -0.11\n0.025\n\n\nindigenous\n\n\n\n\n\n\n2.0\n1.3, 2.7\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.7\n1.6, 1.9\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n1.8\n1.3, 2.3\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.59\n-1.0, -0.18\n0.005\n\n\nland\n\n\n\n\n\n\n-0.08\n-0.16, 0.00\n0.039\n\n\nschool_distance\n\n\n\n\n\n\n0.00\n0.00, 0.01\n0.3\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCompare baseline attendance rates based upon assignment to promotion\nThe baseline attendance rates show that there is no significant difference in attendance rates between schools assigned to the promotion (SFP) and those that were not, as indicated by the promotion locality coefficient of 0.05 with a 95% Confidence Interval (CI) of [-0.44, 0.53] and a p-value of 0.9. The p-value is greater than the 0.05 significance level, which means that any differences in baseline attendance are likely due to random variation, not the promotion itself.\nEstimate the difference in attendance rates by assignment to promotion, in the post-treatment period\nThe post attendance period shows a significant positive effect on attendance, with a coefficient of 3.3 (95% CI [2.2, 4.4], p-value &lt; 0.001). This suggests that schools assigned to the feeding program promotion saw an increase in attendance rates by about 3.3 percentage points compared to non-promoted schools. The model’s adjusted R² of 0.026 indicates that a small proportion of the variation in attendance is explained by the promotion, but the effect is still statistically significant.\nUsing this attendance rate estimate and the estimated proportion of “compliers,” estimate the LATE/CACE\nThe Local Average Treatment Effect (LATE) and the Complier Average Causal Effect (CACE) are estimated by considering both unadjusted and adjusted models.\nNo Covariate Adjustment: The coefficient for the promotion locality remains significant (Beta = 8.1, 95% CI [6.2, 10], p-value &lt; 0.001), suggesting that, on average, the promotion led to an 8.1 percentage point increase in attendance for those schools in the promotion locality who complied with the program.\nWith Covariate Adjustment: After adjusting for covariates such as household characteristics and school factors, the effect is slightly higher (Beta = 8.3, 95% CI [6.7, 10], p-value &lt; 0.001), reinforcing the robustness of the promotion’s impact. These results suggest that the school feeding program had a substantial positive impact on attendance rates, particularly for the compliers—those schools that adhered to the promotion. The covariate-adjusted estimates provide additional confidence that the observed effect is not solely driven by confounding factors.\nWhat are the key conditions for accepting the results from the randomized promotion evaluation of the SFP\n\nBaseline Equivalence: The schools in the promoted and non-promoted groups should have similar characteristics before the promotion. The baseline attendance rates in the first table show no significant difference, suggesting this assumption holds true.\nPromotion Effectiveness: The promotion should effectively increase school attendance. This assumption holds in the post-treatment period, where the promoted schools show a significant increase in attendance by 3.3 percentage points.\nNo Direct Effects on Other Factors: The promotion should only affect attendance and not other outcomes (such as student performance or health). This assumption cannot be directly tested but is informed by the program’s design focusing only on increasing school attendance.\n\nShould the SFP be Scaled Up Nationally?\nBased on the results from the regression analysis, the school feeding program shows a significant positive impact on attendance rates. The estimated LATE/CACE of 8.3 percentage points suggests a clear benefit from the program. Therefore, the results support scaling the program up nationally, as the program’s effectiveness in improving attendance is statistically significant and substantial."
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#regression-discontinuity-designs",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#regression-discontinuity-designs",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "5. Regression Discontinuity Designs",
    "text": "5. Regression Discontinuity Designs\nRegression Discontinuity Designs (RDD) are a method used to evaluate the impact of a treatment or intervention by focusing on cases where a specific cutoff or threshold determines who receives the treatment. For example, imagine a school program that only allows students with test scores above a certain level to participate. RDD compares students who are just above the threshold (and get the program) with those who are just below it (and don’t get the program). This helps estimate the effect of the program by assuming that those on either side of the cutoff are very similar, except for receiving the program.\nIn simple terms, RDD looks at situations where a rule or score divides people into different groups and then compares those on either side of that line to see if the treatment makes a real difference.\n\nApplication\nNow consider how the regression discontinuity design (RDD) method can be applied to our School Feeding Program (SFP). After doing some more investigation into the design of SFP, you find that in addition to randomly selecting treatment villages, the authorities targeted the program to low-income households using the national poverty line. The poverty line is based on a poverty index that assigns each household in the country a score between 20 and 100 based on its assets, housing conditions, and socio demographic structure. The poverty line has been officially set at 58. This means that all households with a score of 58 or below are classified as poor, and all households with a score of more than 58 are considered to be non-poor. Even in the treatment villages, only poor households are eligible to enroll in SFP. Your data set includes information on both poor and non-poor households in the treatment villages\n\n# Create data subset with only treatment localities\ndf_treat &lt;- trans_df %&gt;%\n  filter(treatment_locality == 1)\n\nBefore carrying out the regression discontinuity design estimations, you decide to check whether there is any evidence of manipulation of the eligibility index. As a first step, you check whether the density of the eligibility index raises any concerns about manipulation of the index. You plot the percentage of schools against the baseline poverty index.\n\nggplot(df_treat, aes(x = poverty_index)) +\n  geom_vline(xintercept = 58) +\n  geom_density() +\n  labs(x = \"Poverty Index\")\n\n\n\n\n\n\n\n\nWe can also conduct a McCrary density test, to examine this more formally.\n\ntest_density &lt;- rdplotdensity(rdd = rddensity(df_treat$poverty_index, c = 58), \n                              X = df_treat$poverty_index, \n                              type = \"both\")\n\n\n\n\n\n\n\n\nThe figures do not indicate any clustering of schools right below the cutoff of 58.\nNext, you check whether households respected their assignment to the treatment and comparison groups on the basis of their eligibility score. You plot participation in the program against the baseline poverty index and find that two years after the start of the pilot, only households with a score of 58 or below (that is, to the left of the poverty line) have been allowed to enroll in SFP. In addition, all of the eligible households enrolled in SFP. In other words, you find full compliance and have a “sharp” RDD.\n\nggplot(df_treat, aes(y = enrolled, x = poverty_index)) +\n  geom_vline(xintercept = 58) +\n  geom_point() +\n  labs(x = \"Poverty Index\", y = \"Enrolled\")\n\n\n\n\n\n\n\n\nYou now proceed to apply the RDD method to compute the impact of the program. Using follow-up data, you again plot the relationship between the scores on the poverty index and predicted attendance rates and find the relation illustrated in the figure below. In the relationship between the poverty index and the predicted attendance rates, you find a clear break, or discontinuity, at the poverty line (58).\n\ndf_treat %&gt;%\n  filter(round == 1) %&gt;%\n  mutate(enrolled_lab = ifelse(enrolled == 1, \"Enrolled\", \"Not Enrolled\")) %&gt;%\n  ggplot(aes(x = poverty_index, y = attendance_rate,\n             group = enrolled_lab, colour = enrolled_lab, fill = enrolled_lab)) +\n  geom_point(alpha = 0.03) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Poverty Index\", y = \"Attendance_rate\") +\n  scale_colour_viridis_d(\"Enrollment:\", end = 0.7) +\n  scale_fill_viridis_d(\"Enrollment:\", end = 0.7) +\n  theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe discontinuity reflects an increase in attendance rates for those schools eligible to receive the program. Given that schools on both sides of the cutoff score of 58 are very similar, the plausible explanation for the different level of attendance rates is that one group of schools was eligible to enroll in the program and the other was not. You estimate this difference through a regression with the findings shown in the following table.\n\ndf_treat &lt;- df_treat %&gt;%\n  mutate(poverty_index_c0 = poverty_index - 58)\n\nout_rdd &lt;- lm_robust(attendance_rate ~ poverty_index_c0 * enrolled + \n                       age_hh + age_sp + educ_hh + educ_sp + \n                       female_hh + indigenous + hhsize + dirtfloor + \n                       bathroom + land + school_distance,\n                     data = df_treat %&gt;% filter(round == 1))\n\ntbl16 &lt;- tbl_regression(out_rdd, intercept = T) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\nNote: We could also estimate the effect of the program in the following ways\nEstimating the effect of the program on health expenditures again using regression, but include an interaction with a cubic polynomial of the running variable.\n\nout_rdd_cubic &lt;- lm_robust(attendance_rate ~ enrolled * poverty_index_c0 +\n                             enrolled * I(poverty_index_c0^2) + \n                             enrolled * I(poverty_index_c0^3) +\n                             age_hh + age_sp + educ_hh + educ_sp + \n                             female_hh + indigenous + hhsize + dirtfloor +\n                             bathroom + land + school_distance,\n                           data = df_treat %&gt;% filter(round == 1))\ntbl_cubic &lt;- tbl_regression(out_rdd_cubic, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\nEstimating the effect of the program on attendance rates again using regression, but only including observations 5 points above or below the cutoff of 58.\n\nout_rdd5 &lt;- lm_robust(attendance_rate ~ enrolled * poverty_index_c0 + \n                        age_hh + age_sp + educ_hh + educ_sp + \n                        female_hh + indigenous + hhsize + dirtfloor + \n                        bathroom + land + school_distance,\n                      data = df_treat %&gt;% filter(round == 1 &\n                                                   abs(poverty_index_c0) &lt;=5))\n\ntbl_rdd5 &lt;- tbl_regression(out_rdd5, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\n\ntbl_merge_rdd_all &lt;-\n  tbl_merge(\n    tbls = list( tbl16,tbl_cubic, tbl_rdd5),\n    tab_spanner = c(\"**Linear**\", \"**Cubic**\", \"**5 Point Window**\")\n  )\n\ntbl_merge_rdd_all\n\n\n\n\n  \n    \n      Characteristic\n      \n        Linear\n      \n      \n        Cubic\n      \n      \n        5 Point Window\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n75\n73, 77\n&lt;0.001\n75\n73, 77\n&lt;0.001\n75\n72, 77\n&lt;0.001\n    poverty_index_c0\n-0.15\n-0.22, -0.08\n&lt;0.001\n-0.28\n-0.64, 0.08\n0.13\n-0.35\n-0.75, 0.06\n0.094\n    enrolled\n7.7\n7.0, 8.5\n&lt;0.001\n7.6\n6.4, 8.9\n&lt;0.001\n7.3\n5.9, 8.8\n&lt;0.001\n    age_hh\n-0.08\n-0.11, -0.04\n&lt;0.001\n-0.08\n-0.11, -0.04\n&lt;0.001\n-0.05\n-0.10, -0.01\n0.024\n    age_sp\n0.05\n0.01, 0.09\n0.016\n0.05\n0.01, 0.09\n0.016\n0.02\n-0.03, 0.07\n0.5\n    educ_hh\n0.06\n-0.04, 0.16\n0.2\n0.06\n-0.04, 0.16\n0.2\n0.05\n-0.10, 0.21\n0.5\n    educ_sp\n0.12\n0.01, 0.23\n0.034\n0.12\n0.01, 0.23\n0.038\n0.18\n0.01, 0.34\n0.037\n    female_hh\n-0.49\n-1.5, 0.49\n0.3\n-0.48\n-1.5, 0.50\n0.3\n-0.73\n-2.1, 0.65\n0.3\n    indigenous\n1.6\n1.1, 2.1\n&lt;0.001\n1.6\n1.1, 2.1\n&lt;0.001\n1.7\n0.88, 2.6\n&lt;0.001\n    hhsize\n1.7\n1.6, 1.8\n&lt;0.001\n1.7\n1.6, 1.8\n&lt;0.001\n1.7\n1.6, 1.9\n&lt;0.001\n    dirtfloor\n1.5\n0.99, 2.0\n&lt;0.001\n1.5\n1.0, 2.0\n&lt;0.001\n1.6\n0.83, 2.5\n&lt;0.001\n    bathroom\n-0.49\n-0.94, -0.03\n0.036\n-0.49\n-0.95, -0.03\n0.036\n-0.09\n-0.86, 0.68\n0.8\n    land\n0.04\n-0.03, 0.11\n0.3\n0.04\n-0.03, 0.11\n0.2\n0.01\n-0.11, 0.14\n0.8\n    school_distance\n0.00\n0.00, 0.01\n0.11\n0.00\n0.00, 0.01\n0.11\n0.01\n-0.01, 0.02\n0.3\n    poverty_index_c0 * enrolled\n0.17\n0.09, 0.25\n&lt;0.001\n\n\n\n\n\n\n    I(poverty_index_c0^2)\n\n\n\n0.01\n-0.02, 0.04\n0.4\n\n\n\n    I(poverty_index_c0^3)\n\n\n\n0.00\n0.00, 0.00\n0.4\n\n\n\n    enrolled * poverty_index_c0\n\n\n\n0.33\n-0.10, 0.77\n0.13\n0.36\n-0.16, 0.87\n0.2\n    enrolled * I(poverty_index_c0^2)\n\n\n\n-0.01\n-0.05, 0.02\n0.5\n\n\n\n    enrolled * I(poverty_index_c0^3)\n\n\n\n0.00\n0.00, 0.00\n0.6\n\n\n\n  \n  \n    \n      Adjusted R² = 0.457; R² = 0.458; No. Obs. = 4,960\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nCombining all these results together we see a consistent effect of the program.\n\n\nResults\nIs the result of the RDD analysis valid for all schools in the program?\nNo, the RDD estimates represent the effects for schools with attendance rates very close to the defined eligibility cutoff. Intuitively, this is the region where schools just eligible for the program and those just ineligible have the most similar baseline characteristics and can be meaningfully compared.\nCompared with the impact estimated with the randomized assignment method, what does this result say about schools with poverty index just below the cutoff?\nThis result indicates that schools with poverty index just below the eligibility threshold experience a smaller increase in attendance rates than the average eligible school. Specifically, schools just under the cutoff score experience an increase of 7.3 percentage points in attendance rates, which is slightly less than the average improvement observed with the randomized assignment method."
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#difference-in-differences",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#difference-in-differences",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "6. Difference-in-Differences",
    "text": "6. Difference-in-Differences\nThe Difference-in-Differences (DiD) method is a widely used statistical approach to evaluate the causal effect of an intervention by comparing changes over time between a treatment group and a control group. In our School Feeding Program (SFP), this technique can help us measure how the introduction of the program influenced student attendance rates by comparing schools that implemented the program (treatment group) with those that did not (control group), both before and after the program began.\nThis approach leverages the assumption that, without the intervention, both groups would have followed similar trends over time. By examining the difference in attendance rate changes between these groups, we isolate the program’s impact while controlling for underlying trends that affect all schools.\nIn this scenario, you have two rounds of data on two groups of schools: one group that enrolled in the program, and another that did not. Remembering the case of the enrolled and non- enrolled groups, you realize that you cannot simply compare the average attendance rates of the two groups because of selection bias. Because you have data for two periods for each school in the sample, you can use those data to solve some of these challenges by comparing the change in attendance rates for the two groups, assuming that the change in the attendance rates of the non-enrolled group reflects what would have happened to the attendance of the enrolled group in the absence of the program. Note that it does not matter which way you calculate the double difference.\n\nout_did &lt;- lm_robust(attendance_rate ~ round * enrolled, \n                     data = trans_df %&gt;% filter(treatment_locality == 1),\n                     clusters = locality_identifier)\n\nout_did_wcov &lt;- lm_robust(attendance_rate ~ round * enrolled +\n                            age_hh + age_sp + educ_hh + educ_sp + \n                            female_hh + indigenous + hhsize + dirtfloor + \n                            bathroom + land + school_distance, \n                          data = trans_df %&gt;% filter(treatment_locality == 1),\n                          clusters = locality_identifier)\n\ntbl_did &lt;- tbl_regression(out_did, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\ntbl_did_wcov &lt;- tbl_regression(out_did_wcov, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\n\ntbl_merge_did &lt;-\n  tbl_merge(\n    tbls = list( tbl_did, tbl_did_wcov),\n    tab_spanner = c(\"**No Covariate Adjustment**\", \"**With Covariate Adjustment**\")\n  )\n\ntbl_merge_did\n\n\n\n\n  \n    \n      Characteristic\n      \n        No Covariate Adjustment\n      \n      \n        With Covariate Adjustment\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n82\n82, 83\n&lt;0.001\n77\n76, 78\n&lt;0.001\n    round\n-1.3\n-1.9, -0.68\n&lt;0.001\n-1.2\n-1.9, -0.62\n&lt;0.001\n    enrolled\n5.4\n5.1, 5.7\n&lt;0.001\n1.3\n1.1, 1.5\n&lt;0.001\n    round * enrolled\n7.0\n6.4, 7.5\n&lt;0.001\n7.0\n6.4, 7.5\n&lt;0.001\n    age_hh\n\n\n\n-0.07\n-0.09, -0.05\n&lt;0.001\n    age_sp\n\n\n\n0.02\n-0.01, 0.04\n0.14\n    educ_hh\n\n\n\n-0.05\n-0.10, 0.00\n0.046\n    educ_sp\n\n\n\n0.07\n0.01, 0.12\n0.030\n    female_hh\n\n\n\n-0.94\n-1.5, -0.40\n0.001\n    indigenous\n\n\n\n2.0\n1.6, 2.4\n&lt;0.001\n    hhsize\n\n\n\n1.7\n1.6, 1.8\n&lt;0.001\n    dirtfloor\n\n\n\n2.0\n1.7, 2.3\n&lt;0.001\n    bathroom\n\n\n\n-0.43\n-0.70, -0.15\n0.003\n    land\n\n\n\n-0.08\n-0.13, -0.03\n0.004\n    school_distance\n\n\n\n0.00\n0.00, 0.01\n0.3\n  \n  \n    \n      Adjusted R² = 0.343; R² = 0.344; No. Obs. = 9,919\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nNext, you estimate the effect using regression analysis. Using a simple linear regression to compute the simple difference-in- differences estimate, you find that the program increased school attendance rate by 7.0. You then refine your analysis by adding additional control variables. In other words, you use a multivariate linear regression that takes into account a host of other factors, and you find the same promotion in school attendance rate.\nWhat are the basic assumptions required to accept this result from difference-in-differences?\nTo accept this result, we assume that there are no differential time varying factors between the two groups other than the program. We assume that the treatment and comparison groups would have equal trends or changes in outcomes in the absence of treatment. While this assumption can’t be tested in the post intervention period, we can compare trends before the intervention starts.\nBased on the result from difference-in-differences, should HISP be scaled up nationally?\nNo, based on this result, the SFP should not be scaled up nationally because it has Increased by less than the $10 threshold level. Taking the estimated impact under random assignment as the “true” impact of the program suggests that the difference in difference estimate may be biased. In fact, in this case, using the nonenrolled households as a comparison group does not accurately represent the counterfactual trend in attendance rates."
  },
  {
    "objectID": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#next-page-previous",
    "href": "posts/M&E_01_School_Feeding_Causal_inference_&_Counterfactuals/index.html#next-page-previous",
    "title": "School Feeding Program (SFP) Practical Evaluation Using Different Methodologies and R Codes",
    "section": "Next page previous",
    "text": "Next page previous"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html",
    "href": "posts/Mathematical and analytical skills/index.html",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Ahoy, fellow math maestros and numerical neophytes alike! In the vast terrain of the professional realm, where equations reign supreme and spreadsheets take center stage, it’s time to sprinkle a dash of humor onto the canvas of calculations. Join me on this delightful journey as we explore ways to enhance our mathematical and analytical prowess with a grin on our faces.\n\n\n\n\n\n\n\n\nEmbrace mathematics as the quirky companion in your cubicle. Inject humor into the daily grind of numbers. Consider math the office prankster – sometimes challenging, often surprising, but always ready to bring a smile to your face. Crack a math-related joke during coffee breaks and transform your numerical nemesis into a trusted ally.\nExample: “Why was the equal sign so humble? Because he knew he wasn't less than or greater than anyone else!”\n\n\n\n\n\n\n\n\nExcel, the caped crusader of corporate calculation! Dive into the realm of formulas and functions fearlessly, like a hero facing down a villain. Master PivotTables, VLOOKUPs, and SUMIFS – wield them with the finesse of a mathematical ninja. Soon, your coworkers will marvel at your spreadsheet superpowers, making you the hero the office never knew it needed.\n\n\n\n\n\n\n\n\nIntroduce brain-teasing puzzles and games into your daily routine – because who said work can’t be fun? Sudoku, crosswords, and logic puzzles not only entertain but also supercharge your analytical thinking. Challenge your colleagues to friendly puzzle-solving competitions during breaks. The winner earns bragging rights and a metaphorical crown, turning the office into a playground of mental gymnastics.\n\n\n\nTransform mundane meetings into engaging sessions by incorporating brain teasers and quick math challenges. Kickstart discussions with a burst of intellectual energy. Not only will this sharpen everyone’s analytical skills, but it will also foster a collaborative and dynamic workplace.\nExample: Open your next meeting with a puzzle: “If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?” Watch as your colleagues’ brains kick into high gear!\n\n\n\nCharts and graphs need not be dull – let them be your canvas for creativity! Infuse a bit of humor into your data visualizations. Turn a line graph into a rollercoaster of success or transform a bar chart into a skyscraper of achievements. The more entertaining, the more memorable your insights become.\n\n\n\nIn the pursuit of mathematical and analytical excellence, don’t forget to have a bit of fun along the way. Whether you’re a seasoned number-cruncher or a math-phobic beginner, infusing humor into your calculations not only enhances your skills but also makes the workplace a more enjoyable space for everyone. So, put on your mathematical cape, embrace the numbers, and let the workplace be your stage for a comedy of calculations. Happy calculating!"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#introduction",
    "href": "posts/Mathematical and analytical skills/index.html#introduction",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Ahoy, fellow math maestros and numerical neophytes alike! In the vast terrain of the professional realm, where equations reign supreme and spreadsheets take center stage, it’s time to sprinkle a dash of humor onto the canvas of calculations. Join me on this delightful journey as we explore ways to enhance our mathematical and analytical prowess with a grin on our faces."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#counting-chuckles-a-positive-perspective",
    "href": "posts/Mathematical and analytical skills/index.html#counting-chuckles-a-positive-perspective",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Embrace mathematics as the quirky companion in your cubicle. Inject humor into the daily grind of numbers. Consider math the office prankster – sometimes challenging, often surprising, but always ready to bring a smile to your face. Crack a math-related joke during coffee breaks and transform your numerical nemesis into a trusted ally.\nExample: “Why was the equal sign so humble? Because he knew he wasn't less than or greater than anyone else!”"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#excel-erate-your-skills-unleashing-spreadsheet-superpowers",
    "href": "posts/Mathematical and analytical skills/index.html#excel-erate-your-skills-unleashing-spreadsheet-superpowers",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Excel, the caped crusader of corporate calculation! Dive into the realm of formulas and functions fearlessly, like a hero facing down a villain. Master PivotTables, VLOOKUPs, and SUMIFS – wield them with the finesse of a mathematical ninja. Soon, your coworkers will marvel at your spreadsheet superpowers, making you the hero the office never knew it needed."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#the-power-of-puzzles-mental-gymnastics-for-all",
    "href": "posts/Mathematical and analytical skills/index.html#the-power-of-puzzles-mental-gymnastics-for-all",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Introduce brain-teasing puzzles and games into your daily routine – because who said work can’t be fun? Sudoku, crosswords, and logic puzzles not only entertain but also supercharge your analytical thinking. Challenge your colleagues to friendly puzzle-solving competitions during breaks. The winner earns bragging rights and a metaphorical crown, turning the office into a playground of mental gymnastics."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#math-meetings-with-a-twist-brainy-banter-for-team-bonding",
    "href": "posts/Mathematical and analytical skills/index.html#math-meetings-with-a-twist-brainy-banter-for-team-bonding",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Transform mundane meetings into engaging sessions by incorporating brain teasers and quick math challenges. Kickstart discussions with a burst of intellectual energy. Not only will this sharpen everyone’s analytical skills, but it will also foster a collaborative and dynamic workplace.\nExample: Open your next meeting with a puzzle: “If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?” Watch as your colleagues’ brains kick into high gear!"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#graphs-with-a-grin-visualizing-data-creatively",
    "href": "posts/Mathematical and analytical skills/index.html#graphs-with-a-grin-visualizing-data-creatively",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Charts and graphs need not be dull – let them be your canvas for creativity! Infuse a bit of humor into your data visualizations. Turn a line graph into a rollercoaster of success or transform a bar chart into a skyscraper of achievements. The more entertaining, the more memorable your insights become."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#conclusion",
    "href": "posts/Mathematical and analytical skills/index.html#conclusion",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "In the pursuit of mathematical and analytical excellence, don’t forget to have a bit of fun along the way. Whether you’re a seasoned number-cruncher or a math-phobic beginner, infusing humor into your calculations not only enhances your skills but also makes the workplace a more enjoyable space for everyone. So, put on your mathematical cape, embrace the numbers, and let the workplace be your stage for a comedy of calculations. Happy calculating!"
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_Framework/index.html",
    "href": "posts/Monitoring_and_Evaluation_Framework/index.html",
    "title": "How to Develop a Research, Monitoring, and Evaluation Framework for Your Project",
    "section": "",
    "text": "Creating a Research, Monitoring, and Evaluation (M&E) framework is essential for the success of any project. It helps track progress, measure outcomes, and ensure that your project is on track to achieve its goals. In this post, we’ll break down the first part of building an effective M&E framework: Framework Development. We’ll keep things simple and use examples along the way to make it easier to understand.\n\n1. Define the Project Scope and Objectives\nBefore diving into any project, it’s important to have a clear idea of what you’re trying to achieve.\n\nProject Overview: Start by summarizing your project. What is it about? Who will benefit? For example, if you’re working on a project to improve literacy rates in rural schools, your overview might be: “This project aims to improve literacy rates among children in rural areas by providing better learning materials and training teachers.”\nSpecific Objectives: These are the concrete steps you plan to take to achieve your overall goal. Remember to make them SMART—Specific, Measurable, Achievable, Relevant, and Time-bound. An example could be: “By the end of the year, increase reading scores by 20% among students in 10 rural schools.”\nKey Stakeholders: These are the people who are involved in or affected by your project. For a literacy project, stakeholders might include school principals, teachers, parents, and local government officials.\n\n\n\n2. Develop a Theory of Change (TOC)\nThe Theory of Change (TOC) is like a roadmap that shows how your project activities will lead to the desired results. Here’s how to develop a TOC:\n\nProblem Statement: Clearly define the issue your project is addressing. For example, “Many students in rural areas struggle with reading due to a lack of proper learning materials and trained teachers.”\nInputs: These are the resources you’ll need to carry out the project. In our example, this could include funds for buying books, hiring trainers, and creating a teacher training program.\nActivities: These are the things you’ll do to achieve your objectives. For the literacy project, activities might include holding teacher training workshops, distributing reading materials, and organizing reading competitions for students.\nOutputs: These are the direct results of your activities. For example, “100 teachers trained and 500 books distributed to schools.”\nOutcomes: These are the short- and long-term changes you expect to see. Short-term: “Teachers feel more confident in teaching reading.” Long-term: “Students improve their reading skills and performance.”\nImpact: This is the broader, long-term change you hope to achieve. For our project, the impact might be: “Improved literacy rates in rural areas, leading to better opportunities for students.”\n\n\n\n3. Design Key Performance Indicators (KPIs)\nKey Performance Indicators (KPIs) help you measure the success of your project. These can be broken down into three types:\n\nOutput Indicators: These measure the direct results of your activities. In our example, a good output indicator would be “Number of teachers trained” or “Number of books distributed.”\nOutcome Indicators: These measure the changes or benefits resulting from your project. For instance, “Percentage of teachers applying new reading techniques in class” or “Increase in student reading test scores.”\nImpact Indicators: These look at the broader, long-term effects. For example, “Overall improvement in literacy rates in the region” or “Percentage of students who go on to secondary school.”\n\nBaselines and Targets: It’s important to know where you’re starting from and where you want to go. A baseline is the current status before your project begins. For example, you might find that only 40% of students can read at grade level before your intervention. Your target could then be to increase that to 60% by the end of the year.\n\n\nFinal Thoughts\nBuilding the foundation of your Research, Monitoring, and Evaluation framework ensures you have a clear plan for achieving your project’s goals. By defining your scope and objectives, developing a Theory of Change, and designing strong KPIs, you’re setting yourself up for success.\nThink of the M&E framework like a compass that helps you navigate your project. With these steps in place, you’ll have a clear sense of direction and will be able to measure your progress along the way. Whether you’re working on improving literacy or any other type of project, this framework can help guide you to your destination!"
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_Plan/index.html",
    "href": "posts/Monitoring_and_Evaluation_Plan/index.html",
    "title": "How to Build an Evaluation Plan for Your Project",
    "section": "",
    "text": "In our Monitoring and Evaluation (M&E) journey, we’ve covered setting up an M&E framework and building a monitoring plan. Now, it’s time to talk about the Evaluation Plan, which is the final step in assessing the success of your project. While monitoring is about tracking ongoing progress, evaluation looks at the overall impact and effectiveness once a project has been running for some time or is completed.\nIn this post, we’ll break down how to create a simple and effective Evaluation Plan, using examples to make it easy to understand.\n\n1. What Is an Evaluation Plan?\nAn Evaluation Plan helps you determine whether your project achieved its intended goals and objectives. It answers big-picture questions like:\n\nDid the project make a positive difference?\nWhat worked well, and what could have been improved?\nWere the resources used efficiently?\n\nFor example, if your project was aimed at increasing access to clean water in a community, your evaluation would ask: “Has this project resulted in better health outcomes for the community?”\n\n\n2. Define the Evaluation Purpose\nThe first step in building an Evaluation Plan is to define why you’re evaluating the project. This will guide the type of evaluation you conduct. There are two main types of evaluations:\n\nFormative Evaluation: Done during the project, it helps improve the project while it’s still running. For example, midway through a clean water project, you could evaluate whether people are adopting the use of the water pumps. If not, you can adjust the project accordingly.\nSummative Evaluation: Done at the end of the project, this type assesses overall outcomes and impacts. For example, at the end of the clean water project, you could evaluate whether there’s been a decrease in waterborne diseases in the community.\n\nBe clear on the purpose of your evaluation, as it will guide your approach.\n\n\n3. Choose Your Evaluation Questions\nOnce you’ve defined your purpose, develop a set of key evaluation questions. These questions should focus on the outcomes and impact of your project. Here are some examples:\n\nEffectiveness: Did the project achieve its objectives? For example, “Did access to clean water improve in the target community?”\nEfficiency: Was the project implemented in a cost-effective manner? For example, “Were the resources (money, time, and personnel) used efficiently?”\nSustainability: Are the project benefits likely to continue after funding ends? For example, “Will the water pumps be maintained by the community in the long term?”\nImpact: What were the long-term changes brought about by the project? For example, “Did the incidence of waterborne diseases decrease in the community?”\n\nYour evaluation questions will help focus your data collection and analysis.\n\n\n4. Decide on Data Collection Methods\nJust like in your Monitoring Plan, you’ll need to decide how to collect the data for evaluation. However, in evaluation, you’re often looking for more in-depth information to assess the project’s impact. Here are some common methods:\n\nSurveys: These can help you assess changes in behavior or outcomes. For example, a survey could ask households about their health and water usage after the project has ended.\nInterviews and Focus Groups: These provide qualitative insights into the project’s impact. For example, you could hold focus group discussions with community members to hear firsthand how the clean water project has affected their lives.\nObservation and Site Visits: Directly observing the project sites can give you a clearer picture of what’s been achieved. For example, visiting the water pumps to check if they’re still operational and in use.\nPre-and Post-Project Data: Comparing data collected before the project began with data collected after it ends can show measurable changes. For example, comparing the rates of waterborne diseases before and after the project can give you concrete evidence of the project’s impact.\n\n\n\n5. Establish an Evaluation Timeline\nEvaluation doesn’t always happen at the end of a project; it can take place at different stages. Here’s a breakdown:\n\nMid-term Evaluation: This happens halfway through the project. It’s a great way to assess whether the project is on track and make improvements. For example, if fewer people are using the water pumps than expected, the mid-term evaluation can highlight this, and you can adjust accordingly.\nEnd-line Evaluation: This occurs at the end of the project to measure whether it met its objectives. For example, after the water pumps have been installed and the project has concluded, the end-line evaluation would measure the overall impact on the community’s access to clean water and health outcomes.\nPost-Project Evaluation: This happens some time after the project has ended to see if the benefits have been sustained. For example, a post-project evaluation could assess whether the water pumps are still functional and if the community continues to benefit from improved health outcomes months or years after the project’s conclusion.\n\n\n\n6. Analyze and Report Your Findings\nOnce you’ve collected your evaluation data, it’s time to analyze it. This step is about turning the raw data into actionable insights. Ask yourself questions like:\n\nWhat worked? If the evaluation shows that the project successfully improved access to clean water, you’ll want to document the strategies that made it work.\nWhat didn’t work? If some of the water pumps were not used, figure out why. Maybe they were located too far from people’s homes, or there wasn’t enough education on their benefits.\nWhat lessons can be learned? Every project teaches valuable lessons. If the community played a role in maintaining the water pumps, that could be an important insight for future projects.\n\nFinally, write a report summarizing your findings. Make sure it’s clear and easy to understand. For example, if you find that the water pumps reduced waterborne diseases by 30%, include that in your report alongside recommendations for future projects.\n\n\nFinal Thoughts\nBuilding an Evaluation Plan ensures that you can assess your project’s effectiveness and impact, learn from your experiences, and improve future projects. By setting clear evaluation questions, choosing appropriate data collection methods, and analyzing the findings, you’ll gain valuable insights into what worked and what didn’t."
  },
  {
    "objectID": "posts/Monitoring_and_Evaluation_wellness_health/index.html",
    "href": "posts/Monitoring_and_Evaluation_wellness_health/index.html",
    "title": "Monitoring and Evaluation in Health and Wellness: A Statistical Analysis Approach",
    "section": "",
    "text": "Monitoring and evaluation (M&E) play a pivotal role in improving health and wellness programs globally. In this analysis, I focus on coronary artery disease (CAD) prevention, vitamin B complex usage, and addressing common health concerns like dry cough and throat infections. Using data-driven methodologies, I explored how first aid practices, home remedies, and preventive strategies impact health outcomes.\nAs a researcher, I employed the STAR (Situation, Task, Action, Result) framework to structure the analysis. This method not only highlights the challenges and tasks in health monitoring but also suggests actionable solutions backed by statistical models. Using R and Python, I conducted a detailed examination of health indicators from datasets such as the Global Burden of Disease (GBD) dataset and health survey data from the World Health Organization (WHO).\n\nUnderstanding the Situation: Coronary Artery Disease and Vitamin B Complex\nCoronary artery disease (CAD) is a leading cause of mortality worldwide, and early detection and preventive measures are critical. One potential area of interest in preventive health is the role of Vitamin B complex in heart health. Several studies suggest that deficiencies in Vitamin B may contribute to homocysteine build-up, which is linked to an increased risk of CAD.\nThrough a monitoring lens, I examined the correlation between Vitamin B complex intake and CAD incidence across various populations. The data was sourced from the National Health and Nutrition Examination Survey (NHANES), which offers a comprehensive look at health trends in different demographic groups.\nUsing R for data cleaning and Python for machine learning algorithms, I analyzed how CAD risk decreases with increased Vitamin B complex intake. A linear regression model indicated a statistically significant relationship (p &lt; 0.05) between higher Vitamin B complex levels and lower CAD incidence. These results suggest a critical task for public health initiatives: improving Vitamin B intake as a preventive measure against CAD.\n\n\nTask: First Aid and Dry Cough Management\nHealth systems often focus on reactive measures rather than preventive actions. In the context of M&E, first aid practices can be essential for reducing complications in common health issues such as dry cough, which can evolve into severe throat infections if left untreated. By monitoring first aid practices, we can determine their effectiveness in controlling symptoms and preventing progression.\nTo assess this, I used data from the Health Information National Trends Survey (HINTS), which contains self-reported health management practices. The survey responses provided insights into how often individuals resort to home remedies for cough and whether these measures reduce the likelihood of progressing to more severe conditions like throat infections.\nIn Python, I applied a decision tree model to classify cases where first aid (including home remedies) successfully managed cough symptoms. The results showed that early first aid intervention, including the use of Vitamin B complex supplements, effectively reduced the duration and severity of dry cough (accuracy of 83%). This illustrates a strong potential for integrating first aid monitoring into wellness programs to track and evaluate effectiveness.\n\n\nAction: Home Remedies for Cough and Preventing Throat Infections\nHome remedies have long been used for managing coughs, particularly dry coughs that are linked to throat infections. Remedies such as honey, ginger, and steam inhalation are popular among populations that prefer non-pharmaceutical interventions. Monitoring the effectiveness of these remedies is important for understanding how well they work as a first line of defense before medical intervention is sought.\nTo assess the effectiveness of home remedies for cough, I used a sample dataset from Google Health Trends, which tracks search patterns related to health queries like “home remedies for cough” and “throat infection.” Using R, I conducted sentiment analysis and time-series forecasting to observe seasonal variations in the popularity of home remedies.\nThe analysis revealed spikes in searches during colder months, correlating with an increase in reported throat infections. Additionally, the data showed that home remedies were perceived as more effective for mild symptoms of dry cough, with a positive sentiment score of 65%. By monitoring these trends, health organizations can better understand public reliance on non-medical interventions and strategize accordingly.\n\n\nResults: Statistical Insights and Recommendations\nThe result of this comprehensive analysis provided valuable insights for health monitoring and evaluation efforts:\n\nVitamin B Complex and Coronary Artery Disease: The linear regression model demonstrated that Vitamin B complex supplementation significantly reduces the risk of CAD. This provides evidence for health programs to advocate for regular intake of this vitamin as part of CAD prevention strategies.\nFirst Aid and Cough Management: Monitoring first aid practices revealed that timely intervention with home remedies can prevent the progression of dry cough to throat infections. Decision tree analysis indicated that home-based care, when applied early, is effective for 83% of individuals experiencing cough symptoms.\nHome Remedies for Cough and Throat Infection: Time-series forecasting from Google Health Trends showed a clear seasonal pattern in home remedy usage, particularly during colder months. This insight allows public health initiatives to tailor awareness campaigns on cough management and throat infection prevention when these remedies are most in demand.\n\n\n\nConclusion\nThis analysis underscores the importance of data-driven monitoring and evaluation in health and wellness programs. From coronary artery disease prevention through Vitamin B complex supplementation to the management of dry cough and throat infections using home remedies, the integration of statistical methodologies offers clear, actionable insights. By leveraging data from sources like NHANES, HINTS, and Google Health Trends, public health organizations can improve the effectiveness of their programs and interventions.\nUltimately, regular monitoring, coupled with evaluation using advanced analytical tools such as R and Python, enables better health outcomes and resource allocation in wellness programs. As the next step, health professionals should consider implementing real-time monitoring dashboards to track the effectiveness of home remedies, first aid interventions, and vitamin supplementation across diverse populations.\n\n\nReferences\n\nNational Health and Nutrition Examination Survey (NHANES)\nReference for Vitamin B complex intake and coronary artery disease (CAD):\n\nCenters for Disease Control and Prevention (CDC). (2021). National Health and Nutrition Examination Survey (NHANES). Retrieved from: https://www.cdc.gov/nchs/nhanes/index.htm\n\nGlobal Burden of Disease (GBD) Dataset\nReference for coronary artery disease global trends and burden:\n\nInstitute for Health Metrics and Evaluation (IHME). (2020). Global Burden of Disease Study 2019 (GBD 2019) Results. Seattle, United States: IHME. Available from: http://ghdx.healthdata.org/gbd-results-tool\n\nHealth Information National Trends Survey (HINTS)\nReference for first aid and dry cough management:\n\nNational Cancer Institute (NCI). (2022). Health Information National Trends Survey (HINTS). Retrieved from: https://hints.cancer.gov/\n\nGoogle Health Trends\nReference for search trends related to home remedies for cough and throat infections:\n\nGoogle. (2024). Google Health Trends Dataset. Retrieved from: https://trends.google.com/trends/\n\nHomocysteine and Coronary Artery Disease Study\nReference for the relationship between homocysteine levels, Vitamin B complex, and CAD:\n\nClarke, R., Halsey, J., Lewington, S., & et al. (2010). Homocysteine and Coronary Heart Disease: Meta-analysis of 30 Studies. JAMA. 288(16):2015–2022. doi:10.1001/jama.288.16.2015"
  },
  {
    "objectID": "posts/Olympics gender equality analysis/index.html",
    "href": "posts/Olympics gender equality analysis/index.html",
    "title": "Comparisons 01: Gender Equality Analysis Olympics",
    "section": "",
    "text": "Inspired by Georgios KaramanisThis Chart is a contribution to day 1 of the hashtag#30DayChartChallengeHere are facts about the achievement of gender equality reported by www.olympic.com\nYou can get the code here on github 2024 Day 1 chart challenge\n\nCode snippet for Chart:\nlibrary(tidyverse)\nlibrary(ggforce)\nlibrary(camcorder)\n\ngg_record(dir = here::here(\"2024/01/\"), device = \"png\", width = 1080 * 2, height = 1350 * 2, units = \"px\", dpi = 320)\n\n# https://olympics.com/en/news/paris-2024-first-games-to-achieve-full-gender-parity\n\nr &lt;- 1.3\n\n\n\nwomen &lt;- tribble(\n ~olympics, ~year, ~pct, ~x0, ~y0, ~col,\n  \"Tokyo\", 1964, 13, 2*r - r/2, 3*r, \"#0081C8\",\n  \"Tokyo\", 2020, 48.7, 4*r, 3*r, \"black\",\n  \"Beijing\", 2022, 45, 6*r + r/2, 3*r, \"#EE334E\",\n  \"Bueno Aires\", 2018, 50, 3*r - r/4, 2*r, \"#FCB131\",\n  \"Lausanne\", 2020, 50, 5*r + r/4, 2*r, \"#00A651\"\n)\n\nf1 &lt;- \"Graphik\"\nf1b &lt;- \"Graphik Compact\"\nf2 &lt;- \"Produkt\"\nf2b &lt;- \"Produkt Medium\"\n\nggplot(women) +\n  # geom_circle(aes(x0 = x0, y0 = y0, r = 1.3, colour = col), linewidth = 8) +\n  geom_vline(aes(xintercept = x0), alpha = 0.1, linetype = \"dashed\") +\n  geom_arc_bar(aes(x0 = x0, y0 = y0, r = 1.5, r0 = 1.1, start = 0, end = 2 * pi), fill = \"grey99\", color = NA) +\n  geom_arc_bar(aes(x0 = x0, y0 = y0, r = 1.5, r0 = 1.1, fill = col, start = 0, end = pct * 2 * pi / 100), color = NA) +\n  geom_text(aes(x0, y0 + if_else(year %in% c(1932, 1992), -2.5, 2.5), label = paste0(olympics, year, \"\\n\", pct, \"%\"), color = col), lineheight = 0.9, family = f1, size = 5, fontface = \"bold\") +\n  annotate(\"text\", 5.25, 9, label = \"Olympics Gender Parity\\nWomen Equality\", family = f2b, color = \"purple4\", size = 14, lineheight = 0.8) +\n  annotate(\"text\", 5.25, -2, label = paste0(\"Source: www.olympics.com, \", 2023, \"\\n\", \"Graphic: VICTOR MANDELA\"), family = f2, color = \"purple4\") +\n  scale_color_identity() +\n  scale_fill_identity() +\n  coord_fixed(xlim = c(0.5, 10), ylim = c(-2, 10)) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"#C9E4DE\", color = NA)\n  )\nOlympic Movement: Achievements in gender equality\n\nParis 1900: Female athletes first took part in the Olympic Games, four years after the first modern Olympics took place in Athens\n1996: Promotion of women becomes a mission of the IOC and is enshrined in the Olympic Charter\nTokyo 2020: The last edition of the Games were the most gender-balanced to date with 48.7 per cent of athletes women. At Tokyo 1964, only 13 per cent of the athletes were women.\nTokyo 2020: Following a rule change allowing one male and one female athlete to jointly carry their flag during the Opening Ceremony, 91 per cent of NOCs had a female flag bearer\nTokyo 2020: Three disciplines achieved gender balance (BMX racing, mountain biking and freestyle wrestling)\n\n\n\n\nBeijing\n\n\n\nBeijing 2022: The last Olympic Winter Games were the most gender balanced to date with 45 per cent female athletes\nParis 2024: Out of the 10,500 athletes participating in the Games, 5,250 will be men and 5,250 women. These Games will be the first to reach full gender parity in terms of number of athletes.\nFemale IOC membership currently stands at 40 per cent, up from 21 per cent at the start of the Olympic Agenda 2020\n\n\n\n\nLausanne\n\n\n\nYouth Olympic Games: The Youth Games Buenos Aires 2018 and Winter Youth Games Lausanne 2020 reached full gender parity in overall athlete participation (2,000 athletes per gender in 2018 and 936 in 2020)\n\n\n\nBueno Aires 2018\n\n\nFemale representation on the IOC Executive Board stands at 33.3 per cent, versus 26.6 per cent before the Olympic Agenda 2020\n50 per cent of the members of IOC Commissions positions have been held by women since 2022, compared with 20.3 per cent prior to the Olympic Agenda 2020. In addition, a record high of 13 of the 31 commissions were chaired by women in 2022."
  },
  {
    "objectID": "posts/Project management/index.html",
    "href": "posts/Project management/index.html",
    "title": "Mastering Project Management: A Simple Guide to Success",
    "section": "",
    "text": "Mastering Project Management: A Simple Guide to Success\n\nIntroduction:\nHello, project enthusiasts! Whether you’re a seasoned project manager or someone stepping into the world of coordinating tasks, we’re here to make project management a breeze. In this easy-to-understand guide, we’ll explore how to boost your competence, ensure timely delivery, maintain high-quality output, and foster great communication. Let’s break it down without the jargon and make project success accessible to all!\n1. “Competence: The ABCs of Mastery”\n\n\n\n\n\nImagine project management as a skill playground. Start by learning the basics – understand your project’s objectives, scope, and team dynamics. Familiarize yourself with project management tools; they’re like the cool kids’ toys that make your job easier. As you play in this skill playground, you’ll naturally enhance your competence over time.\n2. “Timely Delivery: The Art of Time Management”\n\n\n\n\n\nThink of project timelines as a recipe – break down the tasks into bite-sized pieces. Create a timeline that’s realistic and achievable. Use a calendar or a project management tool to keep everyone on the same page. Remember, punctuality is the secret ingredient to successful project delivery.\n3. “Quality Output: Crafting a Masterpiece”\n\n\n\n\n\nQuality output is like baking a cake – you need the right ingredients and a foolproof recipe. Clearly define project requirements, encourage collaboration, and perform regular checks to ensure everything is baking – or, in this case, developing – smoothly. The result? A deliciously successful project!\n4. “Great Communication: The Art of Conversation”\n\n\n\n\n\nPicture project communication as a friendly chat. Be clear, concise, and approachable. Share updates regularly and encourage open dialogue. Remember, good communication is a two-way street. Listen as much as you talk, and you’ll find your project sailing smoothly.\n5. “Collaboration: Teamwork Makes the Dream Work”\n\n\n\n\n\nThink of your project team as a well-oiled machine. Encourage collaboration by creating a supportive environment. Clearly define roles, communicate openly, and celebrate achievements together. Remember, everyone plays a crucial part in the success of the project.\n\nConclusion:\nProject management doesn’t have to be a complex puzzle. Break it down into manageable steps, like assembling a Lego set. Start with the basics, manage your time wisely, focus on delivering quality, communicate openly, and foster teamwork. By simplifying project management, you’re not just ensuring success; you’re making the entire process enjoyable for everyone involved. So, grab your toolkit, put on your chef’s hat, and let’s cook up some project success together!"
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "",
    "text": "Why Analyze Your Data? Analyzing data collected through CAPI is crucial for drawing meaningful insights and making informed decisions based on your research findings.\nSteps for Analyzing CAPI Data\n\nExporting Data: Once data collection is complete, you can export the data from SurveyCTO in formats like CSV or Excel for analysis.\nExample: After finishing a survey on community health, export the data to Excel for further analysis.\nUsing Statistical Software: You can import the exported data into statistical software such as R, SPSS, or Excel to perform various analyses.\nExample: In R, you might run a simple analysis to determine the average number of doctor visits per household and visualize the results with a bar chart.\nInterpreting Results: Look for trends, patterns, and outliers in the data to inform your conclusions.\nExample: If most respondents report visiting a doctor at least once a year, you can conclude that access to healthcare is relatively good in the community."
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#analyzing-capi-data-in-surveycto",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#analyzing-capi-data-in-surveycto",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "",
    "text": "Why Analyze Your Data? Analyzing data collected through CAPI is crucial for drawing meaningful insights and making informed decisions based on your research findings.\nSteps for Analyzing CAPI Data\n\nExporting Data: Once data collection is complete, you can export the data from SurveyCTO in formats like CSV or Excel for analysis.\nExample: After finishing a survey on community health, export the data to Excel for further analysis.\nUsing Statistical Software: You can import the exported data into statistical software such as R, SPSS, or Excel to perform various analyses.\nExample: In R, you might run a simple analysis to determine the average number of doctor visits per household and visualize the results with a bar chart.\nInterpreting Results: Look for trends, patterns, and outliers in the data to inform your conclusions.\nExample: If most respondents report visiting a doctor at least once a year, you can conclude that access to healthcare is relatively good in the community."
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#case-studies-of-successful-capi-implementations",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#case-studies-of-successful-capi-implementations",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "2. Case Studies of Successful CAPI Implementations",
    "text": "2. Case Studies of Successful CAPI Implementations\nLearning from Real-World Examples Examining successful CAPI projects can provide valuable insights and inspire your research initiatives.\nExample Case Study 1: Household Energy Use Survey A research team conducted a survey to understand energy consumption in rural households. By using CAPI, they efficiently gathered data from over 500 households within a month. The findings highlighted a significant shift towards solar energy, informing local government policies on renewable energy initiatives.\nExample Case Study 2: Agricultural Practices Assessment Another project focused on assessing farming practices. Enumerators collected data using GPS to identify the locations of farms and their practices. The resulting data provided insights into crop diversity and soil health, which were used to develop training programs for farmers."
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#common-challenges-in-capi-and-how-to-overcome-them",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#common-challenges-in-capi-and-how-to-overcome-them",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "3. Common Challenges in CAPI and How to Overcome Them",
    "text": "3. Common Challenges in CAPI and How to Overcome Them\nIdentifying Common Challenges While CAPI offers many benefits, researchers may encounter challenges during implementation.\nChallenge 1: Technical Issues Technical problems, such as device malfunctions or software glitches, can hinder data collection.\nSolution: Train enumerators on troubleshooting common issues and provide backup devices when possible.\nExample: If a tablet freezes during an interview, the enumerator should know how to restart it quickly to minimize disruption.\nChallenge 2: Respondent Reluctance Some respondents may be hesitant to participate or provide honest answers.\nSolution: Build rapport and trust with respondents. Clearly explain the survey’s purpose and how their input will be used.\nExample: An enumerator can say, “We’re here to understand your community’s needs better. Your answers will help us improve services and resources.”"
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#future-trends-in-capi-and-digital-data-collection",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#future-trends-in-capi-and-digital-data-collection",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "4. Future Trends in CAPI and Digital Data Collection",
    "text": "4. Future Trends in CAPI and Digital Data Collection\nLooking Ahead: Emerging Trends As technology continues to evolve, the landscape of data collection will also change. Here are some future trends to watch:\n\nIncreased Use of Artificial Intelligence: AI can enhance data analysis by identifying patterns and trends more quickly.\nExample: Imagine an AI tool that can automatically analyze survey responses and generate reports on community health trends, saving researchers hours of manual analysis.\nIntegration of Real-Time Data Collection: Real-time data collection will become more common, allowing researchers to adjust their approaches based on immediate findings.\nExample: During a survey on public transport usage, researchers could adapt their questions based on initial results, focusing on areas with low satisfaction rates.\nMobile Technology Advancements: With ongoing advancements in mobile technology, CAPI tools will become more user-friendly and accessible.\nExample: Future versions of SurveyCTO may include voice recognition features, allowing enumerators to collect responses simply by speaking, further reducing errors and enhancing efficiency."
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#user-feedback-and-iterative-design-in-surveycto",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#user-feedback-and-iterative-design-in-surveycto",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "5. User Feedback and Iterative Design in SurveyCTO",
    "text": "5. User Feedback and Iterative Design in SurveyCTO\nThe Importance of User Feedback Incorporating feedback from enumerators and respondents is essential for improving survey design and implementation.\nExample: After conducting a pilot survey, enumerators might report that a specific question is confusing. Adjusting the question based on this feedback can enhance clarity and improve response quality.\nIterative Design Process Adopting an iterative design approach means continuously improving your survey instrument based on ongoing feedback and testing.\nExample: If initial responses indicate that a question about household income is sensitive and leads to non-responses, you could rephrase it to ask about general income ranges instead, making respondents more comfortable."
  },
  {
    "objectID": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#training-field-enumerators-for-capi",
    "href": "posts/Survey_CTO_Analysis, Challenges, and Future Trends/index.html#training-field-enumerators-for-capi",
    "title": "Survey CTO Analysis, Challenges, and Future Trends",
    "section": "6. Training Field Enumerators for CAPI",
    "text": "6. Training Field Enumerators for CAPI\nThe Role of Training in Success Training field enumerators effectively is crucial for successful data collection.\nKey Components of Training\n\nTechnical Skills: Ensure enumerators are comfortable using the CAPI software and devices.\nExample: Conduct hands-on workshops where enumerators practice using SurveyCTO to enter sample data and navigate through various question types.\nInterviewing Techniques: Teach enumerators effective interviewing strategies to build rapport and encourage participation.\nExample: Role-playing exercises can help enumerators practice their approach, allowing them to refine their techniques in a supportive environment.\nUnderstanding Ethics and Privacy: Enumerators must understand the ethical implications of data collection and how to protect respondents’ privacy.\nExample: Provide guidance on obtaining informed consent and handling sensitive information responsibly.\n\nConclusion By analyzing data effectively, learning from successful case studies, addressing common challenges, and preparing for future trends, researchers can enhance their CAPI projects. Training enumerators and incorporating user feedback are vital steps in ensuring high-quality data collection. Stay tuned for more insights and practical tips in our upcoming posts!"
  },
  {
    "objectID": "posts/Survey_CTO_Foundational Knowledge and Setup/index.html",
    "href": "posts/Survey_CTO_Foundational Knowledge and Setup/index.html",
    "title": "Survey CTO Foundational Knowledge and Setup",
    "section": "",
    "text": "1. Introduction to CAPI and SurveyCTO\nWhat is CAPI? Computer-Assisted Personal Interviewing (CAPI) is a method where interviewers use electronic devices (like tablets or smartphones) to conduct surveys. Instead of paper forms, they enter responses directly into software, making data collection faster and more efficient.\nExample: Imagine a team conducting a health survey in a rural area. Using CAPI, an interviewer can walk into a household with a tablet, ask questions about family health practices, and record the answers on the spot, eliminating the need for paper forms.\nWhy Use CAPI?\n\nEfficiency: CAPI allows real-time data entry, reducing errors and speeding up the collection process.\nFlexibility: Interviewers can easily navigate between questions based on previous answers.\nData Quality: Immediate validation checks help ensure that the data collected is accurate.\n\nExample: If a respondent indicates that they have a chronic illness, the CAPI system can automatically trigger additional questions related to treatment and management, ensuring comprehensive data collection.\nWhat is SurveyCTO? SurveyCTO is a powerful tool for designing and implementing CAPI surveys. It provides a user-friendly interface, allowing researchers to create surveys without needing advanced technical skills.\n\n2. Step-by-Step Guide to Setting Up CAPI with SurveyCTO\nStep 1: Create a Survey Account Start by signing up for a SurveyCTO account. The platform offers a free trial, which is great for getting familiar with its features.\nExample: Go to the SurveyCTO website, click “Sign Up,” and fill in your details. You can explore the dashboard to see various options available.\nStep 2: Design Your Survey\n\nUse the Form Designer: Create questions using different types (e.g., multiple choice, text, rating scales).\nExample: If you’re surveying about dietary habits, you might include a multiple-choice question: “What is your primary source of protein?” with options like “Meat,” “Fish,” “Beans,” and “Nuts.”\nAdd Logic: Use branching logic to direct respondents based on their answers. For instance, if a respondent answers “yes” to “Do you eat fish regularly?” you can automatically show them a follow-up question about their preferred types of fish.\n\nStep 3: Test Your Survey Before launching, test your survey to catch any errors. Use the preview feature to see how the survey will look on mobile devices.\nExample: Conduct a test run with a colleague to identify any confusing questions or technical issues. Make necessary adjustments based on their feedback.\nStep 4: Deploy Your Survey Once everything looks good, deploy your survey to field workers. They can download it to their devices and start collecting data!\nExample: If your team is conducting a survey on education, field workers can download the survey onto their tablets and head to schools to gather data from students and teachers.\n\n\n3. Best Practices for Designing Survey Instruments\n1. Keep It Simple Use clear and concise language. Avoid jargon and complex terms. Make sure respondents understand the questions easily.\nExample: Instead of asking, “How often do you engage in physical activity?” consider rephrasing it to “How many days a week do you exercise?”\n2. Limit the Length Shorter surveys tend to have higher completion rates. Aim for around 10-15 minutes of survey time.\nExample: If you’re gathering feedback on a community program, keep your survey to 10 questions to encourage more respondents to complete it.\n3. Use a Mix of Question Types Combine different types of questions to keep the survey engaging. Use multiple-choice questions for quick answers and open-ended questions for more detailed feedback.\nExample: After a multiple-choice question about preferred community services, include an open-ended question like “What other services would you like to see in the community?”\n4. Pilot Your Survey Before rolling it out to everyone, conduct a pilot test with a small group. Gather feedback on clarity and user experience, and make necessary adjustments.\nExample: If you’re surveying urban residents, test your survey with a small group from different neighborhoods to ensure the questions resonate with diverse perspectives.\n5. Think About Mobile Use Since CAPI is conducted on mobile devices, design your survey to be mobile-friendly. Ensure buttons are large enough to click, and text is readable on small screens.\nExample: Use larger fonts and clear buttons for answering questions to make navigation easy for interviewers in the field, especially when using tablets in bright sunlight.\nConclusion CAPI and SurveyCTO provide researchers with an innovative and efficient way to collect data. By understanding the basics and following best practices, you can create effective surveys that yield high-quality data. In the next posts, we’ll dive deeper into data collection strategies and how to manage the information you gather. Stay tuned!"
  },
  {
    "objectID": "posts/Team morale/index.html",
    "href": "posts/Team morale/index.html",
    "title": "Boosting Team Performance",
    "section": "",
    "text": "🚀 Boosting Team Performance: Lessons Learned from My Journey in Monitoring & Evaluation 🌟\n\n\nIn the fast-paced world of Monitoring and Evaluation (M&E), staying ahead requires constant innovation and dedication. After more than six years of navigating this field, making countless mistakes, and learning the hard way, I've finally honed some key insights that have significantly improved team performance and project outcomes. Here's what I've learned and want to share with you in less than 2 minutes.\n\n\n📚 Comprehensive Training Materials & Protocols: A Game Changer\nOne of the major milestones in my career has been the development of comprehensive training materials and protocols. After many trial and error moments, the results finally showed—a 40% increase in staff proficiency and adherence to established procedures! This improvement not only streamlined our workflow but also enhanced the quality of our outputs. 🎯\n\n\n\n\n\n🛠️ Leading Capacity Building: Empowering Through Knowledge\nIn my early years, I underestimated the power of proper capacity building. But leading efforts in drafting research methodology, sampling designs, research design, and creating CAPI questionnaires has been transformative. This process has empowered our team to deploy these tools effectively, ensuring that we maintain high standards in our research activities. 💡\n\n\n\n\n\n🔍 Coordinating Data Collection & Analysis: From Numbers to Narratives\nCoordinating data collection, analysis, interpretation, and presentation of research and evaluation results used to be a daunting task. I made many mistakes along the way. However, with persistence and learning, I now transform raw data into actionable insights that drive decision-making and strategic planning. 📊\n\n\n\n\n\n🤝 Cross-Departmental Collaboration: Enhancing Synergy\nSupporting other departments through monitoring, research, statistical analysis, and implementation meetings has taught me the value of collaboration. Early on, I often worked in silos, missing out on valuable insights. Now, these brainstorming sessions have led to innovative solutions and enhanced project outcomes. 🌐\n\n\n\n\n\n🎯 Executing Research Projects: Measuring Impact\nExecuting and implementing research projects that systematically investigate the impact of our platform was another area where I faced challenges. By embracing quasi-experimental or experimental study designs, we've now been able to provide robust evidence of our platform's effectiveness. This scientific approach ensures that our findings are reliable and actionable. 🧪\n\n\n\n\n\n\n🌟 Takeaway: Innovation and Collaboration are Key\nThe key to success in M&E lies in continuous learning and collaboration. By developing effective training protocols, leading capacity building, and fostering cross-departmental synergy, we can achieve significant improvements in performance and outcomes.\n🔗 What are your thoughts? Share your experiences or insights in the comments below! Let's keep the conversation going. 💬"
  },
  {
    "objectID": "posts/Wildlife tourism revenue/index.html",
    "href": "posts/Wildlife tourism revenue/index.html",
    "title": "The Growing Importance of Wildlife Economy: A Path to Sustainable Growth",
    "section": "",
    "text": "The Growing Importance of Wildlife Economy: A Path to Sustainable Growth\nThe global wildlife economy plays a crucial role in fostering sustainable development, preserving biodiversity, and providing economic benefits to local communities. By closely monitoring metrics like biodiversity health, tourism revenue, and local employment in wildlife enterprises, stakeholders—governments, donors, and investors—can effectively support a balance between economic growth and conservation. This blog post breaks down the significance of these metrics and why they are essential in shaping policies and investment strategies for wildlife economies.\n\nUnderstanding the Metrics that Matter\nIn the context of the wildlife economy, three critical metrics stand out as indicators of both ecological and economic health:\n\n\nBiodiversity Health Index\nThis index provides insight into the status of wildlife populations and ecosystems. It tracks the number of species, their population trends, and conservation status. Understanding biodiversity health is essential to ensuring the long-term viability of wildlife tourism and related industries. If species are endangered or habitats are declining, the economic benefits from these activities will eventually diminish. This index helps determine the future sustainability of wildlife-based economies, guiding conservation policies and actions (FAO, 2020).\n\nTourism Revenue from Wildlife-Based Activities\nWildlife tourism is a significant revenue source, especially for countries rich in biodiversity, such as Kenya, Tanzania, and Australia. The revenue generated from activities like safaris, ecotourism, and marine wildlife experiences helps local and national economies thrive. For instance, in 2023, Kenya and South Africa experienced a growth of up to 25% in their wildlife tourism revenue, highlighting the importance of sustainable tourism in post-pandemic recovery. Monitoring tourism revenue is critical as it provides governments and investors with data to craft policies that support tourism infrastructure, promote conservation, and ensure long-term growth in the sector (WTTC, 2023).\n\nLocal Community Employment and Income from Wildlife Enterprises\nThis metric assesses how wildlife enterprises contribute to the livelihoods of local communities. Activities like ecotourism, wildlife management, and conservation provide employment opportunities and generate income, especially in rural areas. The positive economic impacts from these activities are evident in countries like Uganda and Costa Rica, where ecotourism has seen significant growth, with revenue increases of 15-20% from 2022 to 2023. For governments and donors, this metric underscores the role of wildlife enterprises in poverty alleviation and community development. Supporting these activities with funding and infrastructure can help lift communities out of poverty while promoting sustainable wildlife management practices (UNWTO, 2022).\n\n\n\nThe 2022-2023 Revenue Shift: What It Means\nThe wildlife tourism industry has shown remarkable resilience and growth in recent years, especially between 2022 and 2023. Countries heavily dependent on wildlife tourism, such as Kenya, South Africa, and Tanzania, experienced significant increases in revenue—ranging from $900M to $1.7B—as tourists returned post-pandemic. This surge demonstrates the sector’s potential to drive economic recovery and growth.\nHere’s a breakdown of how the revenue shifted from 2022 to 2023 across a selection of countries:\n\n\n\n\n\n\n\n\n\nCountry\n2022 Revenue (USD)\n2023 Revenue (USD)\nChange (%)\n\n\n\n\nKenya\n$1.2B\n$1.5B\n+25%\n\n\nSouth Africa\n$900M\n$1.1B\n+22%\n\n\nTanzania\n$750M\n$900M\n+20%\n\n\nAustralia\n$1.5B\n$1.7B\n+13%\n\n\nMauritius\n$300M\n$410M\n+37%\n\n\nBotswana\n$320M\n$450M\n+41%\n\n\n\n\nThis chart clearly shows the growing importance of wildlife tourism in global economies, particularly in African and island nations. Governments, donors, and investors should recognize the need to continue investing in conservation efforts, sustainable tourism infrastructure, and local community development to sustain this growth.\n\n\nImplications for Governments, Donors, and Investors\n\nFor Governments:\nGovernments should focus on creating policies that not only protect biodiversity but also ensure that the revenue generated from wildlife tourism is reinvested into conservation and local communities. Strengthening tourism infrastructure, offering incentives for sustainable practices, and implementing environmental protection laws are essential to maintaining the growth of this sector (World Bank, 2022).\nFor Donors:\nDonors can contribute by funding community-based conservation programs, supporting sustainable wildlife tourism initiatives, and helping build local capacity. Their investments can help drive the inclusion of local communities in wildlife tourism, enhancing their participation in conservation efforts while simultaneously improving their livelihoods (Global Environment Facility, 2023).\nFor Investors:\nInvestors should recognize wildlife tourism as a profitable sector with significant growth potential. With revenue increases and demand for sustainable experiences on the rise, investors can explore opportunities in eco-resorts, wildlife conservation partnerships, and eco-tourism ventures. Strategic investments will ensure long-term returns while promoting conservation (International Finance Corporation, 2022).\n\n\n\nConclusion\nThe wildlife economy is a dynamic and critical sector for both biodiversity and economic development. By focusing on key metrics like biodiversity health, tourism revenue, and local community benefits, governments, donors, and investors can work together to promote sustainable growth that benefits both people and wildlife. As the sector continues to rebound post-pandemic, these efforts will ensure that wildlife economies thrive for generations to come.\n\n\nReferences and Further Reading\n\nFAO. (2020). The State of World’s Biodiversity for Food and Agriculture. Food and Agriculture Organization. Link to report.\nWTTC. (2023). Economic Impact Report 2023. World Travel and Tourism Council. Link to report.\nUNWTO. (2022). Tourism and Biodiversity Report. United Nations World Tourism Organization. Link to report.\nWorld Bank. (2022). Tourism for Sustainable Development. Link to report.\nGlobal Environment Facility. (2023). Funding Biodiversity Conservation Projects. Link to report.\nInternational Finance Corporation. (2022). Sustainable Investment in Tourism. Link to report."
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#setting-up-and-installation",
    "href": "posts/lift_practical_anlalysis/index.html#setting-up-and-installation",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Setting up and Installation",
    "text": "Setting up and Installation\n\nlist.of.packages=c(\"tidyverse\",\"readxl\",\"nnet\", \"lubridate\", \"maps\")\nnew.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages,dependencies=T)\nsuppressMessages({\n  library(tidyverse)\n  library(readxl)\n  library(nnet)\n  library(lubridate)\n  library(maps)\n})\n\n# set theme\nmytheme &lt;- theme(plot.title=element_text(face=\"bold.italic\",\n                                           size=\"14\", color=\"aquamarine4\"),\n                   axis.title=element_text(face=\"bold.italic\",\n                                           size=10, color=\"aquamarine4\"),\n                   axis.text=element_text(face=\"bold\", size=9,\n                                          color=\"darkblue\"),\n                   panel.background=element_rect(fill=\"white\",\n                                                 color=\"darkblue\"),\n                   panel.grid.major.y=element_line(color=\"grey\",\n                                                   linetype=1),\n                   panel.grid.minor.y=element_line(color=\"grey\",\n                                                   linetype=2),\n                   panel.grid.minor.x=element_blank(),\n                   legend.position=\"top\")\n\nLinks to the data here\n\ndiary1 &lt;- read_xlsx(\"Biweekly1 Final.xlsx\")\ndiary2 &lt;- read_xlsx(\"Biweekly2 Final.xlsx\")\ndiary3 &lt;- read_xlsx(\"Biweekly3__Final_2016_18_11_10_12.0.xlsx\")"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#merging-biweekly-datasets",
    "href": "posts/lift_practical_anlalysis/index.html#merging-biweekly-datasets",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Merging Biweekly Datasets",
    "text": "Merging Biweekly Datasets\nThe first task in any longitudinal analysis is to combine data from multiple periods into a single dataset. By merging three biweekly datasets, we can create a comprehensive view of our respondents’ data over time. Merging by Respondent ID ensures that all responses are aligned for each individual.\n\nmerged_df &lt;- diary1 %&gt;% \n  inner_join(diary2, by = c(\"Respondent_ID\")) %&gt;% #join using respondent_ID\n  mutate_at(\"Respondent_ID\", as.character) %&gt;% #Harmonize the data types by changing into character the joining column\n  full_join(diary3, by = c(\"Respondent_ID\"))"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#subsetting-data-for-income-related-questions",
    "href": "posts/lift_practical_anlalysis/index.html#subsetting-data-for-income-related-questions",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Subsetting Data for Income-Related Questions",
    "text": "Subsetting Data for Income-Related Questions\nThe dataset contains numerous variables, but we are specifically interested in question A_4, which asks about income earned. This multiple-choice question requires subsetting relevant columns from the merged dataset. The codebook guides this process, allowing us to focus on the specific data we need for analysis.\n\nmerged_df %&gt;% \n  select_at(vars(contains(\"A_4_\"))) -&gt; income_df"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#creating-a-frequency-table-for-income-data",
    "href": "posts/lift_practical_anlalysis/index.html#creating-a-frequency-table-for-income-data",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Creating a Frequency Table for Income Data",
    "text": "Creating a Frequency Table for Income Data\nA simple yet effective way to understand income distribution is by creating a frequency table. This table will show how many respondents selected each option for income, providing an overview of income sources.\n\nincome_prep &lt;- income_df %&gt;% \n  gather(key = \"question_name\", value = \"response\") %&gt;% #melting the data\n  mutate(response = replace_na(response, 99)) %&gt;% #convert NA to 99\n  group_by(question_name) %&gt;% \n  count(response, name = \"frequency\") \n\nincome_table &lt;- income_prep %&gt;% \n  ungroup() %&gt;% \n  spread(response, frequency) %&gt;% \n  arrange(`0`, `1`, `99`)\n\nincome_table\n\n# A tibble: 48 × 4\n   question_name   `0`   `1`  `99`\n   &lt;chr&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 A_4_6.y         375   121    68\n 2 A_4_6.x         379   117    68\n 3 A_4_15.x        383   113    68\n 4 A_4_15.y        387   109    68\n 5 A_4_15          400   136    28\n 6 A_4_2.x         410    86    68\n 7 A_4_2.y         419    77    68\n 8 A_4_6           422   114    28\n 9 A_4_9.y         427    69    68\n10 A_4_9.x         437    59    68\n# ℹ 38 more rows"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#visualizing-income-data-with-a-bar-graph",
    "href": "posts/lift_practical_anlalysis/index.html#visualizing-income-data-with-a-bar-graph",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Visualizing Income Data with a Bar Graph",
    "text": "Visualizing Income Data with a Bar Graph\nOnce the frequency table is generated, we can visualize the data. A bar graph makes it easy to compare the number of respondents across different income categories. The title for this graph is “Count of Respondents Having an Income Earning Activity,” offering clear insight into income distribution.\n\n#preparing the data for plot\nprep_income &lt;- income_prep %&gt;% \n  mutate_at(vars(\"response\"), as.factor) %&gt;%\n  separate(question_name, into = c(\"question\", \"diary_period\"), sep = \"([.])\") %&gt;%\n  mutate(diary_period = replace_na(diary_period, \"One\"),\n         diary_period = case_when(diary_period == \"x\" ~ \"Two\",\n                                  diary_period == \"y\" ~ \"Three\",\n                                  TRUE ~ diary_period),\n         question = str_extract(question, \"[:digit:]+\\\\b\")) \n\n#Plot\n prep_income %&gt;% \n   ggplot(aes(diary_period, frequency, fill = response)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~question, scales = \"free_y\") +\n  labs(title = \"Count of respondents having an income earning activity\",\n       y = \"Frequency of Response\",\n       x = \"Period of Diary\",\n       fill = \"Key\")"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#investigating-the-relationship-between-stress-and-wealth-ppi-score",
    "href": "posts/lift_practical_anlalysis/index.html#investigating-the-relationship-between-stress-and-wealth-ppi-score",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Investigating the Relationship Between Stress and Wealth (PPI Score)",
    "text": "Investigating the Relationship Between Stress and Wealth (PPI Score)\nStress levels (Q_91) can be associated with the wealth proxy variable “ppicut,” providing valuable insight into the socio-economic factors affecting stress. A correlation analysis between these two variables reveals whether wealth status influences stress levels among respondents.\n\n#load the segment data\nsegment_df &lt;- read_xlsx(\"segmentation variables.xlsx\")\n\n#join the new data to the merged\njoined &lt;- segment_df %&gt;% \n  mutate(Respodent_ID = as.character(Respodent_ID)) %&gt;% \n  inner_join(merged_df, by = c(\"Respodent_ID\" = \"Respondent_ID\")) \n\nmultinomial_df &lt;- joined %&gt;% \n  select(starts_with(\"Q_91\"), ppicut) %&gt;% \n  mutate(ppicut = as.factor(ppicut),\n         ppicut = relevel(ppicut, ref = \"poor\")) #make the poor status the reference\n\n# The multinomial model and summary\nmulti_model &lt;- multinom(ppicut ~ ., \n             data = multinomial_df)\n\n# weights:  25 (16 variable)\ninitial  value 753.216943 \niter  10 value 505.215239\niter  20 value 497.300727\nfinal  value 497.290157 \nconverged\n\nsummary(multi_model)\n\nCall:\nmultinom(formula = ppicut ~ ., data = multinomial_df)\n\nCoefficients:\n            (Intercept)     Q_91.x        Q_91.y       Q_91\ncomfortable -1.62871133 0.03750504  0.0635473286  0.2961931\nultra poor  -3.17084979 0.23501748 -0.0992323354 -0.2142214\nupper poor  -0.08364761 0.17288377 -0.0006068483  0.1182401\nwealthy     -0.66441374 0.02458017 -0.3704320573 -0.3862365\n\nStd. Errors:\n            (Intercept)    Q_91.x    Q_91.y      Q_91\ncomfortable   0.5525604 0.1315151 0.1376271 0.1374913\nultra poor    1.8341828 0.4476755 0.4807289 0.4854138\nupper poor    0.4122153 0.1013992 0.1061545 0.1052570\nwealthy       1.2398195 0.3318420 0.3794236 0.3779930\n\nResidual Deviance: 994.5803 \nAIC: 1026.58"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#visualizing-the-stress-wealth-relationshipusing-a-multinomial-plot",
    "href": "posts/lift_practical_anlalysis/index.html#visualizing-the-stress-wealth-relationshipusing-a-multinomial-plot",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Visualizing the Stress-Wealth Relationship(Using a Multinomial Plot)",
    "text": "Visualizing the Stress-Wealth Relationship(Using a Multinomial Plot)\nA scatter plot or similar graph is a powerful way to visualize the relationship between stress levels and PPI score. This graphical representation helps us better understand the distribution of data points and any potential correlation.\n\nmultinomial_df %&gt;% \n  gather(value, key, -ppicut) %&gt;% \n  ggplot(aes(key, ppicut, color = key)) +\n  geom_count() +\n  facet_wrap(~value)"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#surveyor-performance-analysis-average-time",
    "href": "posts/lift_practical_anlalysis/index.html#surveyor-performance-analysis-average-time",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Surveyor Performance Analysis (Average time)",
    "text": "Surveyor Performance Analysis (Average time)\nUsing the VStart and VEnd variables, we can calculate the average time each surveyor (Srvyr) spent on interviews. This performance metric is crucial for evaluating the efficiency of data collection and understanding how long respondents take to complete surveys.\n\n## preparing the data\nsurveyor_prep &lt;- joined %&gt;% \n  select(starts_with(c(\"Vstart\", \"VEnd\")), Srvyr.x.x) %&gt;% \n  drop_na()#removing missing date values\n\n#Mean time take by surveyors\nsury_df &lt;- surveyor_prep %&gt;% \n  group_by(Srvyr.x.x) %&gt;% \n  mutate(time_for_period1 = difftime(VEnd, VStart, \n                                units = \"days\"),\n         time_for_period2 = difftime(VEnd.x, VStart.x, \n                                units = \"days\"),\n         time_for_period3 = difftime(VEnd.y, VStart.y, \n                                units = \"days\"),) %&gt;%\n  summarise(across(starts_with(\"time_for_period\"), ~mean(.x, na.rm = TRUE), .names = \"mean_{.col}\"))"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#gender-differences-in-savings-tool-usage-men-vs-women",
    "href": "posts/lift_practical_anlalysis/index.html#gender-differences-in-savings-tool-usage-men-vs-women",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Gender Differences in Savings Tool Usage (Men vs Women)",
    "text": "Gender Differences in Savings Tool Usage (Men vs Women)\nWe can further explore the dataset by examining which savings tools are preferred by men versus women. A comparative plot allows us to see if certain tools are more likely to be used by one gender over the other, shedding light on gendered financial behavior.\n\nsaving_tools_prep &lt;- joined %&gt;% \n  select(Q_153, starts_with(\"A_10\"))\n\n\nsaving_tools_df &lt;- saving_tools_prep %&gt;% \n  gather(key, value, -Q_153) %&gt;% #melting the data\n  mutate(value = replace_na(value, 99)) %&gt;% #convert NA to 99\n  group_by(key, Q_153) %&gt;% \n  count(value, name = \"frequency\") %&gt;% \n  mutate_at(vars(\"value\"), as.factor) %&gt;%\n  separate(key, into = c(\"question\", \"diary_period\"), sep = \"([.])\") %&gt;%\n  mutate(diary_period = replace_na(diary_period, \"One\"),\n         diary_period = case_when(diary_period == \"x\" ~ \"Two\",\n                                  diary_period == \"y\" ~ \"Three\",\n                                  TRUE ~ diary_period),\n         question = str_extract(question, \"[:digit:]+\\\\b\"),\n         question = as.double(question),\n         question = case_when(question == 1 ~ \"Keeping money at home\",\n                              question == 2 ~ \"On the body/in \\nclothes/in wallet\",\n                              question == 3 ~ \"Lend to others\",\n                              question == 4 ~ \"Buy something to sell later\",\n                              question == 5 ~ \"Savings group\",\n                              question == 6 ~ \"MDI\",\n                              question == 7 ~ \"Micro finance institution\",\n                              question == 8 ~ \"Bank account\",\n                              question == 9 ~ \"Buy stock \",\n                              question == 10 ~ \"Buy cattle \",\n                              TRUE ~ \"Other\"\n),\nvalue = relevel(value, ref = \"1\")) \n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 78 rows [1, 2, 3, 4, 5,\n6, 19, 20, 21, 22, 23, 35, 36, 37, 38, 39, 40, 53, 54, 55, ...].\n\n#Plot for period 1\n saving_tools_df %&gt;% \n   filter(diary_period == \"One\",\n          value != 99) %&gt;% \n   ggplot(aes(Q_153, frequency, fill = value)) +\n  geom_col(position = \"stack\") +\n  facet_wrap(~question, scales = \"free_y\") +\n  labs(title = \"Count by gender of use of daving tools\",\n       y = \"Frequency use of saving tools\",\n       x = \"Period of Diary\",\n       fill = \"Key\")"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#mapping-respondent-locations",
    "href": "posts/lift_practical_anlalysis/index.html#mapping-respondent-locations",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Mapping Respondent Locations",
    "text": "Mapping Respondent Locations\nBy selecting one of the biweekly datasets, we can visualize where respondents are located using a geographical plot. This map gives us a clear view of the geographic distribution of participants in the study, providing context for the socio-economic and environmental factors at play.\n\n## data prep\nworld &lt;- map_data(\"world\") %&gt;% \n  filter(region == c( \"Uganda\"))\n\n## Map by gender\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"gray50\", size = 0.05, alpha = 0.2\n  ) +\n  geom_point(\n    data = joined,\n    aes(Longitude, Latitude, color = Q_153),\n    alpha = 0.8\n  ) +\n  theme_void()+\n  labs(x = NULL, y = NULL, color = NULL)"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#pie-chart-for-people-porvety-index-ppi",
    "href": "posts/lift_practical_anlalysis/index.html#pie-chart-for-people-porvety-index-ppi",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Pie Chart for People porvety Index (PPI)",
    "text": "Pie Chart for People porvety Index (PPI)\nUsing any column of choice, a pie chart is a useful tool for showing proportional distribution within a dataset. For example, visualizing savings tools or income sources with a pie chart gives a clear snapshot of the most common categories.\n\n## Prepare data for piechart\npiechart_prep &lt;- joined %&gt;% \n  select(ppicut) %&gt;% \n  count(ppicut, name = \"count\") %&gt;% \n  mutate(value = count/sum(count),\n         ppicut = str_to_upper(ppicut))\n\n# Create a basic bar\npie &lt;-  piechart_prep %&gt;% \n  ggplot( aes(x=\"\", y=value, fill=ppicut)) + \n  geom_bar(stat=\"identity\", width=1) +\n  # Convert to pie (polar coordinates) and add labels\n  coord_polar(\"y\", start=0) + \n  geom_text(aes(label = paste0(round(value*100), \"%\")),\n            position = position_stack(vjust = 0.5)) +\n  # Add color scale (hex colors)\n  scale_fill_manual(values=c(\"#55DDE0\", \"#33658A\", \"#2F4858\", \"#F6AE2D\", \"#F26419\")) +\n  # Remove labels and add title\n  labs(x = NULL, y = NULL, fill = NULL, title = \"People poverty index\") +\n  # Tidy up the theme\n  theme_classic() + \n  theme(axis.line = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          plot.title = element_text(hjust = 0.5, color = \"#666666\"))\n\npie"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#the-power-of-dplyr-for-data-manipulation",
    "href": "posts/lift_practical_anlalysis/index.html#the-power-of-dplyr-for-data-manipulation",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "The Power of dplyr for Data Manipulation",
    "text": "The Power of dplyr for Data Manipulation\nThe dplyr package in R offers a suite of functions for filtering, selecting, and summarizing data. For instance, filtering respondents by income bracket and summarizing their savings allows us to draw meaningful conclusions about their financial behavior. The ease of grouping and plotting with dplyr and ggplot makes complex analyses accessible.\n\nsaving_tools_prep &lt;- joined %&gt;% \n  select(Q_153, starts_with(\"A_10\"))\n\n\nsaving_tools_df &lt;- saving_tools_prep %&gt;% \n  gather(key, value, -Q_153) %&gt;% #melting the data\n  mutate(value = replace_na(value, 99)) %&gt;% #convert NA to 99\n  group_by(key, Q_153) %&gt;% \n  count(value, name = \"frequency\") %&gt;% \n  mutate_at(vars(\"value\"), as.factor) %&gt;%\n  separate(key, into = c(\"question\", \"diary_period\"), sep = \"([.])\") %&gt;%\n  mutate(diary_period = replace_na(diary_period, \"One\"),\n         diary_period = case_when(diary_period == \"x\" ~ \"Two\",\n                                  diary_period == \"y\" ~ \"Three\",\n                                  TRUE ~ diary_period),\n         question = str_extract(question, \"[:digit:]+\\\\b\"),\n         question = as.double(question),\n         question = case_when(question == 1 ~ \"Keeping money at home\",\n                              question == 2 ~ \"On the body/in \\nclothes/in wallet\",\n                              question == 3 ~ \"Lend to others\",\n                              question == 4 ~ \"Buy something to sell later\",\n                              question == 5 ~ \"Savings group\",\n                              question == 6 ~ \"MDI\",\n                              question == 7 ~ \"Micro finance institution\",\n                              question == 8 ~ \"Bank account\",\n                              question == 9 ~ \"Buy stock \",\n                              question == 10 ~ \"Buy cattle \",\n                              TRUE ~ \"Other\"\n),\nvalue = relevel(value, ref = \"1\")) \n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 78 rows [1, 2, 3, 4, 5,\n6, 19, 20, 21, 22, 23, 35, 36, 37, 38, 39, 40, 53, 54, 55, ...].\n\n#Plot for period 1\n saving_tools_df %&gt;% \n   filter(diary_period == \"One\",\n          value != 99) %&gt;% \n   ggplot(aes(Q_153, frequency, fill = value)) +\n  geom_col(position = \"stack\") +\n  facet_wrap(~question, scales = \"free_y\") +\n  labs(title = \"Count by gender of use of daving tools\",\n       y = \"Frequency use of saving tools\",\n       x = \"Period of Diary\",\n       fill = \"Key\")"
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#sampling-methods-and-r-implementation",
    "href": "posts/lift_practical_anlalysis/index.html#sampling-methods-and-r-implementation",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Sampling Methods and R Implementation",
    "text": "Sampling Methods and R Implementation\nWhen working with survey data, sampling is key to ensuring representative results. In this section, we discuss simple random sampling, stratified sampling, and cluster sampling, providing an overview of how to implement these methods in R.\nsampling can be divided into two types.\n\nProbabilistic sampling\nNon-probalistic sampling\n\n\nProbabilistic sampling\nSome of the probabilistic sampling techniques are simple random sampling, systematic sampling, cluster sampling and stratified sampling.\n\nsimple random sampling\nIn r we can use the ~sample function~ to select a random sample with or without replacement.\n\nx &lt;- 1:12\n# a random permutation\nsample(x)\n\n [1]  2  1  9  3  8  6  5 11  4  7 12 10\n\n\n\n\nsystematic sampling\nThis technique selects units based on a fixed sampling interval. In R we can set up a function that checks for our condition as in the example here\n\n\nstratified sampling\nThis involves grouping the data into selected statas. We can use the sample_n or the sample_f after we have group the data.\n\nby_cyl_n &lt;- mtcars %&gt;% \n  group_by(cyl) %&gt;% \n  sample_n(2)\n\n#or\n\nby_cyl_f &lt;- mtcars %&gt;% \n  group_by(cyl) %&gt;% \n  sample_frac(.1)\n\n\n\nCluster sampling\nThis technique divides the population in clusters of equal size n and selects clusters every individual time."
  },
  {
    "objectID": "posts/lift_practical_anlalysis/index.html#handling-null-values-in-r",
    "href": "posts/lift_practical_anlalysis/index.html#handling-null-values-in-r",
    "title": "Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R",
    "section": "Handling Null Values in R",
    "text": "Handling Null Values in R\nNull values are inevitable in real-world datasets, and handling them effectively is essential. Two common approaches are either removing rows with null values or imputing missing data based on the median or mean. Each method has its pros and cons, which should be carefully considered based on the analysis.\n\nWays of Dealing with Missing Values\nThere are three types of missing values types.\n\nMissing at random\nMissing completely at random\nMissing not at random.\n\nThe major ways of dealing with missing values are:-\n\nDrop NULL - dropping the values if they are considerably small taking into account the rule of thumb in sample size number.\nImputation - Replacing the missing values with the mean, median or mode. This highly depends on the distribution of the data variable and the spread. Imputation can be done using kNN, boostrap aggregation and random forest as ways of machine learning.\n\n\n\nConclusion:\nFinancial diaries data provides a wealth of information about individuals’ financial behavior over time. By combining datasets, analyzing key variables, and visualizing trends, we can extract valuable insights to inform decision-making and policy. Using R, the process becomes streamlined and efficient, allowing researchers to focus on what matters most—understanding the data."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Business Intelligence (BI) tools play a crucial role in turning raw data into actionable insights, aiding decision-makers in making informed choices. In this blog post, we’ll explore the similarities, differences, and unique features of six popular BI tools: Excel, Power BI, Tableau, SAS, Python Dash, and R Shiny. Our analysis will focus on the learning curve and business capability rating of each tool.\n\n\n\nExcel:\n\nSimilarities: Ubiquitous in business for data analysis.\nDifferences: Limited for extensive data processing and visualization.\nUniqueness: Familiar interface but may require advanced functions for complex analytics.\n\nPower BI:\n\nSimilarities: Integrated with Microsoft products.\nDifferences: Emphasis on visualization and dashboards.\nUniqueness: User-friendly, with some learning required for advanced features.\n\nTableau:\n\nSimilarities: Focus on data visualization.\nDifferences: Steeper learning curve; powerful for interactive dashboards.\nUniqueness: Robust visualization capabilities, strong community support.\n\nSAS:\n\nSimilarities: Advanced analytics, statistical analysis.\nDifferences: Requires programming skills; traditional use for complex models.\nUniqueness: Industry-wide usage in healthcare and finance, extensive analytics capabilities.\n\nPython Dash:\n\nSimilarities: Python-based for web-based dashboards.\nDifferences: Programming-centric; suitable for data scientists.\nUniqueness: Flexibility and customization using Python.\n\nR Shiny:\n\nSimilarities: R-based, excellent for statistical analysis.\nDifferences: Requires knowledge of R programming.\nUniqueness: Strong statistical capabilities, ideal for creating interactive web applications.\n\n\n\n\nLearning curve VS Business Capability rating\n\n\n\n\n\n\n\nExcel:\n\nStrengths: Versatile for small to medium-sized datasets.\nWeaknesses: Limited scalability, less advanced analytics.\n\nPower BI:\n\nStrengths: Seamless Microsoft integration, excellent visualizations.\nWeaknesses: May require additional tools for advanced analytics.\n\nTableau:\n\nStrengths: Powerful visualization, extensive data connectivity.\nWeaknesses: Steeper learning curve, higher cost.\n\nSAS:\n\nStrengths: Robust analytics, statistical modeling, and data management.\nWeaknesses: High cost, steeper learning curve.\n\nPython Dash:\n\nStrengths: Customizable with Python, suitable for data science applications.\nWeaknesses: Learning curve for those unfamiliar with Python.\n\nR Shiny:\n\nStrengths: Strong statistical capabilities, great for R users.\nWeaknesses: Learning curve for those unfamiliar with R.\n\n\nIn conclusion, the choice of BI tool depends on specific business needs, data scale, customization requirements, and the existing skill set. Whether opting for user-friendly interfaces or diving into more complex analytics, understanding these tools’ nuances is crucial for effective decision-making."
  },
  {
    "objectID": "posts/post-with-code/index.html#learning-curve",
    "href": "posts/post-with-code/index.html#learning-curve",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Excel:\n\nSimilarities: Ubiquitous in business for data analysis.\nDifferences: Limited for extensive data processing and visualization.\nUniqueness: Familiar interface but may require advanced functions for complex analytics.\n\nPower BI:\n\nSimilarities: Integrated with Microsoft products.\nDifferences: Emphasis on visualization and dashboards.\nUniqueness: User-friendly, with some learning required for advanced features.\n\nTableau:\n\nSimilarities: Focus on data visualization.\nDifferences: Steeper learning curve; powerful for interactive dashboards.\nUniqueness: Robust visualization capabilities, strong community support.\n\nSAS:\n\nSimilarities: Advanced analytics, statistical analysis.\nDifferences: Requires programming skills; traditional use for complex models.\nUniqueness: Industry-wide usage in healthcare and finance, extensive analytics capabilities.\n\nPython Dash:\n\nSimilarities: Python-based for web-based dashboards.\nDifferences: Programming-centric; suitable for data scientists.\nUniqueness: Flexibility and customization using Python.\n\nR Shiny:\n\nSimilarities: R-based, excellent for statistical analysis.\nDifferences: Requires knowledge of R programming.\nUniqueness: Strong statistical capabilities, ideal for creating interactive web applications.\n\n\n\n\nLearning curve VS Business Capability rating"
  },
  {
    "objectID": "posts/post-with-code/index.html#business-capability-rating",
    "href": "posts/post-with-code/index.html#business-capability-rating",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Excel:\n\nStrengths: Versatile for small to medium-sized datasets.\nWeaknesses: Limited scalability, less advanced analytics.\n\nPower BI:\n\nStrengths: Seamless Microsoft integration, excellent visualizations.\nWeaknesses: May require additional tools for advanced analytics.\n\nTableau:\n\nStrengths: Powerful visualization, extensive data connectivity.\nWeaknesses: Steeper learning curve, higher cost.\n\nSAS:\n\nStrengths: Robust analytics, statistical modeling, and data management.\nWeaknesses: High cost, steeper learning curve.\n\nPython Dash:\n\nStrengths: Customizable with Python, suitable for data science applications.\nWeaknesses: Learning curve for those unfamiliar with Python.\n\nR Shiny:\n\nStrengths: Strong statistical capabilities, great for R users.\nWeaknesses: Learning curve for those unfamiliar with R.\n\n\nIn conclusion, the choice of BI tool depends on specific business needs, data scale, customization requirements, and the existing skill set. Whether opting for user-friendly interfaces or diving into more complex analytics, understanding these tools’ nuances is crucial for effective decision-making."
  },
  {
    "objectID": "posts/welcome/index.html#introduction-to-geospatial-data",
    "href": "posts/welcome/index.html#introduction-to-geospatial-data",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Introduction to Geospatial Data",
    "text": "Introduction to Geospatial Data\nGeospatial data comes in various forms, such as points, lines, and polygons, each representing different aspects of the Earth’s surface. The most common file formats for storing geospatial data are GeoJSON and Shapefiles.\n\n\n\nGeospatial using R\n\n\n\nLoading Geospatial Data in R\nIn R, the sf package is widely used for handling geospatial data. Let’s start by loading a Shapefile containing information about city boundaries.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load geospatial data\ncities &lt;- st_read(\"path/to/cities.shp\")\nThis code snippet assumes you have a Shapefile named cities.shp in your working directory. The st_read function from the sf package is used to read the Shapefile and create a spatial data frame.\n\n\nExploring Geospatial Data\nOnce the data is loaded, let’s explore its structure and attributes.\n# Display summary of the spatial data\nsummary(cities)\nThis will provide an overview of the spatial data, including the geometry type (point, line, or polygon), bounding box, and attribute data."
  },
  {
    "objectID": "posts/welcome/index.html#geospatial-data-visualization",
    "href": "posts/welcome/index.html#geospatial-data-visualization",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Geospatial Data Visualization",
    "text": "Geospatial Data Visualization\nVisualization is crucial for understanding geospatial patterns. We’ll use the ggplot2 package for creating basic maps.\n# Install and load ggplot2\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Plotting the cities on a map\nggplot() +\n  geom_sf(data = cities) +\n  ggtitle(\"Cities Map\")\nHere, geom_sf is used to plot the spatial features on a map. Customize the plot further by adding layers, adjusting colors, and incorporating additional geospatial data."
  },
  {
    "objectID": "posts/welcome/index.html#spatial-queries-and-analysis",
    "href": "posts/welcome/index.html#spatial-queries-and-analysis",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Spatial Queries and Analysis",
    "text": "Spatial Queries and Analysis\nPerforming spatial queries allows us to extract meaningful information from geospatial data. Let’s say we want to find cities within a specific region.\n# Define a bounding box for the region\nbbox &lt;- st_bbox(c(xmin, ymin, xmax, ymax), crs = st_crs(cities))\n\n# Extract cities within the bounding box\ncities_in_region &lt;- cities[st_within(cities, st_as_sfc(bbox)), ]\nHere, st_within is used to filter cities that fall within the specified bounding box."
  },
  {
    "objectID": "posts/welcome/index.html#basic-geospatial-statistics",
    "href": "posts/welcome/index.html#basic-geospatial-statistics",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Basic Geospatial Statistics",
    "text": "Basic Geospatial Statistics\nUnderstanding the spatial distribution of data is crucial. Let’s explore basic spatial statistics using the spdep package.\n# Install and load spdep\ninstall.packages(\"spdep\")\nlibrary(spdep)\n\n# Spatial autocorrelation analysis\nmoran &lt;- moran.test(cities$population, listw = poly2nb(st_as_sfc(cities)))\nprint(moran)\nThis example conducts a Moran’s I test to assess spatial autocorrelation in the population data."
  },
  {
    "objectID": "posts/welcome/index.html#conclusion",
    "href": "posts/welcome/index.html#conclusion",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has provided a foundational understanding of geospatial data in R. We covered loading data, visualization, spatial queries, and basic statistics. As you delve deeper into geospatial analysis, you’ll find R to be a versatile and powerful tool for unlocking valuable insights from spatial datasets.\nIn the next parts of this series, we will explore advanced topics such as spatial regression, machine learning with geospatial data, and building interactive web maps. Stay tuned for more insights into the fascinating world of spatial analytics with R!"
  },
  {
    "objectID": "courses/Upcoming Courses/Machine Learning basics  practicals/index.html",
    "href": "courses/Upcoming Courses/Machine Learning basics  practicals/index.html",
    "title": "Machine Learning for Humanitarian Practical data",
    "section": "",
    "text": "How can machine learning be used to improve humanitarian outcomes? From early warning systems for food insecurity to predicting disease outbreaks and optimizing aid distribution, ML is changing how we address real-world challenges.\nWe’re thrilled to announce our new course:\n🎓 Machine Learning for Humanitarian Practical Data – A beginner-to-intermediate level course focused on building practical ML skills using real humanitarian datasets.\n\n\nMachine learning can feel overwhelming—especially in sectors like health, education, climate resilience, and crisis response where clean, structured data is rare. This course bridges that gap by teaching machine learning with noisy, messy, real-world data, just like what you work with on the ground.\nNo more theory-only learning. You’ll gain hands-on experience applying ML to humanitarian and development datasets that matter.\n\n\n\nBy the end of this course, learners will be able to:\n✅ Understand the ML workflow: from data preparation to model evaluation\n✅ Explore key ML algorithms: regression, classification, decision trees, random forests, and ensemble models\n✅ Handle missing data, imbalance, and data quality issues in humanitarian datasets\n✅ Evaluate model performance with real metrics\n✅ Use tools like caret, tidymodels, and xgboost in R\n✅ Communicate ML findings through compelling visuals and reports\n\n\n\nJoin us for a free webinar before the course starts. This session is designed to:\n👋 Introduce the course and instructors\n📊 Discuss real use cases of ML in humanitarian work\n🧠 Answer your questions\n🎯 Help you decide if this course is right for you\n📅 [Insert webinar date and time]\n\n\n\nThis course is designed for:\n🧑‍🔬 M&E Specialists and Researchers\n📊 Humanitarian Data Analysts and Program Officers\n🧠 Technical Staff in NGOs, UN agencies, and Social Enterprises\n📈 Public Health, Climate, Education, and Agriculture Professionals\n💻 Anyone curious about applying machine learning to messy, real-world data!\n📌 Some basic understanding of R or Python is helpful—but not required. We’ll guide you!\n\n\n\nReserve your slot by filling the form below. You can also indicate interest in the free webinar session.\n\nhtml\nCopyEdit\n&lt;!-- [Insert KoboToolbox HTML Embed Code Here] --&gt;\n📣 Real data. Real problems. Real solutions. This isn’t a textbook course—it’s a mission-driven learning journey into how machine learning can improve lives and systems.\nLet’s turn your humanitarian data into powerful insights. 💪\n📩 For more details or bulk registrations, contact us at: [your-email@example.com]"
  },
  {
    "objectID": "courses/Upcoming Courses/Machine Learning basics  practicals/index.html#machine-learning-for-humanitarian-practical-data",
    "href": "courses/Upcoming Courses/Machine Learning basics  practicals/index.html#machine-learning-for-humanitarian-practical-data",
    "title": "Machine Learning for Humanitarian Practical data",
    "section": "",
    "text": "How can machine learning be used to improve humanitarian outcomes? From early warning systems for food insecurity to predicting disease outbreaks and optimizing aid distribution, ML is changing how we address real-world challenges.\nWe’re thrilled to announce our new course:\n🎓 Machine Learning for Humanitarian Practical Data – A beginner-to-intermediate level course focused on building practical ML skills using real humanitarian datasets.\n\n\nMachine learning can feel overwhelming—especially in sectors like health, education, climate resilience, and crisis response where clean, structured data is rare. This course bridges that gap by teaching machine learning with noisy, messy, real-world data, just like what you work with on the ground.\nNo more theory-only learning. You’ll gain hands-on experience applying ML to humanitarian and development datasets that matter.\n\n\n\nBy the end of this course, learners will be able to:\n✅ Understand the ML workflow: from data preparation to model evaluation\n✅ Explore key ML algorithms: regression, classification, decision trees, random forests, and ensemble models\n✅ Handle missing data, imbalance, and data quality issues in humanitarian datasets\n✅ Evaluate model performance with real metrics\n✅ Use tools like caret, tidymodels, and xgboost in R\n✅ Communicate ML findings through compelling visuals and reports\n\n\n\nJoin us for a free webinar before the course starts. This session is designed to:\n👋 Introduce the course and instructors\n📊 Discuss real use cases of ML in humanitarian work\n🧠 Answer your questions\n🎯 Help you decide if this course is right for you\n📅 [Insert webinar date and time]\n\n\n\nThis course is designed for:\n🧑‍🔬 M&E Specialists and Researchers\n📊 Humanitarian Data Analysts and Program Officers\n🧠 Technical Staff in NGOs, UN agencies, and Social Enterprises\n📈 Public Health, Climate, Education, and Agriculture Professionals\n💻 Anyone curious about applying machine learning to messy, real-world data!\n📌 Some basic understanding of R or Python is helpful—but not required. We’ll guide you!\n\n\n\nReserve your slot by filling the form below. You can also indicate interest in the free webinar session.\n\nhtml\nCopyEdit\n&lt;!-- [Insert KoboToolbox HTML Embed Code Here] --&gt;\n📣 Real data. Real problems. Real solutions. This isn’t a textbook course—it’s a mission-driven learning journey into how machine learning can improve lives and systems.\nLet’s turn your humanitarian data into powerful insights. 💪\n📩 For more details or bulk registrations, contact us at: [your-email@example.com]"
  },
  {
    "objectID": "courses/Upcoming Courses/ai_for_me/index.html",
    "href": "courses/Upcoming Courses/ai_for_me/index.html",
    "title": "AI for M&E MasterClass Access",
    "section": "",
    "text": "Click the button below to pay for this course using Mastercard, M-Pesa, or Visa.\nAfter payment, you will receive your access credentials via email.\n\n\n\n\n\n\n\n\nPay Now\n\n\n\n\n\n  \n    \n    Get latest Courses and Webinars\n    \n    \n      \n        Your email:\n        \n      \n\n      \n        Name and Designation:\n        \n      \n\n      \n        \n          Send\n        \n        \n          Exit"
  },
  {
    "objectID": "courses/Upcoming Courses/ai_for_me/index.html#ai-for-me-masterclass-access",
    "href": "courses/Upcoming Courses/ai_for_me/index.html#ai-for-me-masterclass-access",
    "title": "AI for M&E MasterClass Access",
    "section": "",
    "text": "Click the button below to pay for this course using Mastercard, M-Pesa, or Visa.\nAfter payment, you will receive your access credentials via email.\n\n\n\n\n\n\n\n\nPay Now\n\n\n\n\n\n  \n    \n    Get latest Courses and Webinars\n    \n    \n      \n        Your email:\n        \n      \n\n      \n        Name and Designation:\n        \n      \n\n      \n        \n          Send\n        \n        \n          Exit"
  },
  {
    "objectID": "courses/Upcoming Courses/R for DataScience/index.html",
    "href": "courses/Upcoming Courses/R for DataScience/index.html",
    "title": "R for Data Science MasterClass",
    "section": "",
    "text": "In today’s data-driven world, data science is no longer a luxury—it’s a necessity. Whether you’re analyzing health outcomes, optimizing business decisions, or conducting academic research, R has become one of the most powerful tools in the data scientist’s toolbox. That’s why we’re excited to launch our upcoming course:\n🎓 Data Science for R – A hands-on, beginner-friendly course designed to help you master the art of data analytics with R programming.\n\n\nData is everywhere—but making sense of it requires the right tools and the right training. R provides robust packages, visualizations, and analytical power for handling everything from small surveys to big data. Whether you’re in research, monitoring and evaluation, economics, agriculture, health, or education, this course equips you with skills to turn data into impact.\n\n\n\nBy the end of this course, learners will be able to:\n✅ Understand the core syntax and structure of R\n✅ Import, clean, and wrangle datasets using dplyr, tidyr, and other tidyverse tools\n✅ Create beautiful, insightful visualizations using ggplot2\n✅ Perform basic statistical analysis and build predictive models\n✅ Reproducibly report results using R Markdown and Quarto\n✅ Automate reports and build dashboards using Shiny (Intro level)\n\n\n\nTo get you started and answer any burning questions, we’re offering a free webinar 🎙️ just before the official classes begin! This is your chance to:\n👋 Meet the instructors\n💬 Interact with other learners\n🎯 Get an overview of the curriculum\n✅ Ask questions and see if the course is right for you\n📅 [Date and time of the webinar goes here]\n\n\n\nThis course is ideal for:\n🧑‍🔬 Researchers & Academics\n📊 M&E and Data Officers\n📈 Development Practitioners\n💼 NGO Program Staff\n🎓 Students and Graduates in social sciences, statistics, or related fields\n💡 Anyone curious about turning messy data into meaningful insights!\nNo prior experience in R is needed—just a willingness to learn and a laptop!\n\n\n\nKindly fill in the short form below to register for the course or to attend the free webinar:\n\nhtml\nCopyEdit\n&lt;!-- [Insert KoboToolbox HTML Embed Link Here] --&gt;\n🔥 Seats are limited, and we’re already seeing high interest. Don’t miss this chance to level up your data skills with one of the most in-demand tools in the world of analytics!\n📧 For inquiries, feel free to reach out at: [your-email@example.com]\n💻 Let’s build your data career together!"
  },
  {
    "objectID": "courses/Upcoming Courses/R for DataScience/index.html#unlock-the-power-of-data-with-r-join-our-data-science-for-r-course",
    "href": "courses/Upcoming Courses/R for DataScience/index.html#unlock-the-power-of-data-with-r-join-our-data-science-for-r-course",
    "title": "R for Data Science MasterClass",
    "section": "",
    "text": "In today’s data-driven world, data science is no longer a luxury—it’s a necessity. Whether you’re analyzing health outcomes, optimizing business decisions, or conducting academic research, R has become one of the most powerful tools in the data scientist’s toolbox. That’s why we’re excited to launch our upcoming course:\n🎓 Data Science for R – A hands-on, beginner-friendly course designed to help you master the art of data analytics with R programming.\n\n\nData is everywhere—but making sense of it requires the right tools and the right training. R provides robust packages, visualizations, and analytical power for handling everything from small surveys to big data. Whether you’re in research, monitoring and evaluation, economics, agriculture, health, or education, this course equips you with skills to turn data into impact.\n\n\n\nBy the end of this course, learners will be able to:\n✅ Understand the core syntax and structure of R\n✅ Import, clean, and wrangle datasets using dplyr, tidyr, and other tidyverse tools\n✅ Create beautiful, insightful visualizations using ggplot2\n✅ Perform basic statistical analysis and build predictive models\n✅ Reproducibly report results using R Markdown and Quarto\n✅ Automate reports and build dashboards using Shiny (Intro level)\n\n\n\nTo get you started and answer any burning questions, we’re offering a free webinar 🎙️ just before the official classes begin! This is your chance to:\n👋 Meet the instructors\n💬 Interact with other learners\n🎯 Get an overview of the curriculum\n✅ Ask questions and see if the course is right for you\n📅 [Date and time of the webinar goes here]\n\n\n\nThis course is ideal for:\n🧑‍🔬 Researchers & Academics\n📊 M&E and Data Officers\n📈 Development Practitioners\n💼 NGO Program Staff\n🎓 Students and Graduates in social sciences, statistics, or related fields\n💡 Anyone curious about turning messy data into meaningful insights!\nNo prior experience in R is needed—just a willingness to learn and a laptop!\n\n\n\nKindly fill in the short form below to register for the course or to attend the free webinar:\n\nhtml\nCopyEdit\n&lt;!-- [Insert KoboToolbox HTML Embed Link Here] --&gt;\n🔥 Seats are limited, and we’re already seeing high interest. Don’t miss this chance to level up your data skills with one of the most in-demand tools in the world of analytics!\n📧 For inquiries, feel free to reach out at: [your-email@example.com]\n💻 Let’s build your data career together!"
  },
  {
    "objectID": "courses/Upcoming Courses/Website Development/index.html",
    "href": "courses/Upcoming Courses/Website Development/index.html",
    "title": "Website Development Using Rstudio for Beginners",
    "section": "",
    "text": "Do you want to showcase your projects, share your research, or build a professional portfolio site — all from RStudio? You’re in the right place. Our brand-new course:\n🎓 Website Development Using RStudio for Beginners\nis designed to teach you how to build stunning, professional websites using Quarto and RStudio, even if you’ve never written a line of web code before!\n\n\nHaving a personal or project website is one of the most powerful tools you can use to:\n✨ Share your data visualizations, reports, and dashboards\n📄 Publish articles, portfolios, and CVs\n📊 Centralize all your M&E projects and research\n🚀 Grow your professional brand and online presence\nAnd the best part? You can build everything directly within RStudio — the tool you’re already using for data analysis!\n\n\n\nBy the end of this hands-on course, you’ll be able to:\n✅ Set up a Quarto website from scratch using RStudio\n✅ Design and organize pages for blogs, CVs, dashboards, or project portfolios\n✅ Customize layout, add logos, links, and navigation menus\n✅ Use .qmd files, .yaml configuration, and SCSS/CSS styling for branding\n✅ Host your site on GitHub Pages or Netlify — FREE and professional\nNo HTML or JavaScript required — we start from the basics!\n\n\n\nTo help you get started, we’re hosting a free intro webinar before the course launch. 🗓️\nThis webinar will cover:\n👀 Live demo of a data portfolio site\n🎯 Overview of Quarto’s power for researchers & analysts\n💬 Q&A session to answer your burning questions\n🎁 Bonuses for early sign-ups!\n📅 [Insert your webinar date and time here]\n\n\n\n💼 Monitoring & Evaluation professionals\n🧑‍🔬 Researchers and Data Scientists\n📝 Bloggers and Data Journalists\n📈 Development Practitioners\n🎓 Students and recent grads with R experience\n🛠️ Anyone curious about creating a digital presence using R\nIf you’ve ever wanted to publish your reports, findings, or dashboards online, this is your chance.\n\n\n\nReady to build your own website using RStudio?\n\nhtml\nCopyEdit\n&lt;!-- [Insert KoboToolbox HTML Embed Link Here] --&gt;\nLimited slots available — reserve your spot early to gain full access and join a community of fellow learners who are building and publishing with R!\n📩 Questions? Contact us at [your-email@example.com]\n🧠 Learn. Build. Publish. Stand out with your own data site!"
  },
  {
    "objectID": "courses/Upcoming Courses/Website Development/index.html#build-your-own-website-with-rstudio-no-coding-experience-needed",
    "href": "courses/Upcoming Courses/Website Development/index.html#build-your-own-website-with-rstudio-no-coding-experience-needed",
    "title": "Website Development Using Rstudio for Beginners",
    "section": "",
    "text": "Do you want to showcase your projects, share your research, or build a professional portfolio site — all from RStudio? You’re in the right place. Our brand-new course:\n🎓 Website Development Using RStudio for Beginners\nis designed to teach you how to build stunning, professional websites using Quarto and RStudio, even if you’ve never written a line of web code before!\n\n\nHaving a personal or project website is one of the most powerful tools you can use to:\n✨ Share your data visualizations, reports, and dashboards\n📄 Publish articles, portfolios, and CVs\n📊 Centralize all your M&E projects and research\n🚀 Grow your professional brand and online presence\nAnd the best part? You can build everything directly within RStudio — the tool you’re already using for data analysis!\n\n\n\nBy the end of this hands-on course, you’ll be able to:\n✅ Set up a Quarto website from scratch using RStudio\n✅ Design and organize pages for blogs, CVs, dashboards, or project portfolios\n✅ Customize layout, add logos, links, and navigation menus\n✅ Use .qmd files, .yaml configuration, and SCSS/CSS styling for branding\n✅ Host your site on GitHub Pages or Netlify — FREE and professional\nNo HTML or JavaScript required — we start from the basics!\n\n\n\nTo help you get started, we’re hosting a free intro webinar before the course launch. 🗓️\nThis webinar will cover:\n👀 Live demo of a data portfolio site\n🎯 Overview of Quarto’s power for researchers & analysts\n💬 Q&A session to answer your burning questions\n🎁 Bonuses for early sign-ups!\n📅 [Insert your webinar date and time here]\n\n\n\n💼 Monitoring & Evaluation professionals\n🧑‍🔬 Researchers and Data Scientists\n📝 Bloggers and Data Journalists\n📈 Development Practitioners\n🎓 Students and recent grads with R experience\n🛠️ Anyone curious about creating a digital presence using R\nIf you’ve ever wanted to publish your reports, findings, or dashboards online, this is your chance.\n\n\n\nReady to build your own website using RStudio?\n\nhtml\nCopyEdit\n&lt;!-- [Insert KoboToolbox HTML Embed Link Here] --&gt;\nLimited slots available — reserve your spot early to gain full access and join a community of fellow learners who are building and publishing with R!\n📩 Questions? Contact us at [your-email@example.com]\n🧠 Learn. Build. Publish. Stand out with your own data site!"
  },
  {
    "objectID": "courses/available_courses.html",
    "href": "courses/available_courses.html",
    "title": "Short Course Catalogue",
    "section": "",
    "text": "KoboToolbox Course\n\n\n\n\n\n\n\n\nIntroduction to CAPI (Computer-Assisted Personal Interviewing)\nOverview of KoboToolbox interface and project setup\nDesigning and structuring forms using XLSForm\nIncorporating skip logic, validation rules, and calculations\nData collection and testing using mobile devices\nManaging and monitoring survey submissions\nExporting and cleaning collected data\n:::"
  },
  {
    "objectID": "courses/available_courses.html#creating-capi-with-kobotoolbox-course",
    "href": "courses/available_courses.html#creating-capi-with-kobotoolbox-course",
    "title": "Short Course Catalogue",
    "section": "",
    "text": "KoboToolbox Course\n\n\n\n\n\n\n\n\nIntroduction to CAPI (Computer-Assisted Personal Interviewing)\nOverview of KoboToolbox interface and project setup\nDesigning and structuring forms using XLSForm\nIncorporating skip logic, validation rules, and calculations\nData collection and testing using mobile devices\nManaging and monitoring survey submissions\nExporting and cleaning collected data"
  },
  {
    "objectID": "courses/available_courses.html#ai-for-me-master-class-course",
    "href": "courses/available_courses.html#ai-for-me-master-class-course",
    "title": "Short Course Catalogue",
    "section": "AI for M&E Master Class Course",
    "text": "AI for M&E Master Class Course\n\nCourse Outline\n\nIntroduction to AI and machine learning in Monitoring & Evaluation\nNatural Language Processing (NLP) for qualitative data\nPredictive analytics for outcome forecasting\nAutomating M&E reporting using AI tools\nEthical use of AI in development contexts\nCase studies from development projects\n\n\n\nExpected Outcomes\n\nGain practical skills to integrate AI into data workflows.\nApply machine learning techniques to M&E problems such as prediction, classification, and automation."
  },
  {
    "objectID": "courses/available_courses.html#practical-introduction-to-r-for-data-science-course",
    "href": "courses/available_courses.html#practical-introduction-to-r-for-data-science-course",
    "title": "Short Course Catalogue",
    "section": "Practical Introduction to R for Data Science Course",
    "text": "Practical Introduction to R for Data Science Course\n\n\n\n\n\nR for Data Science\n\n\n\n\n\nCourse Outline\nIntroduction to R and RStudio - Data manipulation using dplyr and tidyr - Data visualization with ggplot2 - Importing and exporting data - Basic statistical analysis - Writing reproducible reports using RMarkdown\n\n\nExpected Outcomes\n\nClean, analyze, and visualize datasets using R.\nGenerate automated reports and communicate insights effectively."
  },
  {
    "objectID": "courses/available_courses.html#practical-introduction-to-stata-course",
    "href": "courses/available_courses.html#practical-introduction-to-stata-course",
    "title": "Short Course Catalogue",
    "section": "Practical Introduction to STATA Course",
    "text": "Practical Introduction to STATA Course\n\nCourse Outline\n\nGetting started with STATA interface\nImporting and managing datasets\nDescriptive and inferential statistics\nRegression analysis and model interpretation\nData visualization basics in STATA\nAutomating outputs with STATA Do-Files\n\n\n\nExpected Outcomes\n\nPerform basic to intermediate statistical analysis using STATA.\nManage data, run models, and interpret results confidently."
  },
  {
    "objectID": "courses/available_courses.html#visualization-using-data-wrapper-masterclass",
    "href": "courses/available_courses.html#visualization-using-data-wrapper-masterclass",
    "title": "Short Course Catalogue",
    "section": "Visualization Using Data Wrapper MasterClass",
    "text": "Visualization Using Data Wrapper MasterClass\n\n\n\n\n\nDataWrapper Course\n\n\n\n\n\nCourse Outline\n\nIntroduction to Datawrapper interface\nCreating bar, line, and pie charts\nCustomizing map visualizations\nEmbedding visualizations into blogs and websites\nBest practices for clean, interpretable visual design\n\n\n\nExpected Outcomes\n\nMaster data storytelling with high-quality, shareable visualizations.\nCommunicate insights clearly to technical and non-technical audiences."
  },
  {
    "objectID": "courses/available_courses.html#website-creationdevelopment-courses",
    "href": "courses/available_courses.html#website-creationdevelopment-courses",
    "title": "Short Course Catalogue",
    "section": "Website Creation/Development Courses",
    "text": "Website Creation/Development Courses\n\nUsing RStudio\n\n\nCourse Outline\n\nWebsite creation in RStudio using blogdown, distill, and quarto\nManaging content with Markdown and YAML\nPublishing via GitHub Pages or Netlify\nAdding analytics, comments, and social media integrations\n\n\n\nExpected Outcomes\n\nCreate personal blogs or portfolio sites in RStudio.\nUpdate and maintain static websites with confidence.\n\n\n\n\nUsing Quarto\n\n\n\n\n\nUsing Quarto\n\n\n\n\n\nCourse Outline\n\nIntroduction to Quarto framework\nWriting and rendering .qmd documents\nCreating interactive reports and dashboards\nPublishing to web\nCode chunks, figures, and citations integration\n\n\n\nExpected Outcomes\n\nDevelop interactive documents and reproducible reports using Quarto.\nDeploy and update Quarto-based websites and publications.\n\n\n\n\n\n\n\nUsing JavaScript and Python\n\n\nCourse Outline\n\nBasics of HTML, CSS, and JavaScript for interactive web design\nIntroduction to Python for web development (Flask, Streamlit)\nIntegrating JavaScript for interactivity in dashboards\nHosting and deploying Python-based web apps\n\n\n\nExpected Outcomes\n\nUnderstand how to create dynamic web content using Python and JavaScript.\nBuild interactive data apps and dashboards for research or business use. :::"
  },
  {
    "objectID": "myworks/gallery.html",
    "href": "myworks/gallery.html",
    "title": "Gallery with Visualizations",
    "section": "",
    "text": "Boxplots of Yearly Sorghum Yield\n\n\n Correlation of Variables\n\n\n Cross-Fold Metrics for 5 ML Models\n\n\n\n\n\n New Infections\n\n\n Life Expectancy\n\n\n\n\n\n Milk Production\n\n\n ML Model Diagnostics\n\n\n Tourism in East & Southern Africa\n\n\n Stunting Rates Trends\n\n\n\n\n\n EVI for Sorghum (2015–2020)\n\n\n\n:::"
  },
  {
    "objectID": "myworks/awards.html",
    "href": "myworks/awards.html",
    "title": "Wall of Awards",
    "section": "",
    "text": "Cohort 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCohort 3\n\n\n\n\n\n\n\n\n\n\n\n\nCSV Research Solution ltd - Cohort 1(2021)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCohort 2\n\n\n\n\n\n\n\n\n\n\n\n\nCSV Research Solution ltd - Cohort 1(2021)\n\n\n\n\n\n\n\n\n\n\nCohort 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Club - Cohort 1 (2020)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCohort 2"
  },
  {
    "objectID": "myworks/awards.html#website-creationdevelopment-courses",
    "href": "myworks/awards.html#website-creationdevelopment-courses",
    "title": "Wall of Awards",
    "section": "",
    "text": "Coding Club - Cohort 1 (2020)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCohort 2"
  },
  {
    "objectID": "myworks/apps.html",
    "href": "myworks/apps.html",
    "title": "Interactive Dashboards Portfolio",
    "section": "",
    "text": "This portfolio highlights interactive dashboards built with R Shiny, designed to support analysis and decision-making in development, governance, and business contexts.\n\n\n\n\n\nOverview:\nThis dashboard visualizes how donor funds are allocated across various counties and sectors in Kenya. Users can explore the flow of resources based on funder, location, and thematic area.\nThe link to this dashboard is:\nWhy It Matters:\nTransparent tracking of donor investments supports accountability and helps ensure equitable distribution of resources. It is particularly useful for identifying underfunded areas or sectors.\nTarget Users:\n- Donor agencies and development partners\n- National and county government officials\n- Policy researchers and planners\n- NGOs and program evaluators\n\n\n\n\n\n\nOverview:\nThis dashboard tracks sales performance over time, across regions and product categories. It provides a high-level view of sales trends, customer segmentation, and revenue metrics.\nWhy It Matters:\nIt supports evidence-based marketing and operational decisions, helping organizations identify best-selling products, seasonal trends, and areas needing strategic attention.\nTarget Users:\n- Business owners and sales managers\n- Marketing analysts\n- E-commerce and retail platforms\n- Business intelligence teams\n\n\n\n\n\n\nOverview:\nThis dashboard visualizes the Multidimensional Poverty Index across regions in Kenya. It disaggregates poverty into key dimensions—education, health, and living standards—based on survey data.\nWhy It Matters:\nIt provides a nuanced view of poverty, allowing policymakers to target interventions more effectively. Rather than relying solely on income measures, MPI captures broader human development indicators.\nTarget Users:\n- Social policy analysts and M&E specialists\n- National bureaus of statistics\n- Academic and development researchers\n- Humanitarian organizations"
  },
  {
    "objectID": "myworks/apps.html#donor-fund-stream-in-kenya",
    "href": "myworks/apps.html#donor-fund-stream-in-kenya",
    "title": "Interactive Dashboards Portfolio",
    "section": "",
    "text": "Overview:\nThis dashboard visualizes how donor funds are allocated across various counties and sectors in Kenya. Users can explore the flow of resources based on funder, location, and thematic area.\nThe link to this dashboard is:\nWhy It Matters:\nTransparent tracking of donor investments supports accountability and helps ensure equitable distribution of resources. It is particularly useful for identifying underfunded areas or sectors.\nTarget Users:\n- Donor agencies and development partners\n- National and county government officials\n- Policy researchers and planners\n- NGOs and program evaluators"
  },
  {
    "objectID": "myworks/apps.html#sales-dashboard",
    "href": "myworks/apps.html#sales-dashboard",
    "title": "Interactive Dashboards Portfolio",
    "section": "",
    "text": "Overview:\nThis dashboard tracks sales performance over time, across regions and product categories. It provides a high-level view of sales trends, customer segmentation, and revenue metrics.\nWhy It Matters:\nIt supports evidence-based marketing and operational decisions, helping organizations identify best-selling products, seasonal trends, and areas needing strategic attention.\nTarget Users:\n- Business owners and sales managers\n- Marketing analysts\n- E-commerce and retail platforms\n- Business intelligence teams"
  },
  {
    "objectID": "myworks/apps.html#multidimensional-poverty-index-mpi-by-region",
    "href": "myworks/apps.html#multidimensional-poverty-index-mpi-by-region",
    "title": "Interactive Dashboards Portfolio",
    "section": "",
    "text": "Overview:\nThis dashboard visualizes the Multidimensional Poverty Index across regions in Kenya. It disaggregates poverty into key dimensions—education, health, and living standards—based on survey data.\nWhy It Matters:\nIt provides a nuanced view of poverty, allowing policymakers to target interventions more effectively. Rather than relying solely on income measures, MPI captures broader human development indicators.\nTarget Users:\n- Social policy analysts and M&E specialists\n- National bureaus of statistics\n- Academic and development researchers\n- Humanitarian organizations"
  },
  {
    "objectID": "books/books.html",
    "href": "books/books.html",
    "title": "Books",
    "section": "",
    "text": "📥 View / Download\n\n\n\n\n\n\n📥 View / Download"
  },
  {
    "objectID": "books/books.html#meal-dpro-guide-english",
    "href": "books/books.html#meal-dpro-guide-english",
    "title": "Books",
    "section": "",
    "text": "📥 View / Download"
  },
  {
    "objectID": "books/books.html#cluster-analysis-guide",
    "href": "books/books.html#cluster-analysis-guide",
    "title": "Books",
    "section": "",
    "text": "📥 View / Download"
  }
]
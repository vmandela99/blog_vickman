[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm\n\n\n\n\n\n\n\nVisualization\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n  \n\n\n\n\nGetting Weather Data via API with R\n\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\nAPI\n\n\nWeather Data\n\n\nClimate\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nAlbert Rapp\n\n\n\n\n\n\n  \n\n\n\n\nSpartial regression: Lesson3\n\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial Data Manipulation in R: Lesson2\n\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial analysis with R: Lesson1\n\n\n\n\n\n\n\nGIS\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\n  \n\n\n\n\nBusiness intelligence tools\n\n\n\n\n\n\n\nBI\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\n  \n\n\n\n\nHow to build a ShinyApp\n\n\n\n\n\n\n\nR\n\n\nShiny\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2021\n\n\nVictor Mandela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Vickman",
    "section": "",
    "text": "Hi! My name is Victor Mandela, and I am from Kenya üíú.\nI‚Äôm a data enthusiast and statistics warrior, and I strive in environments that need solutions related to data research.\nI have been writing about R in my blog since 2019. I‚Äôm mostly active in the ShinyR and tidyverse community, slack groups and social media sites.\nI‚Äôm a Masters of Science in Statistics holder from the University of Nairobi. My research I am majorly interested in webApp development, machine learning and evidence-based reproducible research."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Business Intelligence (BI) tools play a crucial role in turning raw data into actionable insights, aiding decision-makers in making informed choices. In this blog post, we‚Äôll explore the similarities, differences, and unique features of six popular BI tools: Excel, Power BI, Tableau, SAS, Python Dash, and R Shiny. Our analysis will focus on the learning curve and business capability rating of each tool.\n\n\n\nExcel:\n\nSimilarities: Ubiquitous in business for data analysis.\nDifferences: Limited for extensive data processing and visualization.\nUniqueness: Familiar interface but may require advanced functions for complex analytics.\n\nPower BI:\n\nSimilarities: Integrated with Microsoft products.\nDifferences: Emphasis on visualization and dashboards.\nUniqueness: User-friendly, with some learning required for advanced features.\n\nTableau:\n\nSimilarities: Focus on data visualization.\nDifferences: Steeper learning curve; powerful for interactive dashboards.\nUniqueness: Robust visualization capabilities, strong community support.\n\nSAS:\n\nSimilarities: Advanced analytics, statistical analysis.\nDifferences: Requires programming skills; traditional use for complex models.\nUniqueness: Industry-wide usage in healthcare and finance, extensive analytics capabilities.\n\nPython Dash:\n\nSimilarities: Python-based for web-based dashboards.\nDifferences: Programming-centric; suitable for data scientists.\nUniqueness: Flexibility and customization using Python.\n\nR Shiny:\n\nSimilarities: R-based, excellent for statistical analysis.\nDifferences: Requires knowledge of R programming.\nUniqueness: Strong statistical capabilities, ideal for creating interactive web applications.\n\n\n\n\n\n\n\nExcel:\n\nStrengths: Versatile for small to medium-sized datasets.\nWeaknesses: Limited scalability, less advanced analytics.\n\nPower BI:\n\nStrengths: Seamless Microsoft integration, excellent visualizations.\nWeaknesses: May require additional tools for advanced analytics.\n\nTableau:\n\nStrengths: Powerful visualization, extensive data connectivity.\nWeaknesses: Steeper learning curve, higher cost.\n\nSAS:\n\nStrengths: Robust analytics, statistical modeling, and data management.\nWeaknesses: High cost, steeper learning curve.\n\nPython Dash:\n\nStrengths: Customizable with Python, suitable for data science applications.\nWeaknesses: Learning curve for those unfamiliar with Python.\n\nR Shiny:\n\nStrengths: Strong statistical capabilities, great for R users.\nWeaknesses: Learning curve for those unfamiliar with R.\n\n\nIn conclusion, the choice of BI tool depends on specific business needs, data scale, customization requirements, and the existing skill set. Whether opting for user-friendly interfaces or diving into more complex analytics, understanding these tools‚Äô nuances is crucial for effective decision-making."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome/index.html#introduction-to-geospatial-data",
    "href": "posts/welcome/index.html#introduction-to-geospatial-data",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Introduction to Geospatial Data",
    "text": "Introduction to Geospatial Data\nGeospatial data comes in various forms, such as points, lines, and polygons, each representing different aspects of the Earth‚Äôs surface. The most common file formats for storing geospatial data are GeoJSON and Shapefiles.\n\n\n\nGeospatial using R\n\n\n\nLoading Geospatial Data in R\nIn R, the sf package is widely used for handling geospatial data. Let‚Äôs start by loading a Shapefile containing information about city boundaries.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load geospatial data\ncities &lt;- st_read(\"path/to/cities.shp\")\nThis code snippet assumes you have a Shapefile named cities.shp in your working directory. The st_read function from the sf package is used to read the Shapefile and create a spatial data frame.\n\n\nExploring Geospatial Data\nOnce the data is loaded, let‚Äôs explore its structure and attributes.\n# Display summary of the spatial data\nsummary(cities)\nThis will provide an overview of the spatial data, including the geometry type (point, line, or polygon), bounding box, and attribute data."
  },
  {
    "objectID": "posts/welcome/index.html#geospatial-data-visualization",
    "href": "posts/welcome/index.html#geospatial-data-visualization",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Geospatial Data Visualization",
    "text": "Geospatial Data Visualization\nVisualization is crucial for understanding geospatial patterns. We‚Äôll use the ggplot2 package for creating basic maps.\n# Install and load ggplot2\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Plotting the cities on a map\nggplot() +\n  geom_sf(data = cities) +\n  ggtitle(\"Cities Map\")\nHere, geom_sf is used to plot the spatial features on a map. Customize the plot further by adding layers, adjusting colors, and incorporating additional geospatial data."
  },
  {
    "objectID": "posts/welcome/index.html#spatial-queries-and-analysis",
    "href": "posts/welcome/index.html#spatial-queries-and-analysis",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Spatial Queries and Analysis",
    "text": "Spatial Queries and Analysis\nPerforming spatial queries allows us to extract meaningful information from geospatial data. Let‚Äôs say we want to find cities within a specific region.\n# Define a bounding box for the region\nbbox &lt;- st_bbox(c(xmin, ymin, xmax, ymax), crs = st_crs(cities))\n\n# Extract cities within the bounding box\ncities_in_region &lt;- cities[st_within(cities, st_as_sfc(bbox)), ]\nHere, st_within is used to filter cities that fall within the specified bounding box."
  },
  {
    "objectID": "posts/welcome/index.html#basic-geospatial-statistics",
    "href": "posts/welcome/index.html#basic-geospatial-statistics",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Basic Geospatial Statistics",
    "text": "Basic Geospatial Statistics\nUnderstanding the spatial distribution of data is crucial. Let‚Äôs explore basic spatial statistics using the spdep package.\n# Install and load spdep\ninstall.packages(\"spdep\")\nlibrary(spdep)\n\n# Spatial autocorrelation analysis\nmoran &lt;- moran.test(cities$population, listw = poly2nb(st_as_sfc(cities)))\nprint(moran)\nThis example conducts a Moran‚Äôs I test to assess spatial autocorrelation in the population data."
  },
  {
    "objectID": "posts/welcome/index.html#conclusion",
    "href": "posts/welcome/index.html#conclusion",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has provided a foundational understanding of geospatial data in R. We covered loading data, visualization, spatial queries, and basic statistics. As you delve deeper into geospatial analysis, you‚Äôll find R to be a versatile and powerful tool for unlocking valuable insights from spatial datasets.\nIn the next parts of this series, we will explore advanced topics such as spatial regression, machine learning with geospatial data, and building interactive web maps. Stay tuned for more insights into the fascinating world of spatial analytics with R!"
  },
  {
    "objectID": "posts/post-with-code/index.html#learning-curve",
    "href": "posts/post-with-code/index.html#learning-curve",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Excel:\n\nSimilarities: Ubiquitous in business for data analysis.\nDifferences: Limited for extensive data processing and visualization.\nUniqueness: Familiar interface but may require advanced functions for complex analytics.\n\nPower BI:\n\nSimilarities: Integrated with Microsoft products.\nDifferences: Emphasis on visualization and dashboards.\nUniqueness: User-friendly, with some learning required for advanced features.\n\nTableau:\n\nSimilarities: Focus on data visualization.\nDifferences: Steeper learning curve; powerful for interactive dashboards.\nUniqueness: Robust visualization capabilities, strong community support.\n\nSAS:\n\nSimilarities: Advanced analytics, statistical analysis.\nDifferences: Requires programming skills; traditional use for complex models.\nUniqueness: Industry-wide usage in healthcare and finance, extensive analytics capabilities.\n\nPython Dash:\n\nSimilarities: Python-based for web-based dashboards.\nDifferences: Programming-centric; suitable for data scientists.\nUniqueness: Flexibility and customization using Python.\n\nR Shiny:\n\nSimilarities: R-based, excellent for statistical analysis.\nDifferences: Requires knowledge of R programming.\nUniqueness: Strong statistical capabilities, ideal for creating interactive web applications."
  },
  {
    "objectID": "posts/post-with-code/index.html#business-capability-rating",
    "href": "posts/post-with-code/index.html#business-capability-rating",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Excel:\n\nStrengths: Versatile for small to medium-sized datasets.\nWeaknesses: Limited scalability, less advanced analytics.\n\nPower BI:\n\nStrengths: Seamless Microsoft integration, excellent visualizations.\nWeaknesses: May require additional tools for advanced analytics.\n\nTableau:\n\nStrengths: Powerful visualization, extensive data connectivity.\nWeaknesses: Steeper learning curve, higher cost.\n\nSAS:\n\nStrengths: Robust analytics, statistical modeling, and data management.\nWeaknesses: High cost, steeper learning curve.\n\nPython Dash:\n\nStrengths: Customizable with Python, suitable for data science applications.\nWeaknesses: Learning curve for those unfamiliar with Python.\n\nR Shiny:\n\nStrengths: Strong statistical capabilities, great for R users.\nWeaknesses: Learning curve for those unfamiliar with R.\n\n\nIn conclusion, the choice of BI tool depends on specific business needs, data scale, customization requirements, and the existing skill set. Whether opting for user-friendly interfaces or diving into more complex analytics, understanding these tools‚Äô nuances is crucial for effective decision-making."
  },
  {
    "objectID": "posts/GIS2/index.html",
    "href": "posts/GIS2/index.html",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Geospatial data manipulation is a crucial step in any spatial analysis project. In this blog post, we will delve into the world of geospatial data manipulation using R. Specifically, we‚Äôll explore loading and cleaning free internal data to pave the way for insightful analyses. Let‚Äôs embark on this journey together, using a hypothetical scenario where we have access to free internal geospatial data related to public parks.\n\n\nAssuming you have a Shapefile named parks.shp containing information about public parks, we‚Äôll use the sf package to load the data.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load internal geospatial data (parks)\nparks &lt;- st_read(\"path/to/parks.shp\")\nMake sure to replace ‚Äúpath/to/parks.shp‚Äù with the actual path to your Shapefile. The st_read function reads the Shapefile and creates a spatial data frame.\n\n\n\nBefore diving into manipulation, let‚Äôs explore the structure and attributes of the loaded geospatial data.\n# Display summary of the spatial data\nsummary(parks)\nThis summary will provide information about the geometry type, bounding box, and attributes of the parks dataset.\n\n\n\nClean data is essential for meaningful analyses. Let‚Äôs perform some basic cleaning steps on our parks dataset.\n\n\n# Remove duplicate geometries\nparks &lt;- st_unique(parks)\nThis step ensures that each park is represented only once in the dataset.\n\n\n\n# Check for missing values\nmissing_values &lt;- colSums(is.na(parks))\n\n# Display columns with missing values\nprint(names(missing_values[missing_values &gt; 0]))\nIdentify and handle missing values in relevant columns to maintain data integrity.\n\n\n\n\nVisualizing the geospatial data is a vital step in understanding its characteristics.\n# Plotting parks on a map\nplot(parks, main = \"Public Parks Map\", col = \"green\")\nThis basic map provides a visual overview of the public parks in the dataset. Customize it further using ggplot2 or other plotting libraries for more sophisticated visualizations.\n\n\n\nLet‚Äôs perform a basic analysis by calculating the area of each park.\n# Calculate area of parks\nparks$area &lt;- st_area(parks)\nNow, the area column contains the calculated area for each park. This information could be used for further analysis or visualization.\n\n\n\nIn this blog post, we‚Äôve covered the essential steps of loading and cleaning geospatial data using R. Starting with the hypothetical scenario of public parks, we explored data loading, cleaning, and basic analysis. Clean and well-organized geospatial data sets the stage for more advanced spatial analytics and insights.\nIn future articles, we will delve into advanced geospatial analysis, including spatial regression, machine learning with geospatial data, and the creation of interactive web maps. Stay tuned for more exciting explorations into the world of geospatial data science!"
  },
  {
    "objectID": "posts/GIS2/index.html#loading-free-internal-geospatial-data",
    "href": "posts/GIS2/index.html#loading-free-internal-geospatial-data",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Assuming you have a Shapefile named parks.shp containing information about public parks, we‚Äôll use the sf package to load the data.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load internal geospatial data (parks)\nparks &lt;- st_read(\"path/to/parks.shp\")\nMake sure to replace ‚Äúpath/to/parks.shp‚Äù with the actual path to your Shapefile. The st_read function reads the Shapefile and creates a spatial data frame."
  },
  {
    "objectID": "posts/GIS2/index.html#exploring-internal-geospatial-data",
    "href": "posts/GIS2/index.html#exploring-internal-geospatial-data",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Before diving into manipulation, let‚Äôs explore the structure and attributes of the loaded geospatial data.\n# Display summary of the spatial data\nsummary(parks)\nThis summary will provide information about the geometry type, bounding box, and attributes of the parks dataset."
  },
  {
    "objectID": "posts/GIS2/index.html#cleaning-geospatial-data",
    "href": "posts/GIS2/index.html#cleaning-geospatial-data",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Clean data is essential for meaningful analyses. Let‚Äôs perform some basic cleaning steps on our parks dataset.\n\n\n# Remove duplicate geometries\nparks &lt;- st_unique(parks)\nThis step ensures that each park is represented only once in the dataset.\n\n\n\n# Check for missing values\nmissing_values &lt;- colSums(is.na(parks))\n\n# Display columns with missing values\nprint(names(missing_values[missing_values &gt; 0]))\nIdentify and handle missing values in relevant columns to maintain data integrity."
  },
  {
    "objectID": "posts/GIS2/index.html#spatial-data-exploration",
    "href": "posts/GIS2/index.html#spatial-data-exploration",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Visualizing the geospatial data is a vital step in understanding its characteristics.\n# Plotting parks on a map\nplot(parks, main = \"Public Parks Map\", col = \"green\")\nThis basic map provides a visual overview of the public parks in the dataset. Customize it further using ggplot2 or other plotting libraries for more sophisticated visualizations."
  },
  {
    "objectID": "posts/GIS2/index.html#basic-geospatial-analysis",
    "href": "posts/GIS2/index.html#basic-geospatial-analysis",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Let‚Äôs perform a basic analysis by calculating the area of each park.\n# Calculate area of parks\nparks$area &lt;- st_area(parks)\nNow, the area column contains the calculated area for each park. This information could be used for further analysis or visualization."
  },
  {
    "objectID": "posts/GIS2/index.html#conclusion",
    "href": "posts/GIS2/index.html#conclusion",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "In this blog post, we‚Äôve covered the essential steps of loading and cleaning geospatial data using R. Starting with the hypothetical scenario of public parks, we explored data loading, cleaning, and basic analysis. Clean and well-organized geospatial data sets the stage for more advanced spatial analytics and insights.\nIn future articles, we will delve into advanced geospatial analysis, including spatial regression, machine learning with geospatial data, and the creation of interactive web maps. Stay tuned for more exciting explorations into the world of geospatial data science!"
  },
  {
    "objectID": "posts/spatial regression/index.html",
    "href": "posts/spatial regression/index.html",
    "title": "Spartial regression: Lesson3",
    "section": "",
    "text": "Spatial regression is a powerful technique in spatial analytics that allows us to model relationships between variables while accounting for the spatial dependencies inherent in geospatial data. In this advanced blog post, we will dive into the intricacies of spatial regression using R. Our goal is to uncover hidden patterns and relationships in geospatial datasets, focusing on a hypothetical scenario of housing prices and neighborhood characteristics."
  },
  {
    "objectID": "posts/spatial regression/index.html#understanding-spatial-regression",
    "href": "posts/spatial regression/index.html#understanding-spatial-regression",
    "title": "Spartial regression: Lesson3",
    "section": "Understanding Spatial Regression",
    "text": "Understanding Spatial Regression\nSpatial regression extends traditional regression models by incorporating spatial relationships. It acknowledges that observations closer in space may exhibit similarities or dependencies that traditional models overlook. There are different types of spatial regression models, and in this blog, we will focus on the Spatial Lag Model.\n\nSpatial Lag Model\nThe Spatial Lag Model introduces a spatially lagged dependent variable, indicating the influence of neighboring observations. Let‚Äôs consider housing prices as the dependent variable and neighborhood characteristics as independent variables.\n# Install and load required packages\ninstall.packages(\"spdep\")\nlibrary(spdep)\n\n# Load geospatial data (housing prices and neighborhood characteristics)\nhousing_data &lt;- st_read(\"path/to/housing_data.shp\")\n\n# Create spatial weights matrix\nw &lt;- poly2nb(st_as_sfc(housing_data))\nlw &lt;- nb2listw(w)\n\n# Fit Spatial Lag Model\nmodel &lt;- lm(y ~ x1 + x2 + lag(y, listw = lw), data = housing_data)\nsummary(model)\n\nReplace ‚Äúpath/to/housing_data.shp‚Äù with the actual path to your Shapefile. This example assumes you have a dependent variable y (housing prices) and independent variables x1 and x2 (neighborhood characteristics)."
  },
  {
    "objectID": "posts/spatial regression/index.html#interpretation-of-results",
    "href": "posts/spatial regression/index.html#interpretation-of-results",
    "title": "Spartial regression: Lesson3",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\nThe summary output provides information about coefficients, standard errors, and statistical significance. Pay special attention to the spatial lag coefficient, which indicates the impact of neighboring observations on the dependent variable."
  },
  {
    "objectID": "posts/spatial regression/index.html#diagnostic-checks",
    "href": "posts/spatial regression/index.html#diagnostic-checks",
    "title": "Spartial regression: Lesson3",
    "section": "Diagnostic Checks",
    "text": "Diagnostic Checks\nAssess the model‚Äôs validity and assumptions through diagnostic checks.\n# Spatial autocorrelation of residuals\nresiduals &lt;- residuals(model)\nmoran.test(residuals, listw = lw)\n\nA significant Moran‚Äôs I statistic for residuals indicates spatial autocorrelation, suggesting the need for further model refinement."
  },
  {
    "objectID": "posts/spatial regression/index.html#visualization",
    "href": "posts/spatial regression/index.html#visualization",
    "title": "Spartial regression: Lesson3",
    "section": "Visualization",
    "text": "Visualization\nVisualize the spatial patterns and regression results on a map.\n# Plotting observed vs. predicted values\nplot(housing_data$y, fitted(model), main = \"Observed vs. Predicted\", xlab = \"Observed\", ylab = \"Predicted\")\n\n# Spatial autocorrelation map of residuals\nspplot(residuals, main = \"Spatial Autocorrelation Map of Residuals\", col.regions = colorRampPalette(c(\"blue\", \"white\", \"red\")))\n\nThese visualizations help in understanding how well the model captures spatial patterns and where adjustments might be needed."
  },
  {
    "objectID": "posts/spatial regression/index.html#conclusion",
    "href": "posts/spatial regression/index.html#conclusion",
    "title": "Spartial regression: Lesson3",
    "section": "Conclusion",
    "text": "Conclusion\nSpatial regression in R opens up new dimensions for analyzing geospatial data. In this blog post, we explored the Spatial Lag Model as an advanced technique for modeling spatial dependencies in housing prices and neighborhood characteristics. The interpretation of results, diagnostic checks, and visualizations are crucial components of spatial regression analysis.\nAs you venture into spatial analytics, consider exploring other spatial regression models, incorporating additional spatial variables, and applying advanced techniques to enhance the robustness of your models. Stay tuned for more advanced spatial analytics topics, including machine learning with geospatial data and building interactive web maps. Happy analyzing!"
  },
  {
    "objectID": "posts/spatial regression/index.html#mastering-spatial-regression-in-r-unveiling-patterns-in-geospatial-data",
    "href": "posts/spatial regression/index.html#mastering-spatial-regression-in-r-unveiling-patterns-in-geospatial-data",
    "title": "Spartial regression: Lesson3",
    "section": "",
    "text": "Spatial regression is a powerful technique in spatial analytics that allows us to model relationships between variables while accounting for the spatial dependencies inherent in geospatial data. In this advanced blog post, we will dive into the intricacies of spatial regression using R. Our goal is to uncover hidden patterns and relationships in geospatial datasets, focusing on a hypothetical scenario of housing prices and neighborhood characteristics."
  },
  {
    "objectID": "posts/How to build a shinyapp/index.html",
    "href": "posts/How to build a shinyapp/index.html",
    "title": "How to build a ShinyApp",
    "section": "",
    "text": "We begin to demonstrate the building blocks of a shinyApp.\nAn App needs a *User interface (ui)* and a server. The majic about the *shiny package* is that it can create both of this within R, plus run your app using an additionational shiny function.\nFirst,\n\nload the library using the shiny.\n\n\nlibrary(shiny)\n\n\nCreate ui using the html function\n\n\nui &lt;- fluidPage()\n\n\nDefine a custom function to create the server\n\n\nserver &lt;- function(input,\n\noutput,\n\nsession){\n\n}\n\n\nfinally run your app.\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/How to build a shinyapp/index.html#building-a-hello-world-shinyapp",
    "href": "posts/How to build a shinyapp/index.html#building-a-hello-world-shinyapp",
    "title": "How to build a ShinyApp",
    "section": "",
    "text": "We begin to demonstrate the building blocks of a shinyApp.\nAn App needs a *User interface (ui)* and a server. The majic about the *shiny package* is that it can create both of this within R, plus run your app using an additionational shiny function.\nFirst,\n\nload the library using the shiny.\n\n\nlibrary(shiny)\n\n\nCreate ui using the html function\n\n\nui &lt;- fluidPage()\n\n\nDefine a custom function to create the server\n\n\nserver &lt;- function(input,\n\noutput,\n\nsession){\n\n}\n\n\nfinally run your app.\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/How to build a shinyapp/index.html#example1-of-shiny-app",
    "href": "posts/How to build a shinyapp/index.html#example1-of-shiny-app",
    "title": "How to build a ShinyApp",
    "section": "Example1 of shiny app",
    "text": "Example1 of shiny app\n\nlibrary(shiny)\n\nlibrary(widgetframe)\n\nui &lt;- fluidPage(\n\n\"Hello, world!!!!!!\"\n\n)\n\nserver &lt;- function(input,\n\noutput,\n\nsession){\n\n}\n\nshinyApp(ui = ui, server = server)\n\n\nExample2: Add a question\nWe want to go an extra mile an add a text that asks a question. This is possible but adding *textinput* function that allows us to enter text. It has three arguments, a unique ID that will be used to refer to this input, a label that is displayed to the user and an optional default value.\nOur full out put that is diplayed is contained in the server using the render text function. Inside of that you can use *paste* to create a longer character string. And if add *input$name* you can access the name added using text input. The text is assigned to an output object that will be used in the ui to display.\n\nlibrary(shiny)\n\nlibrary(widgetframe)\n\nui &lt;- fluidPage(\n\ntextInput(\"name\", \"Enter your name:\"),\n\ntextOutput(\"r\")\n\n)\n\nserver &lt;- function(input, output){\n\noutput$r &lt;- renderText({\n\npaste0(\"Do you prefer rain or sunshine,\", input$name, \"?\")\n\n})\n\n}\n\nshinyApp(ui = ui, server = server)\n\nYou did it a text that uses a text input!!"
  },
  {
    "objectID": "posts/getting data from Api/index.html",
    "href": "posts/getting data from Api/index.html",
    "title": "Getting Weather Data via API with R",
    "section": "",
    "text": "I came across this awesome post from Albert Rapp about how to get weather data from API and I thought it was a great technique to use. Here we go.\n\nWhat is an API?\nWe are focusing on two APIs (application programming interfaces) for our project. Broadly speaking, an API is anything that we can throw code at to get results that we want.\nOften this refers to some data source that we tap into. But sometimes it also simply means the syntax of code. For example, ggplot2 has a very distinctive API, i.e.¬†a code syntax to create a chart.\nIn our current case, we will just refer to APIs as data sources and we will need to tap into two such APIs, namely these ones:\n\nUS National Weather Service API\nGoogle Geocoding API\n\nThe first one will give us weather forecasts based on specified coordinates and the second one will turn any address into coordinates for us. Today, we‚Äôll focus on the first one.\n\n\nMaking requests to an API\nIf you‚Äôve never worked with APIs, you know that it can feel like data is hidden away behind an API. Thankfully, the {httr2} package helps us a lot, even if we‚Äôve never dealt with APIs before.\nCase in point, my fellow YouTubeR (see what I did there? it‚Äôs ‚ÄúYouTube‚Äù and ‚ÄúR‚Äù) Melissa Van Bussel put together an excellent video that shows you how to use {httr2} to call the API of openAI or GitLab.\nAnyway, here‚Äôs how to make a request to an API to get data:\n\nNavigate to the URL the data can be accessed from\n(Optional depending on the API) Authenticate\nGet the response\n\nWith the National Weather Service, you can easily try this yourself. Just head to the following url using your web browser:\nhttps://api.weather.gov/points/38.8894,-77.0352\nIf you navigate there, you will get cryptic data like that:\n\nThis is what is known as a JSON file. More on that later. For now, notice that what you see at the end of the url after points/ corresponds to the coordinates that are given in the JSON output.\nThis means that the recipe for calling the weather API is simple: Append points/{lat},{long} at the end of the base_url, i.e.¬†https://api.weather.gov/. In this case, {lat},{long} corresponds to the latitude and longitude of the location you want to get weather forecasts for.\n\n\nMaking a request with {httr2}\nThe {httr2} syntax to make this work mimics this quite well. Here‚Äôs how it looks.\n\nBasically, at the core of every request is the request() function that needs to know the base_url. This returns an &lt;httr2_request&gt; object that can be passed to further req_*() functions to modify the request.\nHere, we used req_url_path_append() to modify the request but there are also other functions (and next week we‚Äôll learn about req_url_query()). Finally, to actually make the request, you can pass everything to req_perform().\n\n\n\nGetting the response\nAs you‚Äôve just seen, your request will return a &lt;httr2_response&gt; and if everything went well, the output will also show you Status: 200 OK. You can get the actual content (the JSON that you‚Äôve seen in your web browser earlier) via one of the many resp_*() functions that handle responses.\n\n\n\nWorking with the response\nAs you‚Äôve seen in the output, the JSON file you receive is structured as a highly nested list. To make sense of this data, we use glimpse() to understand the structure of the object\n\nAnd with¬†pluck()¬†you can easily drill down into specific parts of the list. For example, this could be used to get the URL for the hourly forecasts\n\n\n\nRepeat process for forecasts\nWith the new forecast URL, we can get new JSON data about the forecasts for our location.\n\nIn that output, we can see that there is a list called¬†periods¬†inside of the¬†properties¬†list that contains lists which always have 16 entries. This might be where our forecast data lives.\n\nAha! Each of those 16-entries lists seem to correspond to the forecast of one hour. Thus, to get the things like¬†temperature,¬†startTime,¬†probabilityOfPrecipitation, and¬†short_forecast¬†into a nice tibble format, we have to iterate over all of these 16-entries lists and wrap the values we want into a tibble.\n\nAnd once we have that data, we can transform the¬†time¬†column into a nice date-time object:\n\nThis was awesome. !"
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "In the realm of data visualization, crafting compelling and informative graphics is an art. In this blog post, we will explore the step-by-step procedure of creating a visually stunning representation of educational attainment scores across different town sizes and income deprivation groups. Our tool of choice for this journey will be the powerful combination of ggplot2 and ggbeeswarm packages in R.\n\n\n\n\n\n\nBefore diving into the visualization process, we need to set up our R environment and load the necessary packages. Ensure you have ggplot2 and ggbeeswarm installed. Then, import your dataset containing information about educational attainment, town sizes, and income deprivation groups.\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\nlibrary(camcorder)\n\n# Idea and text from\n# https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/articles/whydochildrenandyoungpeopleinsmallertownsdobetteracademicallythanthoseinlargertowns/2023-07-25\n\n# set the graphic auto save\ngg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 9, height = 8, units = \"in\", dpi = 320)\n\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')\n\n\n\nTake a moment to explore your dataset and understand its structure. Identify the variables of interest and ensure they are in the right format for visualization. Clean the data if needed.\nee &lt;- english_education %&gt;% \n  filter(!size_flag %in% c(\"Not BUA\", \"Other Small BUAs\")) %&gt;% \n  mutate(\n    size = case_when(\n      str_detect(tolower(size_flag), \"london\") ~ \"Inner and outer London\",\n      size_flag == \"City\" ~ \"Cities (excluding London)\",\n      TRUE ~ size_flag\n    ),\n    income = case_when(\n      str_detect(income_flag, \"deprivation\") ~ income_flag,\n      TRUE ~ NA\n    )\n  ) %&gt;% \n  mutate(\n    size = fct_inorder(size), # order the names \n    income = fct_relevel(income, \n                         \"Higher deprivation towns\",\n                         \"Mid deprivation towns\",\n                         \"Lower deprivation towns\",\n    )\n  )\nCreate a summarized median educational score for each London region\nee_size_med &lt;- ee %&gt;% \n  group_by(size) %&gt;% \n  summarise(median = median(education_score, na.rm = TRUE))\n\n\n\nNow, let‚Äôs move on to the heart of our blog post - the creation of our beautiful visualization. We will utilize ggplot2 for its flexibility and ggbeeswarm for its ability to handle overlapping points gracefully.\n\nf1 &lt;- \"Outfit\"\npal &lt;- MetBrewer::met.brewer(\"Klimt\", 4)\ncol_purple &lt;- MetBrewer::met.brewer(\"Klimt\")[1]\n\nggplot() +\n  # Median and annotation\n  geom_vline(data = ee_size_med, aes(xintercept = median),\n             color = col_purple, linewidth = 1) +\n  geom_text(data = ee_size_med %&gt;% \n              filter(size == \"Large Towns\"), \n            aes(median,  Inf, label = \"Average for size group\"), hjust = 1,\n            nudge_x = -1.5, family = f1, fontface = \"bold\", \n            size = 3.5, color = col_purple) +\n  geom_curve(data = ee_size_med %&gt;% \n               filter(size == \"Large Towns\"),\n             aes(x = median - 1.2, xend = median - 0.2,\n                 y = Inf, yend = Inf), color = col_purple, \n             curvature = -0.3, arrow = arrow(length = unit(0.1, \"npc\"))) +\n  # Towns\n  geom_quasirandom(data = ee, aes(education_score, size, color = income)\n                   , alpha = 0.5, dodge.width = 2, method = \"pseudorandom\", size = 2.5) +\n  scale_color_manual(values = pal, na.value = \"grey20\", \n                     breaks = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"), \n                     labels = c(\"Lower\", \"Mid\", \"Higher\")) +\n  scale_x_continuous(minor_break = (-10:10)) +\n  coord_cartesian(clip = \"off\") +\n  facet_wrap(vars(size), ncol = 1, scales = \"free_y\") +\n  labs(\n    title = \"Smaller towns have the highest average educational attainment\",\n    subtitle = \"Educational attainment score, by town size and income deprivation group\",\n    caption = \"Source: UK Office for National Statistics ¬∑ Graphic: Georgios Karamanis\",\n    x = paste0(\"‚Üê Lower attainment\", strrep(\" \", 30), \"Educational attainment index score\", \n               strrep(\" \", 30), \"Higher attainment ‚Üí\"),\n    color = \"Town income\\ndeprivation\"\n  ) +\n  theme_minimal(base_family = f1) +\n  theme(\n    plot.background = element_rect(fill = \"grey99\", color = NA),\n    legend.position = c(0.88, 0.3),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.x = element_text(hjust = 0.32),\n    plot.margin = margin(10, 10, 10, 10),\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.caption = element_text(margin = margin(10, 0, 0, 0)),\n    panel.spacing.y = unit(1, \"lines\"),\n    strip.text = element_text(size = 11, face = \"bold\", margin = margin(10, 0, 10, 0))\n  )\nThis code creates a beeswarm plot, where each point represents a data point, and the median is visualized as a crossbar. This is a beautiful viz of 3 numeric variables, one numeric variable and a median as a measure of central tendancies. The color aesthetic distinguishes income deprivation groups, providing a comprehensive view of the educational attainment landscape.\n\n\n\nBy following this step-by-step guide, you‚Äôve learned how to use ggplot2 and ggbeeswarm to create an aesthetically pleasing and informative visualization of educational attainment scores. This not only enhances your storytelling capabilities but also adds a touch of elegance to your data presentations.\nRemember, the art of visualization lies not just in the final result but in the thoughtful process that leads to it. Now, armed with this knowledge, go forth and create compelling visuals that resonate with your audience."
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#introduction",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#introduction",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "In the realm of data visualization, crafting compelling and informative graphics is an art. In this blog post, we will explore the step-by-step procedure of creating a visually stunning representation of educational attainment scores across different town sizes and income deprivation groups. Our tool of choice for this journey will be the powerful combination of ggplot2 and ggbeeswarm packages in R."
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-1-setup-and-data-loading",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-1-setup-and-data-loading",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "Before diving into the visualization process, we need to set up our R environment and load the necessary packages. Ensure you have ggplot2 and ggbeeswarm installed. Then, import your dataset containing information about educational attainment, town sizes, and income deprivation groups.\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\nlibrary(camcorder)\n\n# Idea and text from\n# https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/articles/whydochildrenandyoungpeopleinsmallertownsdobetteracademicallythanthoseinlargertowns/2023-07-25\n\n# set the graphic auto save\ngg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 9, height = 8, units = \"in\", dpi = 320)\n\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')"
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-2-data-exploration-and-preparation",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-2-data-exploration-and-preparation",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "Take a moment to explore your dataset and understand its structure. Identify the variables of interest and ensure they are in the right format for visualization. Clean the data if needed.\nee &lt;- english_education %&gt;% \n  filter(!size_flag %in% c(\"Not BUA\", \"Other Small BUAs\")) %&gt;% \n  mutate(\n    size = case_when(\n      str_detect(tolower(size_flag), \"london\") ~ \"Inner and outer London\",\n      size_flag == \"City\" ~ \"Cities (excluding London)\",\n      TRUE ~ size_flag\n    ),\n    income = case_when(\n      str_detect(income_flag, \"deprivation\") ~ income_flag,\n      TRUE ~ NA\n    )\n  ) %&gt;% \n  mutate(\n    size = fct_inorder(size), # order the names \n    income = fct_relevel(income, \n                         \"Higher deprivation towns\",\n                         \"Mid deprivation towns\",\n                         \"Lower deprivation towns\",\n    )\n  )\nCreate a summarized median educational score for each London region\nee_size_med &lt;- ee %&gt;% \n  group_by(size) %&gt;% \n  summarise(median = median(education_score, na.rm = TRUE))"
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-3-creating-the-visualization",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-3-creating-the-visualization",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "Now, let‚Äôs move on to the heart of our blog post - the creation of our beautiful visualization. We will utilize ggplot2 for its flexibility and ggbeeswarm for its ability to handle overlapping points gracefully.\n\nf1 &lt;- \"Outfit\"\npal &lt;- MetBrewer::met.brewer(\"Klimt\", 4)\ncol_purple &lt;- MetBrewer::met.brewer(\"Klimt\")[1]\n\nggplot() +\n  # Median and annotation\n  geom_vline(data = ee_size_med, aes(xintercept = median),\n             color = col_purple, linewidth = 1) +\n  geom_text(data = ee_size_med %&gt;% \n              filter(size == \"Large Towns\"), \n            aes(median,  Inf, label = \"Average for size group\"), hjust = 1,\n            nudge_x = -1.5, family = f1, fontface = \"bold\", \n            size = 3.5, color = col_purple) +\n  geom_curve(data = ee_size_med %&gt;% \n               filter(size == \"Large Towns\"),\n             aes(x = median - 1.2, xend = median - 0.2,\n                 y = Inf, yend = Inf), color = col_purple, \n             curvature = -0.3, arrow = arrow(length = unit(0.1, \"npc\"))) +\n  # Towns\n  geom_quasirandom(data = ee, aes(education_score, size, color = income)\n                   , alpha = 0.5, dodge.width = 2, method = \"pseudorandom\", size = 2.5) +\n  scale_color_manual(values = pal, na.value = \"grey20\", \n                     breaks = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"), \n                     labels = c(\"Lower\", \"Mid\", \"Higher\")) +\n  scale_x_continuous(minor_break = (-10:10)) +\n  coord_cartesian(clip = \"off\") +\n  facet_wrap(vars(size), ncol = 1, scales = \"free_y\") +\n  labs(\n    title = \"Smaller towns have the highest average educational attainment\",\n    subtitle = \"Educational attainment score, by town size and income deprivation group\",\n    caption = \"Source: UK Office for National Statistics ¬∑ Graphic: Georgios Karamanis\",\n    x = paste0(\"‚Üê Lower attainment\", strrep(\" \", 30), \"Educational attainment index score\", \n               strrep(\" \", 30), \"Higher attainment ‚Üí\"),\n    color = \"Town income\\ndeprivation\"\n  ) +\n  theme_minimal(base_family = f1) +\n  theme(\n    plot.background = element_rect(fill = \"grey99\", color = NA),\n    legend.position = c(0.88, 0.3),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.x = element_text(hjust = 0.32),\n    plot.margin = margin(10, 10, 10, 10),\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.caption = element_text(margin = margin(10, 0, 0, 0)),\n    panel.spacing.y = unit(1, \"lines\"),\n    strip.text = element_text(size = 11, face = \"bold\", margin = margin(10, 0, 10, 0))\n  )\nThis code creates a beeswarm plot, where each point represents a data point, and the median is visualized as a crossbar. This is a beautiful viz of 3 numeric variables, one numeric variable and a median as a measure of central tendancies. The color aesthetic distinguishes income deprivation groups, providing a comprehensive view of the educational attainment landscape."
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#conclusion",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#conclusion",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "By following this step-by-step guide, you‚Äôve learned how to use ggplot2 and ggbeeswarm to create an aesthetically pleasing and informative visualization of educational attainment scores. This not only enhances your storytelling capabilities but also adds a touch of elegance to your data presentations.\nRemember, the art of visualization lies not just in the final result but in the thoughtful process that leads to it. Now, armed with this knowledge, go forth and create compelling visuals that resonate with your audience."
  }
]
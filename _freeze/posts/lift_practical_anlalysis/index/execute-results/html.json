{
  "hash": "426586c1eab37f0bb18a6c80a3ecad5c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Navigating Financial Diaries Data: A Practical Guide to Analyzing Longitudinal Survey Data in R\"\ndate: \"2024-08-05\"\ncategories: [Monitoring and Evaluation, Analysis]\nimage: \"cross-team.png\"\n---\n\n\n![](dashboard.png)\n\n### Case Study 2: Household Interview questions for Diaries research\n\nIntroduction:\nIn financial diaries research, surveys are conducted biweekly to capture how individuals manage their finances over time. The challenge lies in handling and analyzing these periodic datasets while ensuring consistency across each survey wave. In this guide, we will walk through how to merge datasets, analyze key variables, and visualize insights using R.\n\n\n\n## Setting up and Installation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.of.packages=c(\"tidyverse\",\"readxl\",\"nnet\", \"lubridate\", \"maps\")\nnew.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\nif(length(new.packages)) install.packages(new.packages,dependencies=T)\nsuppressMessages({\n  library(tidyverse)\n  library(readxl)\n  library(nnet)\n  library(lubridate)\n  library(maps)\n})\n\n# set theme\nmytheme <- theme(plot.title=element_text(face=\"bold.italic\",\n                                           size=\"14\", color=\"aquamarine4\"),\n                   axis.title=element_text(face=\"bold.italic\",\n                                           size=10, color=\"aquamarine4\"),\n                   axis.text=element_text(face=\"bold\", size=9,\n                                          color=\"darkblue\"),\n                   panel.background=element_rect(fill=\"white\",\n                                                 color=\"darkblue\"),\n                   panel.grid.major.y=element_line(color=\"grey\",\n                                                   linetype=1),\n                   panel.grid.minor.y=element_line(color=\"grey\",\n                                                   linetype=2),\n                   panel.grid.minor.x=element_blank(),\n                   legend.position=\"top\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiary1 <- read_xlsx(\"Biweekly1 Final.xlsx\")\ndiary2 <- read_xlsx(\"Biweekly2 Final.xlsx\")\ndiary3 <- read_xlsx(\"Biweekly3__Final_2016_18_11_10_12.0.xlsx\")\n```\n:::\n\n\n## Merging Biweekly Datasets\nThe first task in any longitudinal analysis is to combine data from multiple periods into a single dataset. By merging three biweekly datasets, we can create a comprehensive view of our respondents' data over time. Merging by Respondent ID ensures that all responses are aligned for each individual.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmerged_df <- diary1 %>% \n  inner_join(diary2, by = c(\"Respondent_ID\")) %>% #join using respondent_ID\n  mutate_at(\"Respondent_ID\", as.character) %>% #Harmonize the data types by changing into character the joining column\n  full_join(diary3, by = c(\"Respondent_ID\"))\n```\n:::\n\n\n## Subsetting Data for Income-Related Questions\n\nThe dataset contains numerous variables, but we are specifically interested in question A_4, which asks about income earned. This multiple-choice question requires subsetting relevant columns from the merged dataset. The codebook guides this process, allowing us to focus on the specific data we need for analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmerged_df %>% \n  select_at(vars(contains(\"A_4_\"))) -> income_df\n```\n:::\n\n\n## Creating a Frequency Table for Income Data\n\nA simple yet effective way to understand income distribution is by creating a frequency table. This table will show how many respondents selected each option for income, providing an overview of income sources.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nincome_prep <- income_df %>% \n  gather(key = \"question_name\", value = \"response\") %>% #melting the data\n  mutate(response = replace_na(response, 99)) %>% #convert NA to 99\n  group_by(question_name) %>% \n  count(response, name = \"frequency\") \n\nincome_table <- income_prep %>% \n  ungroup() %>% \n  spread(response, frequency) %>% \n  arrange(`0`, `1`, `99`)\n\nincome_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 48 × 4\n   question_name   `0`   `1`  `99`\n   <chr>         <int> <int> <int>\n 1 A_4_6.y         375   121    68\n 2 A_4_6.x         379   117    68\n 3 A_4_15.x        383   113    68\n 4 A_4_15.y        387   109    68\n 5 A_4_15          400   136    28\n 6 A_4_2.x         410    86    68\n 7 A_4_2.y         419    77    68\n 8 A_4_6           422   114    28\n 9 A_4_9.y         427    69    68\n10 A_4_9.x         437    59    68\n# ℹ 38 more rows\n```\n\n\n:::\n:::\n\n\n## Visualizing Income Data with a Bar Graph \n\nOnce the frequency table is generated, we can visualize the data. A bar graph makes it easy to compare the number of respondents across different income categories. The title for this graph is “Count of Respondents Having an Income Earning Activity,” offering clear insight into income distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#preparing the data for plot\nprep_income <- income_prep %>% \n  mutate_at(vars(\"response\"), as.factor) %>%\n  separate(question_name, into = c(\"question\", \"diary_period\"), sep = \"([.])\") %>%\n  mutate(diary_period = replace_na(diary_period, \"One\"),\n         diary_period = case_when(diary_period == \"x\" ~ \"Two\",\n                                  diary_period == \"y\" ~ \"Three\",\n                                  TRUE ~ diary_period),\n         question = str_extract(question, \"[:digit:]+\\\\b\")) \n\n#Plot\n prep_income %>% \n   ggplot(aes(diary_period, frequency, fill = response)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~question, scales = \"free_y\") +\n  labs(title = \"Count of respondents having an income earning activity\",\n       y = \"Frequency of Response\",\n       x = \"Period of Diary\",\n       fill = \"Key\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Income plot-1.png){width=672}\n:::\n:::\n\n\n<!-- ## The mean and the median of different saving tools added. -->\n<!-- Step 5: Analyzing Savings Tool Usage -->\n<!-- Here we shed light on how much respondents have saved across various savings tools. By calculating the median and mean savings for each tool, we gain a deeper understanding of savings behavior. This analysis helps us identify which tools are most commonly used and how much respondents are saving. -->\n\n\n\n\n\n\n## Investigating the Relationship Between Stress and Wealth (PPI Score)\nStress levels (Q_91) can be associated with the wealth proxy variable “ppicut,” providing valuable insight into the socio-economic factors affecting stress. A correlation analysis between these two variables reveals whether wealth status influences stress levels among respondents.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load the segment data\nsegment_df <- read_xlsx(\"segmentation variables.xlsx\")\n\n#join the new data to the merged\njoined <- segment_df %>% \n  mutate(Respodent_ID = as.character(Respodent_ID)) %>% \n  inner_join(merged_df, by = c(\"Respodent_ID\" = \"Respondent_ID\")) \n\nmultinomial_df <- joined %>% \n  select(starts_with(\"Q_91\"), ppicut) %>% \n  mutate(ppicut = as.factor(ppicut),\n         ppicut = relevel(ppicut, ref = \"poor\")) #make the poor status the reference\n\n# The multinomial model and summary\nmulti_model <- multinom(ppicut ~ ., \n             data = multinomial_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# weights:  25 (16 variable)\ninitial  value 753.216943 \niter  10 value 505.215239\niter  20 value 497.300727\nfinal  value 497.290157 \nconverged\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(multi_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nmultinom(formula = ppicut ~ ., data = multinomial_df)\n\nCoefficients:\n            (Intercept)     Q_91.x        Q_91.y       Q_91\ncomfortable -1.62871133 0.03750504  0.0635473286  0.2961931\nultra poor  -3.17084979 0.23501748 -0.0992323354 -0.2142214\nupper poor  -0.08364761 0.17288377 -0.0006068483  0.1182401\nwealthy     -0.66441374 0.02458017 -0.3704320573 -0.3862365\n\nStd. Errors:\n            (Intercept)    Q_91.x    Q_91.y      Q_91\ncomfortable   0.5525604 0.1315151 0.1376271 0.1374913\nultra poor    1.8341828 0.4476755 0.4807289 0.4854138\nupper poor    0.4122153 0.1013992 0.1061545 0.1052570\nwealthy       1.2398195 0.3318420 0.3794236 0.3779930\n\nResidual Deviance: 994.5803 \nAIC: 1026.58 \n```\n\n\n:::\n:::\n\n\n## Visualizing the Stress-Wealth Relationship(Using a Multinomial Plot)\nA scatter plot or similar graph is a powerful way to visualize the relationship between stress levels and PPI score. This graphical representation helps us better understand the distribution of data points and any potential correlation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultinomial_df %>% \n  gather(value, key, -ppicut) %>% \n  ggplot(aes(key, ppicut, color = key)) +\n  geom_count() +\n  facet_wrap(~value)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Multinomial plot-1.png){width=672}\n:::\n:::\n\n\n## Surveyor Performance Analysis (Average time)\nUsing the VStart and VEnd variables, we can calculate the average time each surveyor (Srvyr) spent on interviews. This performance metric is crucial for evaluating the efficiency of data collection and understanding how long respondents take to complete surveys.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## preparing the data\nsurveyor_prep <- joined %>% \n  select(starts_with(c(\"Vstart\", \"VEnd\")), Srvyr.x.x) %>% \n  drop_na()#removing missing date values\n\n#Mean time take by surveyors\nsury_df <- surveyor_prep %>% \n  group_by(Srvyr.x.x) %>% \n  mutate(time_for_period1 = difftime(VEnd, VStart, \n                                units = \"days\"),\n         time_for_period2 = difftime(VEnd.x, VStart.x, \n                                units = \"days\"),\n         time_for_period3 = difftime(VEnd.y, VStart.y, \n                                units = \"days\"),) %>%\n  summarise(across(starts_with(\"time_for_period\"), ~mean(.x, na.rm = TRUE), .names = \"mean_{.col}\"))\n```\n:::\n\n\n## Gender Differences in Savings Tool Usage (Men vs Women)\nWe can further explore the dataset by examining which savings tools are preferred by men versus women. A comparative plot allows us to see if certain tools are more likely to be used by one gender over the other, shedding light on gendered financial behavior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaving_tools_prep <- joined %>% \n  select(Q_153, starts_with(\"A_10\"))\n\n\nsaving_tools_df <- saving_tools_prep %>% \n  gather(key, value, -Q_153) %>% #melting the data\n  mutate(value = replace_na(value, 99)) %>% #convert NA to 99\n  group_by(key, Q_153) %>% \n  count(value, name = \"frequency\") %>% \n  mutate_at(vars(\"value\"), as.factor) %>%\n  separate(key, into = c(\"question\", \"diary_period\"), sep = \"([.])\") %>%\n  mutate(diary_period = replace_na(diary_period, \"One\"),\n         diary_period = case_when(diary_period == \"x\" ~ \"Two\",\n                                  diary_period == \"y\" ~ \"Three\",\n                                  TRUE ~ diary_period),\n         question = str_extract(question, \"[:digit:]+\\\\b\"),\n         question = as.double(question),\n         question = case_when(question == 1 ~ \"Keeping money at home\",\n                              question == 2 ~ \"On the body/in \\nclothes/in wallet\",\n                              question == 3 ~ \"Lend to others\",\n                              question == 4 ~ \"Buy something to sell later\",\n                              question == 5 ~ \"Savings group\",\n                              question == 6 ~ \"MDI\",\n                              question == 7 ~ \"Micro finance institution\",\n                              question == 8 ~ \"Bank account\",\n                              question == 9 ~ \"Buy stock \",\n                              question == 10 ~ \"Buy cattle \",\n                              TRUE ~ \"Other\"\n),\nvalue = relevel(value, ref = \"1\")) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 78 rows [1, 2, 3, 4, 5,\n6, 19, 20, 21, 22, 23, 35, 36, 37, 38, 39, 40, 53, 54, 55, ...].\n```\n\n\n:::\n\n```{.r .cell-code}\n#Plot for period 1\n saving_tools_df %>% \n   filter(diary_period == \"One\",\n          value != 99) %>% \n   ggplot(aes(Q_153, frequency, fill = value)) +\n  geom_col(position = \"stack\") +\n  facet_wrap(~question, scales = \"free_y\") +\n  labs(title = \"Count by gender of use of daving tools\",\n       y = \"Frequency use of saving tools\",\n       x = \"Period of Diary\",\n       fill = \"Key\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Saving tools-1.png){width=672}\n:::\n:::\n\n\n## Mapping Respondent Locations \nBy selecting one of the biweekly datasets, we can visualize where respondents are located using a geographical plot. This map gives us a clear view of the geographic distribution of participants in the study, providing context for the socio-economic and environmental factors at play.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## data prep\nworld <- map_data(\"world\") %>% \n  filter(region == c( \"Uganda\"))\n\n## Map by gender\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"gray50\", size = 0.05, alpha = 0.2\n  ) +\n  geom_point(\n    data = joined,\n    aes(Longitude, Latitude, color = Q_153),\n    alpha = 0.8\n  ) +\n  theme_void()+\n  labs(x = NULL, y = NULL, color = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gender map-1.png){width=672}\n:::\n:::\n\n\n## Pie Chart for People porvety Index (PPI)\nUsing any column of choice, a pie chart is a useful tool for showing proportional distribution within a dataset. For example, visualizing savings tools or income sources with a pie chart gives a clear snapshot of the most common categories.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Prepare data for piechart\npiechart_prep <- joined %>% \n  select(ppicut) %>% \n  count(ppicut, name = \"count\") %>% \n  mutate(value = count/sum(count),\n         ppicut = str_to_upper(ppicut))\n\n# Create a basic bar\npie <-  piechart_prep %>% \n  ggplot( aes(x=\"\", y=value, fill=ppicut)) + \n  geom_bar(stat=\"identity\", width=1) +\n  # Convert to pie (polar coordinates) and add labels\n  coord_polar(\"y\", start=0) + \n  geom_text(aes(label = paste0(round(value*100), \"%\")),\n            position = position_stack(vjust = 0.5)) +\n  # Add color scale (hex colors)\n  scale_fill_manual(values=c(\"#55DDE0\", \"#33658A\", \"#2F4858\", \"#F6AE2D\", \"#F26419\")) +\n  # Remove labels and add title\n  labs(x = NULL, y = NULL, fill = NULL, title = \"People poverty index\") +\n  # Tidy up the theme\n  theme_classic() + \n  theme(axis.line = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          plot.title = element_text(hjust = 0.5, color = \"#666666\"))\n\npie\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/piechart-1.png){width=672}\n:::\n:::\n\n\n## The Power of dplyr for Data Manipulation\nThe dplyr package in R offers a suite of functions for filtering, selecting, and summarizing data. For instance, filtering respondents by income bracket and summarizing their savings allows us to draw meaningful conclusions about their financial behavior. The ease of grouping and plotting with dplyr and ggplot makes complex analyses accessible.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaving_tools_prep <- joined %>% \n  select(Q_153, starts_with(\"A_10\"))\n\n\nsaving_tools_df <- saving_tools_prep %>% \n  gather(key, value, -Q_153) %>% #melting the data\n  mutate(value = replace_na(value, 99)) %>% #convert NA to 99\n  group_by(key, Q_153) %>% \n  count(value, name = \"frequency\") %>% \n  mutate_at(vars(\"value\"), as.factor) %>%\n  separate(key, into = c(\"question\", \"diary_period\"), sep = \"([.])\") %>%\n  mutate(diary_period = replace_na(diary_period, \"One\"),\n         diary_period = case_when(diary_period == \"x\" ~ \"Two\",\n                                  diary_period == \"y\" ~ \"Three\",\n                                  TRUE ~ diary_period),\n         question = str_extract(question, \"[:digit:]+\\\\b\"),\n         question = as.double(question),\n         question = case_when(question == 1 ~ \"Keeping money at home\",\n                              question == 2 ~ \"On the body/in \\nclothes/in wallet\",\n                              question == 3 ~ \"Lend to others\",\n                              question == 4 ~ \"Buy something to sell later\",\n                              question == 5 ~ \"Savings group\",\n                              question == 6 ~ \"MDI\",\n                              question == 7 ~ \"Micro finance institution\",\n                              question == 8 ~ \"Bank account\",\n                              question == 9 ~ \"Buy stock \",\n                              question == 10 ~ \"Buy cattle \",\n                              TRUE ~ \"Other\"\n),\nvalue = relevel(value, ref = \"1\")) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 78 rows [1, 2, 3, 4, 5,\n6, 19, 20, 21, 22, 23, 35, 36, 37, 38, 39, 40, 53, 54, 55, ...].\n```\n\n\n:::\n\n```{.r .cell-code}\n#Plot for period 1\n saving_tools_df %>% \n   filter(diary_period == \"One\",\n          value != 99) %>% \n   ggplot(aes(Q_153, frequency, fill = value)) +\n  geom_col(position = \"stack\") +\n  facet_wrap(~question, scales = \"free_y\") +\n  labs(title = \"Count by gender of use of daving tools\",\n       y = \"Frequency use of saving tools\",\n       x = \"Period of Diary\",\n       fill = \"Key\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/dplyr verbs-1.png){width=672}\n:::\n:::\n\n\n## Sampling Methods and R Implementation\nWhen working with survey data, sampling is key to ensuring representative results. In this section, we discuss simple random sampling, stratified sampling, and cluster sampling, providing an overview of how to implement these methods in R.\n\nsampling can be divided into two types.\n\n1.  Probabilistic sampling\n\n2.  Non-probalistic sampling\n\n### Probabilistic sampling\n\nSome of the probabilistic sampling techniques are simple random sampling, systematic sampling, cluster sampling and stratified sampling.\n\n#### simple random sampling\n\nIn *r* we can use the \\~sample function\\~ to select a random sample with or without replacement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:12\n# a random permutation\nsample(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  3  2  8 11  7  1  9  4  5  6 12 10\n```\n\n\n:::\n:::\n\n\n#### systematic sampling\n\nThis technique selects units based on a fixed sampling interval. In R we can set up a function that checks for our condition as in the example [here](https://medium.com/swlh/probability-sampling-with-r-a03d77788d98)\n\n#### stratified sampling\n\nThis involves grouping the data into selected statas. We can use the *sample_n* or the *sample_f* after we have group the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nby_cyl_n <- mtcars %>% \n  group_by(cyl) %>% \n  sample_n(2)\n\n#or\n\nby_cyl_f <- mtcars %>% \n  group_by(cyl) %>% \n  sample_frac(.1)\n```\n:::\n\n\n#### Cluster sampling\n\nThis technique divides the population in clusters of equal size n and selects clusters every individual time.\n\n## Handling Null Values in R\nNull values are inevitable in real-world datasets, and handling them effectively is essential. Two common approaches are either removing rows with null values or imputing missing data based on the median or mean. Each method has its pros and cons, which should be carefully considered based on the analysis.\n\n\n### Ways of Dealing with Missing Values\n\nThere are three types of missing values types.\n\n1.  Missing at random\n\n2.  Missing completely at random\n\n3.  Missing not at random.\n\nThe major ways of dealing with missing values are:-\n\n1.  **Drop NULL** - dropping the values if they are considerably small taking into account the rule of thumb in sample size number.\n\n2.  **Imputation** - Replacing the missing values with the mean, median or mode. This highly depends on the distribution of the data variable and the spread. Imputation can be done using kNN, boostrap aggregation and random forest as ways of machine learning.\n\n### Conclusion:\n\nFinancial diaries data provides a wealth of information about individuals' financial behavior over time. By combining datasets, analyzing key variables, and visualizing trends, we can extract valuable insights to inform decision-making and policy. Using R, the process becomes streamlined and efficient, allowing researchers to focus on what matters most—understanding the data.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}